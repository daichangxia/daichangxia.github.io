<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[eureka服务治理]]></title>
    <url>%2F2020%2F06%2F15%2Feureka%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86%2F</url>
    <content type="text"><![CDATA[Eureka(d版本) 源码分析2018年7月4日 16:48 Eureka serverEureka是一种基于REST（Representational State Transfer）的服务，主要用于AWS云，用于定位服务，以实现中间层服务器的负载平衡和故障转移。 开启通过@EnableEurekaServer注解开启服务端功能. 屏幕剪辑的捕获时间: 2018/7/19 17:33 引入一个EurekaServerMarkerConfiguration类,生命周期为运行期.在EurekaServerMarkerConfiguration中创建了一个Maker()对象注入到spring容器中.而这个Maker类是一个空类.就类似于开关的功能. 屏幕剪辑的捕获时间: 2018/7/19 17:37 自动装配配置可以看到这个类有个注解:@ConditionalOnBean,这个注解是:当且仅当spring容器中有这个实例时,会创建被注解所注释的类的实例.所以Maker对象的注入会使EurekaServerAutoConfiguration创建实例.这个就是eurekaserver的自动配置类. 1.这个类被@Configuration注释,该类会构建bean，用作初始化Spring容器. 2.@Import注解又把EurekaServerInitializerConfiguration的bean放到spring容器中. 2.1.EurekaServerInitializerConfiguration这个类用作初始化配置,在这个类中,注入了EurekaServerBootstrap的实例,执行contextInitialized方法,然后会在日志中打印”Started Eureka Server”.可见, EurekaServerBootstrap就是eureka server的真正初始化入口. EurekaServerBootstrap3.EurekaServerBootstrap: 屏幕剪辑的捕获时间: 2018/7/20 10:37 初始化环境变量可以看出主要做了两件事，initEurekaEnvironment()与initEurekaServerContext() initEurekaEnvironment():初始化一些必要的环境变量，因为Eureka配置基于Spring的配置中间件Archaius，Archaius是Netflix公司开源的java配置管理类库,主要用于多配置存储的动态获取.这些环境变量都是针对这个配置中间件使用的。 主要初始化的是: 3.1.eureka_dateCenter:设置数据中心,主要是在AWS环境下,需要设置为cloud,因为在AWS cloud中,无法使用标准的主机名或ip地址来识别eureka server.但是eureka server可以用来根据主机名找到其他服务,所以在AWS cloud 下,需要为集群中每一个server配置一个弹性IP,用来为eureka server提供一组标准的可识别地址. 3.2 eureka_environment:环境,默认为test.用于在不同的服务器环境(prod,test)下读取不同的配置文件 初始化服务initEurekaServerContext()是服务初始化： 1.初始化上下文 2.从其他节点中读取注册信息,并开发注册 屏幕剪辑的捕获时间: 2018/7/25 9:49 synUp为PeerAwareInstanceRegistryImpl里面的方法.在这个类中引入了eurekaClient对象. 2.1.步骤: ①.在每次等待30s后(30s为同步请求默认时间间隔),默认为5次,通过eurekaClient去拉取所有的应用,得到各应用的实例信息. ②.判断得到的应用的实例是否为可注册,主要为对亚马逊环境下的实例做处理,当区域是 US_EAST_1(aws的一个集群区域,美东（北佛杰尼亚）),并且没有设置机房(为默认项),或者在配置了机房,同一个区域下,也可以进行注册.然后根据给定的时间去发起同步请求.然后来到AbstractInstanceRegistry类. 主要成员AbstractInstanceRegistry:有几个全局变量: ConcurrentHashMap&lt;String, Map&lt;String, Lease&gt;&gt; registry:注册表,结构:key:实例名,value:map ConcurrentMap&lt;String, InstanceStatus&gt; overriddenInstanceStatusMap,覆盖状态表.每小时清空一次 CircularQueue&lt;Pair&lt;Long, String&gt;&gt; recentRegisteredQueue:最近注册的队列. CircularQueue&lt;Pair&lt;Long, String&gt;&gt; recentCanceledQueue:最近取消的队列. ConcurrentLinkedQueue recentlyChangedQueue:最近状态发生过变化的队列 volatile int numberOfRenewsPerMinThreshold:心跳阙值 volatile int expectedNumberOfRenewsPerMin:期望最大心跳. 等数据存储结构来记录最近的操作. ③.同步请求时首先对实例的覆盖状态做了一系列判断, 覆盖状态:覆盖状态不是实例的状态,而是另一种属性,正常的实例一般有up,out-of-service两种状态.主要是用作在不关闭该实例的情况下,将其暂停服务.例:eureka.shouldFilterOnlyUpInstances = true,可将覆盖状态不为up的服务直接暂停 eureka状态eureka上的实例有以下几种覆盖状态: UNKNOWN,STARTING,UP,OUT_OF_SERVICE,DOWN 屏幕剪辑的捕获时间: 2018/7/30 15:21 首先根据appName,判断注册表map是否存在该应用,如果不存在,就把他放进去. 根据实例id判断是否存在租期.租期结构为: Lease existingLease,如果不存在,就会通过计算覆盖状态去得到最终实例状态,然后再新增一个实例.这部分涉及到一个保护模式和覆盖规则. 如果存在,则会用已经存在的实例去代替此次同步的实例. 将租期放到recentRegisteredQueue. 自我保护模式:简要说下, 当Eureka Server节点在短时间内丢失过多客户端时（可能发生了网络分区故障），那么这个节点就会进入自我保护模式。一旦进入该模式，Eureka Server就会保护服务注册表中的信息，不再删除服务注册表中的数据（也就是不会注销任何微服务）。当网络故障恢复后，该Eureka Server节点会自动退出自我保护模式。具体计算为: 在每一次新注册实例的时候,都会计算一次阙值:阈值=(expectedNumberOfRenewsPerMin+2)*0.85. 因此:总的阙值=(实例个数2)阙值百分比(默认为0.85) 例子:6个实例,阙值为620.85=10,即一分钟server如果收到心跳次数如果小于10,将触发保护模式. 覆盖规则:每个新注册的实例的覆盖状态默认为UNKNOW 在对覆盖状态不为unknow的实例,以id为key,覆盖状态为value放进overriddenInstanceStatusMap,然后再通过读取map,给此次实例赋值覆盖状态. 随后,通过一些规则去计算覆盖状态: 屏幕剪辑的捕获时间: 2018/7/27 16:46 进入FirstMatchWinsCompositeRule类,这个类初始化的时候加入了4种覆盖规则,然后在apply下去匹配,匹配成功后返回. 屏幕剪辑的捕获时间: 2018/7/26 17:03 考略到实例的最终状态会被覆盖状态覆盖,所以四条规则以可信赖为主, 1.DownOrStartingRule:当服务状态为down或者staring的时候,返回可信赖和状态 2.OverrideExistsRule:覆盖状态map里面有该实例的覆盖状态,返回map里面的状态和可信赖 3.AsgEnabledRule:aws环境下的实例,如果是out-of-service或者up,返回状态. 4.LeaseExistsRule:由于eureka会复制实例信息到集群中的其他服务器,所以要看是否为eureka的复制请求,如果是,并且已存在的实例状态为up或者out-of-service,返回可信赖和状态 5.AlwaysMatchInstanceStatusRule:其他4种都不成功时,则直接用默认的规则:直接返回可信赖和状态 最终实例的状态由计算的覆盖状态确定.然后将改租期放到recentlyChangedQueue队列. 屏幕剪辑的捕获时间: 2018/7/30 16:36 所有实例同步完成后,返回成功个数. 并且实例数加1.最终返同步制请求成功的实例的注册表的个数. 开放注册. 屏幕剪辑的捕获时间: 2018/7/30 17:02 如果同步的个数为0,即此次启动的server为初始server,则会限制默认为5分钟的时间后(可配置),才会被client拉取注册信息. 对注册数据中心做判断,在aws环境下,有时候eureka server 启动后,aws 防火墙可能没有立即启动,这会导致出站连接失败,但是入站连接已经生效了,这就意味着客户端切换到这个节点时,由于缺少这个实例外向的心跳,其他的eureka 节点会过期所有已经切换到aws上的server的实例. 解决办法是通过阻塞和等待,直到成功一次能ping通所有的eureka 节点,才会开放注册. 屏幕剪辑的捕获时间: 2018/8/7 0:21 应由管理器将实例状态设置为UP. 同时,会启动一个定时器去定时清理过期的客户端. 屏幕剪辑的捕获时间: 2018/8/7 0:23 设置延时多少秒去下线服务: 屏幕剪辑的捕获时间: 2018/8/7 0:31 然后去下线服务: 如果是自我保护模式:直接返回. 屏幕剪辑的捕获时间: 2018/8/7 0:36 然后将下线的服务全部放到一个list里面,然后随机的去取消.在取消的时候,如果出发保护模式的阙值为取消终止点. 从registry中移除这个app下面的实例id. 监听事件注册所有的监听: 屏幕剪辑的捕获时间: 2018/8/7 0:54 Eureka的server端会发出5个事件通知，分别是： EurekaInstanceCanceledEvent 服务下线事件 EurekaInstanceRegisteredEvent 服务注册事件 EurekaInstanceRenewedEvent 服务续约事件 EurekaRegistryAvailableEvent Eureka注册中心启动事件 EurekaServerStartedEvent Eureka Server启动事件 在server项目中,通过@EventListener,则可以监听上面5类事件 eurekaController类:主要是提供接口./lastn—获取最近注册的和取消的实例.等 屏幕剪辑的捕获时间: 2018/8/7 1:19 rest接口同时,server端会提供rest接口: &lt;img src=”C:%5CUsers%5Cdaicx2%5CDesktop%5Ceureka%5Ceureka.assets%5Cclip_image001.png” alt=”eureka-core-I .6.2jar - -maven com.netflix_eureka 53 aWS cluster lease registry 57 resources AbstractVlPResource.cIass style=”zoom:50%;” &gt; ApplicationResource_class @Prod publi p p p p p “ /&gt; 屏幕剪辑的捕获时间: 2018/8/9 20:53 接口在eureka-core包下面,只分析几个, 服务注册接口 屏幕剪辑的捕获时间: 2018/8/9 20:57 1.1.验证instanceinfo包含所有必需的字段 1.2.处理客户可能在数据中心信息中登记丢失数据的情况 然后回到AbstractInstanceRegistry.class的register方法. 屏幕剪辑的捕获时间: 2018/8/9 21:09 server提供的续约和下线接口,在InstanceResource.class里面, 续约:PUT请求 屏幕剪辑的捕获时间: 2018/8/9 21:13 首先服务续约发布事件, 屏幕剪辑的捕获时间: 2018/8/9 21:28 然后, 屏幕剪辑的捕获时间: 2018/8/9 21:30 经过一系列判断,最终执行 屏幕剪辑的捕获时间: 2018/8/9 21:32 将其时间戳进行更新. 服务的下线 屏幕剪辑的捕获时间: 2018/8/9 21:34 同样是调用register的 屏幕剪辑的捕获时间: 2018/8/9 21:36 从overriddenInstanceStatusMap移除,然后重新计算阙值. 屏幕剪辑的捕获时间: 2018/8/9 21:38 屏幕剪辑的捕获时间: 2018/8/9 21:18 根据服务名和id,从register取出实例,得到时间戳.如果小于更新的时间戳,返回404, Eureka client初始化 \1. 创建 EurekaInstanceConfig对象 \2. 使用 EurekaInstanceConfig对象 创建 InstanceInfo对象 \3. 使用 EurekaInstanceConfig对象 + InstanceInfo对象 创建 ApplicationInfoManager对象 \4. 创建 EurekaClientConfig对象 \5. 使用 ApplicationInfoManager对象 + EurekaClientConfig对象 创建 EurekaClient对象 开启1.首先在启动类上找到注解@EnableDiscoveryClient,此注解开启服务的注册发现功能. 屏幕剪辑的捕获时间: 2018/8/9 10:40 加载类2.注解@EnableDiscoveryClient上有注解@Import(EnableDiscoveryClientImportSelector.class)修饰， @Import不仅可以单独导入一个配置，另外也可以导入普通的java类，并将其声明为一个bean。此处导入EnableDiscoveryClientImportSelector.class后，加载我们用到的bean，EnableDiscoveryClientImportSelector类关键方法： 屏幕剪辑的捕获时间: 2018/8/8 0:57 方法isEnabled(),返回默认为true，那么说明只要是引入了EnableDiscoveryClientImportSelector类，spring.cloud.discovery.enabled就处于enable状态。 3.EnableDiscoveryClientImportSelector继承了类SpringFactoryImportSelector,我们再来看这个类的源码：主要方法: 屏幕剪辑的捕获时间: 2018/8/8 1:05 然后调用标注, 屏幕剪辑的捕获时间: 2018/8/8 1:09 然后进入loadSpringFactories方法, 屏幕剪辑的捕获时间: 2018/8/8 1:10 屏幕剪辑的捕获时间: 2018/8/8 1:11 至此,会导入这个文件里面的类.这个spring.factories指的是@EnableEurekaClient对应源码中META-INF下的spring.factories文件 &lt;img src=”C:%5CUsers%5Cdaicx2%5CDesktop%5Ceureka%5Ceureka.assets%5Cclip_image001-1591683373048.png” alt=” style=”zoom:50%;” &gt; InetUtilsProperties.class SpringFactorylmportSelector.class UtilAutoConfiguration.class v b META-INF maven MANIFEST.MF { spring-configuration-metadata.json “ /&gt; 屏幕剪辑的捕获时间: 2018/8/8 1:14 里面的内容是:spring.factories 屏幕剪辑的捕获时间: 2018/8/8 1:21 EurekaDiscoveryClientConfiguration打开导入的第一个类, 屏幕剪辑的捕获时间: 2018/8/8 1:24 上面的注解表示,当前容器中中存在EurekaInstanceConfigBean,EurekaClient,ConfigServerProperties的时候，当前这个配置类会被进行加载，否则不会加载。 同时,在EurekaDiscoveryClientConfiguration类中,会生成一个maker()空类作为开关,去开启EurekaClientAutoConfiguration类,如图: 屏幕剪辑的捕获时间: 2018/8/8 1:49 在这个类里面,会实例化eureka client对象放进容器, 主要方法为: 屏幕剪辑的捕获时间: 2018/8/8 1:59 从配置中心文件中取相应的启动配置,初始化配置. · 易混淆概念: · EurekaInstanceConfig，重在应用实例，例如，应用名、应用的端口等等。此处应用指的是，Application Consumer 和 Application Provider。 · EurekaClientConfig，重在 Eureka-Client，例如， 连接的 Eureka-Server 的地址、获取服务提供者列表的频率、注册自身为服务提供者的频率等等。 屏幕剪辑的捕获时间: 2018/8/9 15:13 EurekaClienteurekaClient接口继承了LookupService接口,来实现服务发现功能.同时,具体实现由DiscoveryClient来完成. 因此,在这个DiscoveryClient类里面,分别有一下功能: · 向 Eureka-Server 注册自身服务 · 向 Eureka-Server 续约自身服务 · 向 Eureka-Server 取消自身服务，当关闭时 · 从 Eureka-Server 查询应用集合和应用实例信息 · 简单来理解，对 Eureka-Server 服务的增删改查 屏幕剪辑的捕获时间: 2018/8/9 17:44 备份初始化的时候,注意引入了一个backupRegistryProvider,这个是注册中心备份接口. 但是点进去,发现: 屏幕剪辑的捕获时间: 2018/8/9 17:48 所以目前基本没什么用. 屏幕剪辑的捕获时间: 2018/8/9 17:52 1.如果shouldRegisterWithEureka与shouldFetchRegistry都为false,那么直接return 屏幕剪辑的捕获时间: 2018/8/9 19:19 创建定时任务创建发送心跳与刷新缓存的线程池初始化定时任务 可见,定时任务是以秒为单位,以设置的renewalIntervalInSecs为间隔.执行new HeartbeatThread(), 屏幕剪辑的捕获时间: 2018/8/9 19:31 这个类里面是执行了renew()方法. 服务注册 屏幕剪辑的捕获时间: 2018/8/9 19:33 EurekaTransport为DiscoveryClient的内部类.带有httpclient的属性,从而进行http请求.真正实现发送请求的类为: AbstractJerseyEurekaHttpClient.这个类里面有2个请求,分别为服务注册,续约和取消.这个类初始化的时候,会为每个client生成一个url, 格式为:”serviceUrl”,serviceUrl可以从DefaultEndpoint类中得知: 屏幕剪辑的捕获时间: 2018/8/9 20:13 格式为:http://networkAddress:port/[relateUri](可设置),networkAddress是从 屏幕剪辑的捕获时间: 2018/8/9 20:19 通过dns解析的集群地址. 屏幕剪辑的捕获时间: 2018/8/9 20:00 最后jeyseyclient发起post请求.进行注册. 服务的取消 :也是这个类进行操作, 屏幕剪辑的捕获时间: 2018/8/9 20:22 jeyseyclient发起delete请求. 服务的续约 屏幕剪辑的捕获时间: 2018/8/9 20:28 将实例的lastDirtyTimestamp时间戳进行更新.]]></content>
      <categories>
        <category>微服务</category>
        <category>spring cloud eureka</category>
      </categories>
      <tags>
        <tag>spring cloud eureka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式]]></title>
    <url>%2F2020%2F06%2F15%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[设计模式简介它是解决特定问题的一系列套路，是前辈们的代码设计经验的总结，具有一定的普遍性，可以反复使用。其目的是为了提高代码的可重用性、代码的可读性和代码的可靠性。 分类 范围\目的 创建型模式 结构型模式 行为型模式 类模式 工厂方法 (类）适配器 模板方法,解释器 对象模式 单例, 原型, 抽象工厂 ,建造者 代理 (对象）,适配器, 桥接, 装饰 ,外观, 享元, 组合 策略, 命令 ,职责链, 状态 ,观察者 ,中介者, 迭代器, 访问者, 备忘录 单例（Singleton）模式：某个类只能生成一个实例，该类提供了一个全局访问点供外部获取该实例，其拓展是有限多例模式。 原型（Prototype）模式：将一个对象作为原型，通过对其进行复制而克隆出多个和原型类似的新实例。 工厂方法（Factory Method）模式：定义一个用于创建产品的接口，由子类决定生产什么产品。 抽象工厂（AbstractFactory）模式：提供一个创建产品族的接口，其每个子类可以生产一系列相关的产品。 建造者（Builder）模式：将一个复杂对象分解成多个相对简单的部分，然后根据不同需要分别创建它们，最后构建成该复杂对象。 代理（Proxy）模式：为某对象提供一种代理以控制对该对象的访问。即客户端通过代理间接地访问该对象，从而限制、增强或修改该对象的一些特性。 适配器（Adapter）模式：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。 桥接（Bridge）模式：将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度。 装饰（Decorator）模式：动态的给对象增加一些职责，即增加其额外的功能。 外观（Facade）模式：为多个复杂的子系统提供一个一致的接口，使这些子系统更加容易被访问。 享元（Flyweight）模式：运用共享技术来有效地支持大量细粒度对象的复用。 组合（Composite）模式：将对象组合成树状层次结构，使用户对单个对象和组合对象具有一致的访问性。 模板方法（TemplateMethod）模式：定义一个操作中的算法骨架，而将算法的一些步骤延迟到子类中，使得子类可以不改变该算法结构的情况下重定义该算法的某些特定步骤。 策略（Strategy）模式：定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的改变不会影响使用算法的客户。 命令（Command）模式：将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。 职责链（Chain of Responsibility）模式：把请求从链中的一个对象传到下一个对象，直到请求被响应为止。通过这种方式去除对象之间的耦合。 状态（State）模式：允许一个对象在其内部状态发生改变时改变其行为能力。 观察者（Observer）模式：多个对象间存在一对多关系，当一个对象发生改变时，把这种改变通知给其他多个对象，从而影响其他对象的行为。 中介者（Mediator）模式：定义一个中介对象来简化原有对象之间的交互关系，降低系统中对象间的耦合度，使原有对象之间不必相互了解。 迭代器（Iterator）模式：提供一种方法来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 访问者（Visitor）模式：在不改变集合元素的前提下，为一个集合中的每个元素提供多种访问方式，即每个元素有多个访问者对象访问。 备忘录（Memento）模式：在不破坏封装性的前提下，获取并保存一个对象的内部状态，以便以后恢复它。 解释器（Interpreter）模式：提供如何定义语言的文法，以及对语言句子的解释方法，即解释器。 UML建模语言​ 统一建模语言（Unified Modeling Language，UML）是用来设计软件蓝图的可视化建模语言，1997 年被国际对象管理组织（OMG）采纳为面向对象的建模语言的国际标准。它的特点是简单、统一、图形化、能表达软件设计中的动态与静态信息。 ​ 统一建模语言能为软件开发的所有阶段提供模型化和可视化支持。而且融入了软件工程领域的新思想、新方法和新技术，使软件设计人员沟通更简明，进一步缩短了设计时间，减少开发成本。它的应用领域很宽，不仅适合于一般系统的开发，而且适合于并行与分布式系统的建模。 ​ UML 从目标系统的不同角度出发，定义了用例图、类图、对象图、状态图、活动图、时序图、协作图、构件图、部署图等 9 种图。 ​ 本教程主要介绍软件设计模式中经常用到的类图，以及类之间的关系。另外，在实验部分将简单介绍 UML 建模工具的使用方法，当前业界使用最广泛的是 Rational Rose。使用 Umlet 的人也很多，它是一个轻量级的开源 UML 建模工具，简单实用，常用于小型软件系统的开发与设计。 类、接口和类图注意：“可见性”表示该属性对类外的元素是否可见，包括公有（Public）、私有（Private）、受保护（Protected）和朋友（Friendly）4 种，在类图中分别用符号+、-、#、~表示。 类 接口 类图 类之间的关系依赖关系某个类的方法通过局部变量、方法的参数或者对静态方法的调用来访问另一个类（被依赖类）中的某些方法来完成一些职责。 关联关系一个类的对象作为另一个类的成员变量来实现关联关系. 聚合关系has-a 的关系. 组合关系类之间的整体与部分的关系. 泛化关系一般与特殊的关系,继承关系. 实现关系接口与实现类之间的关系. 软件设计原则开闭原则简介开闭原则（Open Closed Principle，OCP）由勃兰特·梅耶（Bertrand Meyer）提出，他在 1988 年的著作《面向对象软件构造》（Object Oriented Software Construction）中提出：软件实体应当对扩展开放，对修改关闭（Software entities should be open for extension，but closed for modification），这就是开闭原则的经典定义。 作用开闭原则是面向对象程序设计的终极目标，它使软件实体拥有一定的适应性和灵活性的同时具备稳定性和延续性。具体来说，其作用如下。 对软件测试的影响软件遵守开闭原则的话，软件测试时只需要对扩展的代码进行测试就可以了，因为原有的测试代码仍然能够正常运行。 可以提高代码的可复用性粒度越小，被复用的可能性就越大；在面向对象的程序设计中，根据原子和抽象编程可以提高代码的可复用性。 可以提高软件的可维护性遵守开闭原则的软件，其稳定性高和延续性强，从而易于扩展和维护。 实现方法可以通过“抽象约束、封装变化”来实现开闭原则，即通过接口或者抽象类为软件实体定义一个相对稳定的抽象层，而将相同的可变因素封装在相同的具体实现类中。 因为抽象灵活性好，适应性广，只要抽象的合理，可以基本保持软件架构的稳定。而软件中易变的细节可以从抽象派生来的实现类来进行扩展，当软件需要发生变化时，只需要根据需求重新派生一个实现类来扩展就可以了。 例子Windows 的主题是桌面背景图片、窗口颜色和声音等元素的组合。用户可以根据自己的喜爱更换自己的桌面主题，也可以从网上下载新的主题。这些主题有共同的特点，可以为其定义一个抽象类（Abstract Subject），而每个具体的主题（Specific Subject）是其子类。用户窗体可以根据需要选择或者增加新的主题，而不需要修改原代码，所以它是满足开闭原则的，其类图如图 1 所示。 里氏替换原则简介里氏替换原则（Liskov Substitution Principle，LSP）由麻省理工学院计算机科学实验室的里斯科夫（Liskov）女士在 1987 年的“面向对象技术的高峰会议”（OOPSLA）上发表的一篇文章《数据抽象和层次》（Data Abstraction and Hierarchy）里提出来的，她提出：继承必须确保超类所拥有的性质在子类中仍然成立（Inheritance should ensure that any property proved about supertype objects also holds for subtype objects）。 里氏替换原则主要阐述了有关继承的一些原则，也就是什么时候应该使用继承，什么时候不应该使用继承，以及其中蕴含的原理。里氏替换原是继承复用的基础，它反映了基类与子类之间的关系，是对开闭原则的补充，是对实现抽象化的具体步骤的规范。 作用 里氏替换原则是实现开闭原则的重要方式之一。 它克服了继承中重写父类造成的可复用性变差的缺点。 它是动作正确性的保证。即类的扩展不会给已有的系统引入新的错误，降低了代码出错的可能性。 实现方法里氏替换原则通俗来讲就是：子类可以扩展父类的功能，但不能改变父类原有的功能。也就是说：子类继承父类时，除添加新的方法完成新增功能外，尽量不要重写父类的方法。 例子鸟一般都会飞行，如燕子的飞行速度大概是每小时 120 千米。但是新西兰的几维鸟由于翅膀退化无法飞行。假如要设计一个实例，计算这两种鸟飞行 300 千米要花费的时间。显然，拿燕子来测试这段代码，结果正确，能计算出所需要的时间；但拿几维鸟来测试，结果会发生“除零异常”或是“无穷大”，明显不符合预期，其类图如图 1 所示。 依赖倒置原则简介依赖倒置原则（Dependence Inversion Principle，DIP）是 Object Mentor 公司总裁罗伯特·马丁（Robert C.Martin）于 1996 年在 C++ Report 上发表的文章。 依赖倒置原则的原始定义为：高层模块不应该依赖低层模块，两者都应该依赖其抽象；抽象不应该依赖细节，细节应该依赖抽象（High level modules shouldnot depend upon low level modules.Both should depend upon abstractions.Abstractions should not depend upon details. Details should depend upon abstractions）。其核心思想是：要面向接口编程，不要面向实现编程。 作用 依赖倒置原则可以降低类间的耦合性。 依赖倒置原则可以提高系统的稳定性。 依赖倒置原则可以减少并行开发引起的风险。 依赖倒置原则可以提高代码的可读性和可维护性。 实现方法依赖倒置原则的目的是通过要面向接口的编程来降低类间的耦合性，所以我们在实际编程中只要遵循以下4点，就能在项目中满足这个规则。 每个类尽量提供接口或抽象类，或者两者都具备。 变量的声明类型尽量是接口或者是抽象类。 任何类都不应该从具体类派生。 使用继承时尽量遵循里氏替换原则。 例子 代码实例: 1234567891011121314151617181920212223242526272829303132333435363738394041package principle;public class DIPtest&#123; public static void main(String[] args) &#123; Customer wang=new Customer(); System.out.println("顾客购买以下商品："); wang.shopping(new ShaoguanShop()); wang.shopping(new WuyuanShop()); &#125;&#125;//商店interface Shop&#123; public String sell(); //卖&#125;//韶关网店class ShaoguanShop implements Shop&#123; public String sell() &#123; return "韶关土特产：香菇、木耳……"; &#125; &#125;//婺源网店class WuyuanShop implements Shop&#123; public String sell() &#123; return "婺源土特产：绿茶、酒糟鱼……"; &#125;&#125; //顾客class Customer&#123; public void shopping(Shop shop) &#123; //购物 System.out.println(shop.sell()); &#125;&#125; 单一职责原则简介单一职责原则（Single Responsibility Principle，SRP）又称单一功能原则，由罗伯特·C.马丁（Robert C. Martin）于《敏捷软件开发：原则、模式和实践》一书中提出的。这里的职责是指类变化的原因，单一职责原则规定一个类应该有且仅有一个引起它变化的原因，否则类应该被拆分（There should never be more than one reason for a class to change）。 该原则提出对象不应该承担太多职责，如果一个对象承担了太多的职责，至少存在以下两个缺点： 一个职责的变化可能会削弱或者抑制这个类实现其他职责的能力； 当客户端需要该对象的某一个职责时，不得不将其他不需要的职责全都包含进来，从而造成冗余代码或代码的浪费。 作用单一职责原则的核心就是控制类的粒度大小、将对象解耦、提高其内聚性。如果遵循单一职责原则将有以下优点。 降低类的复杂度。一个类只负责一项职责，其逻辑肯定要比负责多项职责简单得多。 提高类的可读性。复杂性降低，自然其可读性会提高。 提高系统的可维护性。可读性提高，那自然更容易维护了。 变更引起的风险降低。变更是必然的，如果单一职责原则遵守得好，当修改一个功能时，可以显著降低对其他功能的影响。 实现方法单一职责原则是最简单但又最难运用的原则，需要设计人员发现类的不同职责并将其分离，再封装到不同的类或模块中。而发现类的多重职责需要设计人员具有较强的分析设计能力和相关重构经验。下面以大学学生工作管理程序为例介绍单一职责原则的应用。 例子大学学生工作主要包括学生生活辅导和学生学业指导两个方面的工作，其中生活辅导主要包括班委建设、出勤统计、心理辅导、费用催缴、班级管理等工作，学业指导主要包括专业引导、学习辅导、科研指导、学习总结等工作。如果将这些工作交给一位老师负责显然不合理，正确的做 法是生活辅导由辅导员负责，学业指导由学业导师负责，其类图如图 1 所示。 接口隔离原则简介接口隔离原则（Interface Segregation Principle，ISP）要求程序员尽量将臃肿庞大的接口拆分成更小的和更具体的接口，让接口中只包含客户感兴趣的方法。 2002 年罗伯特·C.马丁给“接口隔离原则”的定义是：客户端不应该被迫依赖于它不使用的方法（Clients should not be forced to depend on methods they do not use）。该原则还有另外一个定义：一个类对另一个类的依赖应该建立在最小的接口上（The dependency of one class to another one should depend on the smallest possible interface）。 以上两个定义的含义是：要为各个类建立它们需要的专用接口，而不要试图去建立一个很庞大的接口供所有依赖它的类去调用。 作用接口隔离原则是为了约束接口、降低类对接口的依赖性，遵循接口隔离原则有以下 5 个优点。 将臃肿庞大的接口分解为多个粒度小的接口，可以预防外来变更的扩散，提高系统的灵活性和可维护性。 接口隔离提高了系统的内聚性，减少了对外交互，降低了系统的耦合性。 如果接口的粒度大小定义合理，能够保证系统的稳定性；但是，如果定义过小，则会造成接口数量过多，使设计复杂化；如果定义太大，灵活性降低，无法提供定制服务，给整体项目带来无法预料的风险。 使用多个专门的接口还能够体现对象的层次，因为可以通过接口的继承，实现对总接口的定义。 能减少项目工程中的代码冗余。过大的大接口里面通常放置许多不用的方法，当实现这个接口的时候，被迫设计冗余的代码。 实现方法在具体应用接口隔离原则时，应该根据以下几个规则来衡量。 接口尽量小，但是要有限度。一个接口只服务于一个子模块或业务逻辑。 为依赖接口的类定制服务。只提供调用者需要的方法，屏蔽不需要的方法。 了解环境，拒绝盲从。每个项目或产品都有选定的环境因素，环境不同，接口拆分的标准就不同深入了解业务逻辑。 提高内聚，减少对外交互。使接口用最少的方法去完成最多的事情。 例子学生成绩管理程序一般包含插入成绩、删除成绩、修改成绩、计算总分、计算均分、打印成绩信息、査询成绩信息等功能，如果将这些功能全部放到一个接口中显然不太合理，正确的做法是将它们分别放在输入模块、统计模块和打印模块等 3 个模块中，其类图如图 1 所示。 迪米特法则简介迪米特法则（Law of Demeter，LoD）又叫作最少知识原则（Least Knowledge Principle，LKP)，产生于 1987 年美国东北大学（Northeastern University）的一个名为迪米特（Demeter）的研究项目，由伊恩·荷兰（Ian Holland）提出，被 UML 创始者之一的布奇（Booch）普及，后来又因为在经典著作《程序员修炼之道》（The Pragmatic Programmer）提及而广为人知。 迪米特法则的定义是：只与你的直接朋友交谈，不跟“陌生人”说话（Talk only to your immediate friends and not to strangers）。其含义是：如果两个软件实体无须直接通信，那么就不应当发生直接的相互调用，可以通过第三方转发该调用。其目的是降低类之间的耦合度，提高模块的相对独立性。 迪米特法则中的“朋友”是指：当前对象本身、当前对象的成员对象、当前对象所创建的对象、当前对象的方法参数等，这些对象同当前对象存在关联、聚合或组合关系，可以直接访问这些对象的方法。 作用迪米特法则要求限制软件实体之间通信的宽度和深度，正确使用迪米特法则将有以下两个优点。 降低了类之间的耦合度，提高了模块的相对独立性。 由于亲合度降低，从而提高了类的可复用率和系统的扩展性。 但是，过度使用迪米特法则会使系统产生大量的中介类，从而增加系统的复杂性，使模块之间的通信效率降低。所以，在釆用迪米特法则时需要反复权衡，确保高内聚和低耦合的同时，保证系统的结构清晰。 实现方法从迪米特法则的定义和特点可知，它强调以下两点： 从依赖者的角度来说，只依赖应该依赖的对象。 从被依赖者的角度说，只暴露应该暴露的方法。 所以，在运用迪米特法则时要注意以下 6 点。 在类的划分上，应该创建弱耦合的类。类与类之间的耦合越弱，就越有利于实现可复用的目标。 在类的结构设计上，尽量降低类成员的访问权限。 在类的设计上，优先考虑将一个类设置成不变类。 在对其他类的引用上，将引用其他对象的次数降到最低。 不暴露类的属性成员，而应该提供相应的访问器（set 和 get 方法）。 谨慎使用序列化（Serializable）功能。 例子明星由于全身心投入艺术，所以许多日常事务由经纪人负责处理，如与粉丝的见面会，与媒体公司的业务洽淡等。这里的经纪人是明星的朋友，而粉丝和媒体公司是陌生人，所以适合使用迪米特法则，其类图如图 1 所示. 合成复用原则合成复用原则（Composite Reuse Principle，CRP）又叫组合/聚合复用原则（Composition/Aggregate Reuse Principle，CARP）。它要求在软件复用时，要尽量先使用组合或者聚合等关联关系来实现，其次才考虑使用继承关系来实现。 作用通常类的复用分为继承复用和合成复用两种，继承复用虽然有简单和易实现的优点，但它也存在以下缺点。 继承复用破坏了类的封装性。因为继承会将父类的实现细节暴露给子类，父类对子类是透明的，所以这种复用又称为“白箱”复用。 子类与父类的耦合度高。父类的实现的任何改变都会导致子类的实现发生变化，这不利于类的扩展与维护。 它限制了复用的灵活性。从父类继承而来的实现是静态的，在编译时已经定义，所以在运行时不可能发生变化。 采用组合或聚合复用时，可以将已有对象纳入新对象中，使之成为新对象的一部分，新对象可以调用已有对象的功能，它有以下优点。 它维持了类的封装性。因为成分对象的内部细节是新对象看不见的，所以这种复用又称为“黑箱”复用。 新旧类之间的耦合度低。这种复用所需的依赖较少，新对象存取成分对象的唯一方法是通过成分对象的接口。 复用的灵活性高。这种复用可以在运行时动态进行，新对象可以动态地引用与成分对象类型相同的对象。 实现方法合成复用原则是通过将已有的对象纳入新对象中，作为新对象的成员对象来实现的，新对象可以调用已有对象的功能，从而达到复用。 例子汽车按“动力源”划分可分为汽油汽车、电动汽车等；按“颜色”划分可分为白色汽车、黑色汽车和红色汽车等。如果同时考虑这两种分类，其组合就很多。 总结这 7 种设计原则是软件设计模式必须尽量遵循的原则，各种原则要求的侧重点不同。其中，开闭原则是总纲，它告诉我们要对扩展开放，对修改关闭；里氏替换原则告诉我们不要破坏继承体系；依赖倒置原则告诉我们要面向接口编程；单一职责原则告诉我们实现类要职责单一；接口隔离原则告诉我们在设计接口的时候要精简单一；迪米特法则告诉我们要降低耦合度；合成复用原则告诉我们要优先使用组合或者聚合关系复用，少用继承关系复用。 创建型设计模式特点:将对象的创建与使用分离 单例模式简介单例类只有一个实例对象.根据effective java建议,使用枚举实现.原因如下: 其他方法可以通过反射方式破坏单例.想要阻止,需要在构造方法中进行判断,拦截. 如果实现了系列化,还需要重写readResolve()方法.防止通过反序列化方式破坏单例. 代码实现123456789101112public enum Single &#123; INSTANCE; private Object object; Single() &#123; object = new Object(); &#125; public Object getObject() &#123; return object; &#125;&#125; 原型模式简介在有些系统中，存在大量相同或相似对象的创建问题，如果用传统的构造函数来创建对象，会比较复杂且耗时耗资源，用原型模式生成对象就很高效，就像孙悟空拔下猴毛轻轻一吹就变出很多孙悟空一样简单。 实现由于 Java提供了对象的 clone() 方法，浅克隆,所以用 Java 实现原型模式很简单. 图例 原型模式的扩展在原型模式的基础上,增加一个原型管理器PrototypeManager 类. 工厂方法模式简介工厂方法（FactoryMethod）模式的定义：定义一个创建产品对象的工厂接口，将产品对象的实际创建工作推迟到具体子工厂类当中。这满足创建型模式中所要求的“创建与使用相分离”的特点。 我们把被创建的对象称为“产品”，把创建产品的对象称为“工厂”。如果要创建的产品不多，只要一个工厂类就可以完成，这种模式叫“简单工厂模式”，它不属于 GoF 的 23 种经典设计模式，它的缺点是增加新产品时会违背“开闭原则”。 本节介绍的“工厂方法模式”是对简单工厂模式的进一步抽象化，其好处是可以使系统在不修改原来代码的情况下引进新的产品，即满足开闭原则。 特点工厂方法模式的主要优点有： 用户只需要知道具体工厂的名称就可得到所要的产品，无须知道产品的具体创建过程； 在系统增加新的产品时只需要添加具体产品类和对应的具体工厂类，无须对原工厂进行任何修改，满足开闭原则； 其缺点是：每增加一个产品就要增加一个具体产品类和一个对应的具体工厂类，这增加了系统的复杂度。 图例 简单工厂模式当需要生成的产品不多且不会增加，一个具体工厂类就可以完成任务时，可删除抽象工厂类。这时工厂方法模式将退化到简单工厂模式. 抽象工厂模式简介工厂方法模式只考虑生产同等级的产品,但是我们会经常遇到综合型工厂,即农场里不光养动物,还可以种植植物.这个时候就要考虑多个等级产品的生产:抽象工厂模式. 抽象工厂模式是工厂方法模式的升级版本，工厂方法模式只生产一个等级的产品，而抽象工厂模式可生产多个等级的产品。 使用抽象工厂模式一般要满足以下条件。 系统中有多个产品族，每个具体工厂创建同一族但属于不同等级结构的产品。 系统一次只可能消费其中某一族产品，即同族的产品一起使用。 特点抽象工厂模式除了具有工厂方法模式的优点外，其他主要优点如下。 可以在类的内部对产品族中相关联的多等级产品共同管理，而不必专门引入多个新的类来进行管理。 当增加一个新的产品族时不需要修改原代码，满足开闭原则。 其缺点是：当产品族中需要增加一个新的产品时，所涉及到的工厂类都需要进行修改。 图例 抽象工厂模式的扩展有一定的“开闭原则”倾斜性： 当增加一个新的产品族时只需增加一个新的具体工厂，不需要修改原代码，满足开闭原则。 当产品族中需要增加一个新种类的产品时，则所有的工厂类都需要进行修改，不满足开闭原则。 建造者模式（Bulider模式）简介将一个复杂对象的构造与它的表示分离，使同样的构建过程可以创建不同的表示. 特点该模式的主要优点如下： 各个具体的建造者相互独立，有利于系统的扩展。 客户端不必知道产品内部组成的细节，便于控制细节风险。 其缺点如下： 产品的组成部分必须相同，这限制了其使用范围。 如果产品的内部变化复杂，该模式会增加很多的建造者类。 图例 结构型设计模式结构型模式描述如何将类或对象按某种布局组成更大的结构。它分为类结构型模式和对象结构型模式，前者采用继承机制来组织接口和类，后者釆用组合或聚合来组合对象。 代理模式简介代理模式的定义：由于某些原因需要给某对象提供一个代理以控制对该对象的访问。这时，访问对象不适合或者不能直接引用目标对象，代理对象作为访问对象和目标对象之间的中介。 特点代理模式的主要优点有： 代理模式在客户端与目标对象之间起到一个中介作用和保护目标对象的作用； 代理对象可以扩展目标对象的功能； 代理模式能将客户端与目标对象分离，在一定程度上降低了系统的耦合度； 其主要缺点是： 在客户端和目标对象之间增加一个代理对象，会造成请求处理速度变慢； 增加了系统的复杂度； 图例 适配器模式（Adapter模式）简介适配器模式（Adapter）的定义如下：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。适配器模式分为类结构型模式和对象结构型模式两种，前者类之间的耦合度比后者高，且要求程序员了解现有组件库中的相关组件的内部结构，所以应用相对较少些。 特点该模式的主要优点如下。 客户端通过适配器可以透明地调用目标接口。 复用了现存的类，程序员不需要修改原有代码而重用现有的适配者类。 将目标类和适配者类解耦，解决了目标类和适配者类接口不一致的问题。 其缺点是：对类适配器来说，更换适配器的实现过程比较复杂。 图例类适配器模式的结构图. 对象适配器模式的结构图. 桥接模式简介将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度 特点桥接（Bridge）模式的优点是： 由于抽象与实现分离，所以扩展能力强； 其实现细节对客户透明。 缺点是：由于聚合关系建立在抽象层，要求开发者针对抽象化进行设计与编程，这增加了系统的理解与设计难度。 图例 装饰模式简介在不改变现有对象结构的情况下，动态地给该对象增加一些职责（即增加其额外功能）的模式，它属于对象结构型模式。 特点装饰（Decorator）模式的主要优点有： 采用装饰模式扩展对象的功能比采用继承方式更加灵活。 可以设计出多个不同的具体装饰类，创造出多个不同行为的组合。 其主要缺点是：装饰模式增加了许多子类，如果过度使用会使程序变得很复杂。 图例 外观模式简介种通过为多个复杂的子系统提供一个一致的接口，而使这些子系统更加容易被访问的模式。该模式对外有一个统一接口，外部应用程序不用关心内部子系统的具体的细节，这样会大大降低应用程序的复杂度，提高了程序的可维护性。 特点主要优点。 降低了子系统与客户端之间的耦合度，使得子系统的变化不会影响调用它的客户类。 对客户屏蔽了子系统组件，减少了客户处理的对象数目，并使得子系统使用起来更加容易。 降低了大型软件系统中的编译依赖性，简化了系统在不同平台之间的移植过程，因为编译一个子系统不会影响其他的子系统，也不会影响外观对象。 外观（Facade）模式的主要缺点如下。 不能很好地限制客户使用子系统类。 增加新的子系统可能需要修改外观类或客户端的源代码，违背了“开闭原则”。 图例 享元模式简介运用共享技术来有効地支持大量细粒度对象的复用。它通过共享已经存在的对象来大幅度减少需要创建的对象数量、避免大量相似类的开销，从而提高系统资源的利用率。 特点主要优点是：相同对象只要保存一份，这降低了系统中对象的数量，从而降低了系统中细粒度对象给内存带来的压力。 其主要缺点是： 为了使对象可以共享，需要将一些不能共享的状态外部化，这将增加程序的复杂性。 读取享元模式的外部状态会使得运行时间稍微变长。 图例 组合模式简介是一种将对象组合成树状的层次结构的模式，用来表示“部分-整体”的关系，使用户对单个对象和组合对象具有一致的访问性。 特点组合模式的主要优点有： 组合模式使得客户端代码可以一致地处理单个对象和组合对象，无须关心自己处理的是单个对象，还是组合对象，这简化了客户端代码； 更容易在组合体内加入新的对象，客户端不会因为加入了新的对象而更改源代码，满足“开闭原则”； 其主要缺点是： 设计较复杂，客户端需要花更多时间理清类之间的层次关系； 不容易限制容器中的构件； 不容易用继承的方法来增加构件的新功能； 图例 行为型模式行为型模式分为类行为模式和对象行为模式，前者采用继承机制来在类间分派行为，后者采用组合或聚合在对象间分配行为。由于组合关系或聚合关系比继承关系耦合度低，满足“合成复用原则”，所以对象行为模式比类行为模式具有更大的灵活性。 模板方法模式简介模板方法（Template Method）模式的定义如下：定义一个操作中的算法骨架，而将算法的一些步骤延迟到子类中，使得子类可以不改变该算法结构的情况下重定义该算法的某些特定步骤。它是一种类行为型模式。 特点该模式的主要优点如下。 它封装了不变部分，扩展可变部分。它把认为是不变部分的算法封装到父类中实现，而把可变部分算法由子类继承实现，便于子类继续扩展。 它在父类中提取了公共的部分代码，便于代码复用。 部分方法是由子类实现的，因此子类可以通过扩展方式增加相应的功能，符合开闭原则。 该模式的主要缺点如下。 对每个不同的实现都需要定义一个子类，这会导致类的个数增加，系统更加庞大，设计也更加抽象。 父类中的抽象方法由子类实现，子类执行的结果会影响父类的结果，这导致一种反向的控制结构，它提高了代码阅读的难度。 图例 策略模式简介策略（Strategy）模式的定义：该模式定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的变化不会影响使用算法的客户。策略模式属于对象行为模式，它通过对算法进行封装，把使用算法的责任和算法的实现分割开来，并委派给不同的对象对这些算法进行管理。 特点策略模式的主要优点如下。 多重条件语句不易维护，而使用策略模式可以避免使用多重条件语句。 策略模式提供了一系列的可供重用的算法族，恰当使用继承可以把算法族的公共代码转移到父类里面，从而避免重复的代码。 策略模式可以提供相同行为的不同实现，客户可以根据不同时间或空间要求选择不同的。 策略模式提供了对开闭原则的完美支持，可以在不修改原代码的情况下，灵活增加新算法。 策略模式把算法的使用放到环境类中，而算法的实现移到具体策略类中，实现了二者的分离。 其主要缺点如下。 客户端必须理解所有策略算法的区别，以便适时选择恰当的算法类。 策略模式造成很多的策略类。 图例 命令模式简介命令（Command）模式的定义如下：将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。这样两者之间通过命令对象进行沟通，这样方便将命令对象进行储存、传递、调用、增加与管理。 特点命令模式的主要优点如下。 降低系统的耦合度。命令模式能将调用操作的对象与实现该操作的对象解耦。 增加或删除命令非常方便。采用命令模式增加与删除命令不会影响其他类，它满足“开闭原则”，对扩展比较灵活。 可以实现宏命令。命令模式可以与组合模式结合，将多个命令装配成一个组合命令，即宏命令。 方便实现 Undo 和 Redo 操作。命令模式可以与后面介绍的备忘录模式结合，实现命令的撤销与恢复。 其缺点是：可能产生大量具体命令类。因为计对每一个具体操作都需要设计一个具体命令类，这将增加系统的复杂性。 图例 责任链模式简介责任链（Chain of Responsibility）模式的定义：为了避免请求发送者与多个请求处理者耦合在一起，将所有请求的处理者通过前一对象记住其下一个对象的引用而连成一条链；当有请求发生时，可将请求沿着这条链传递，直到有对象处理它为止。 注意：责任链模式也叫职责链模式。 特点在责任链模式中，客户只需要将请求发送到责任链上即可，无须关心请求的处理细节和请求的传递过程，所以责任链将请求的发送者和请求的处理者解耦了。 责任链模式是一种对象行为型模式，其主要优点如下。 降低了对象之间的耦合度。该模式使得一个对象无须知道到底是哪一个对象处理其请求以及链的结构，发送者和接收者也无须拥有对方的明确信息。 增强了系统的可扩展性。可以根据需要增加新的请求处理类，满足开闭原则。 增强了给对象指派职责的灵活性。当工作流程发生变化，可以动态地改变链内的成员或者调动它们的次序，也可动态地新增或者删除责任。 责任链简化了对象之间的连接。每个对象只需保持一个指向其后继者的引用，不需保持其他所有处理者的引用，这避免了使用众多的 if 或者 if···else 语句。 责任分担。每个类只需要处理自己该处理的工作，不该处理的传递给下一个对象完成，明确各类的责任范围，符合类的单一职责原则。 其主要缺点如下。 不能保证每个请求一定被处理。由于一个请求没有明确的接收者，所以不能保证它一定会被处理，该请求可能一直传到链的末端都得不到处理。 对比较长的职责链，请求的处理可能涉及多个处理对象，系统性能将受到一定影响。 职责链建立的合理性要靠客户端来保证，增加了客户端的复杂性，可能会由于职责链的错误设置而导致系统出错，如可能会造成循环调用。 图例 责任链: 状态模式简介状态（State）模式的定义：对有状态的对象，把复杂的“判断逻辑”提取到不同的状态对象中，允许状态对象在其内部状态发生改变时改变其行为。 特点状态模式是一种对象行为型模式，其主要优点如下。 状态模式将与特定状态相关的行为局部化到一个状态中，并且将不同状态的行为分割开来，满足“单一职责原则”。 减少对象间的相互依赖。将不同的状态引入独立的对象中会使得状态转换变得更加明确，且减少对象间的相互依赖。 有利于程序的扩展。通过定义新的子类很容易地增加新的状态和转换。 状态模式的主要缺点如下。 状态模式的使用必然会增加系统的类与对象的个数。 状态模式的结构与实现都较为复杂，如果使用不当会导致程序结构和代码的混乱。 图例 观察者模式简介观察者（Observer）模式的定义：指多个对象间存在一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。这种模式有时又称作发布-订阅模式、模型-视图模式，它是对象行为型模式。 特点观察者模式是一种对象行为型模式，其主要优点如下。 降低了目标与观察者之间的耦合关系，两者之间是抽象耦合关系。 目标与观察者之间建立了一套触发机制。 它的主要缺点如下。 目标与观察者之间的依赖关系并没有完全解除，而且有可能出现循环引用。 当观察者对象很多时，通知的发布会花费很多时间，影响程序的效率。 图例 中介者模式简介中介者（Mediator）模式的定义：定义一个中介对象来封装一系列对象之间的交互，使原有对象之间的耦合松散，且可以独立地改变它们之间的交互。中介者模式又叫调停模式，它是迪米特法则的典型应用。 特点中介者模式是一种对象行为型模式，其主要优点如下。 降低了对象之间的耦合性，使得对象易于独立地被复用。 将对象间的一对多关联转变为一对一的关联，提高系统的灵活性，使得系统易于维护和扩展。 其主要缺点是：当同事类太多时，中介者的职责将很大，它会变得复杂而庞大，以至于系统难以维护。 图例 迭代器模式简介迭代器（Iterator）模式的定义：提供一个对象来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 特点迭代器模式是一种对象行为型模式，其主要优点如下。 访问一个聚合对象的内容而无须暴露它的内部表示。 遍历任务交由迭代器完成，这简化了聚合类。 它支持以不同方式遍历一个聚合，甚至可以自定义迭代器的子类以支持新的遍历。 增加新的聚合类和迭代器类都很方便，无须修改原有代码。 封装性良好，为遍历不同的聚合结构提供一个统一的接口。 其主要缺点是：增加了类的个数，这在一定程度上增加了系统的复杂性。 图例 访问者模式简介访问者（Visitor）模式的定义：将作用于某种数据结构中的各元素的操作分离出来封装成独立的类，使其在不改变数据结构的前提下可以添加作用于这些元素的新的操作，为数据结构中的每个元素提供多种访问方式。它将对数据的操作与数据结构进行分离，是行为类模式中最复杂的一种模式。 特点访问者（Visitor）模式是一种对象行为型模式，其主要优点如下。 扩展性好。能够在不修改对象结构中的元素的情况下，为对象结构中的元素添加新的功能。 复用性好。可以通过访问者来定义整个对象结构通用的功能，从而提高系统的复用程度。 灵活性好。访问者模式将数据结构与作用于结构上的操作解耦，使得操作集合可相对自由地演化而不影响系统的数据结构。 符合单一职责原则。访问者模式把相关的行为封装在一起，构成一个访问者，使每一个访问者的功能都比较单一。 访问者（Visitor）模式的主要缺点如下。 增加新的元素类很困难。在访问者模式中，每增加一个新的元素类，都要在每一个具体访问者类中增加相应的具体操作，这违背了“开闭原则”。 破坏封装。访问者模式中具体元素对访问者公布细节，这破坏了对象的封装性。 违反了依赖倒置原则。访问者模式依赖了具体类，而没有依赖抽象类。 图例 备忘录模式简介备忘录（Memento）模式的定义：在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，以便以后当需要时能将该对象恢复到原先保存的状态。该模式又叫快照模式。 特点备忘录模式是一种对象行为型模式，其主要优点如下。 提供了一种可以恢复状态的机制。当用户需要时能够比较方便地将数据恢复到某个历史的状态。 实现了内部状态的封装。除了创建它的发起人之外，其他对象都不能够访问这些状态信息。 简化了发起人类。发起人不需要管理和保存其内部状态的各个备份，所有状态信息都保存在备忘录中，并由管理者进行管理，这符合单一职责原则。 其主要缺点是：资源消耗大。如果要保存的内部状态信息过多或者特别频繁，将会占用比较大的内存资源。 图例 解释器模式简介解释器（Interpreter）模式的定义：给分析对象定义一个语言，并定义该语言的文法表示，再设计一个解析器来解释语言中的句子。也就是说，用编译语言的方式来分析应用中的实例。这种模式实现了文法表达式处理的接口，该接口解释一个特定的上下文。 特点解释器模式是一种类行为型模式，其主要优点如下。 扩展性好。由于在解释器模式中使用类来表示语言的文法规则，因此可以通过继承等机制来改变或扩展文法。 容易实现。在语法树中的每个表达式节点类都是相似的，所以实现其文法较为容易。 解释器模式的主要缺点如下。 执行效率较低。解释器模式中通常使用大量的循环和递归调用，当要解释的句子较复杂时，其运行速度很慢，且代码的调试过程也比较麻烦。 会引起类膨胀。解释器模式中的每条规则至少需要定义一个类，当包含的文法规则很多时，类的个数将急剧增加，导致系统难以管理与维护。 可应用的场景比较少。在软件开发中，需要定义语言文法的应用实例非常少，所以这种模式很少被使用到。 图例]]></content>
      <categories>
        <category>java基础</category>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线上故障定位与修复]]></title>
    <url>%2F2020%2F04%2F26%2F%E7%BA%BF%E4%B8%8A%E6%95%85%E9%9A%9C%E5%AE%9A%E4%BD%8D%E4%B8%8E%E4%BF%AE%E5%A4%8D%2F</url>
    <content type="text"><![CDATA[线上应用故障分析及查看简介:一般情况下,我们运行在线上的应用总会出现一些问题,这个时候需要快速定位及解决故障,以下列出了一些由浅到深的解决方案. 日志分析一般情况下,如果日志记录的足够好的话,能找出大部分线上故障的原因. 服务器指标粗略查看系统CPU和内存1top 线程资源1top -H -p &lt;进程&gt; 硬盘使用情况1du -h 找出系统中大文件 1du -h --maxdepth=1 利用JDK分析(1.8)jstat :jvm状况12//查看jvm启动时的参数.jstat -v jstat: jvm统计信息监控工具.123格式: jstat &lt;commond&gt; interval count//每隔250ms,输出堆使用状况百分比,输出20次.例: jstat -gcutil 250 20 命令 说明 -class 监视类的装载,卸载数量,总空间及类装载所耗时间. -gc 监控堆的状况.包括eden,survivor,olden,永久代等具体信息. -gccapacity 监控内容和-gc相同,但是输出内存使用的最大和最小空间. -gcutil 监控内容和-gc相同,但是输出的是已使用空间与总空间的百分比. -gccause 监控内容和-gcutil相同,但是会额外输出导致上一次GC的原因. -gcnew 监控新生代的gc状况. -gcnewcapacity 监控内容和-gcnew相同,但是输出最大空间和已使用空间. -gcold 监控老年代的状况. -gcoldcapatity 监控内容和-gcold相同,但是输出已使用空间和最大空间. -gcpermcapacity 监控老年代的已使用空间和最大空间. -compoler 输出JIT编译期编译过的方法和耗时. -printcompilation 输出已经被JIT编译期编译过的方法. jinfojinfo 详细查看jvm的信息. jmap生成堆内存转储对象. 1jmap -dump:format=b,file=heapdump.phrof &lt;pid&gt; jhat分析headdump文件 1jhat xxx.headdump jstackjava堆栈跟踪工具. 可视化工具Jconsole: jstat和jstat图形化界面. JvisualVM:多功能图形化工具.多插件. 打开jdk目录下的jvisualvm,连接本地或者线上项目,可以看到CPU,head,Thread等运行情况. 生成headdump文件,查看类的大小等信息,结合代码分析. 结合Arthas分析常用命令 reset命令1234还原指定类,例:reset Test.reset &lt;类名&gt;还原所有增强的类.rest JVM(重点) 系统的实时数据面板,采样间隔默认100ms. 1dashboard 查看全部线程. 1thread 查看最忙的前3个线程. 1thread -n 3 查看指定线程id的堆栈. 1thread id 找出当前阻塞其他线程的线程.只支持使用synchronized关键字锁住的线程,暂不支持用retentlock锁住的. 1thread -b 指定采样间隔1000ms. 1thread -n 3 -i 1000 查看指定状态的线程 1thread --state WAITING logger 查看日志打印策略即classLoaderHash,后续要使用. 1logger 查看指定名字的logger信息. 1logger -n com.xxx.xxx.controller 查看指定classLoader的logger信息. 1logger -c &lt;classLoaderHash&gt; 更新logger的level. 1logger -c &lt;classLoaderHash&gt; --name ROOT --level debug 查看没有appender的logger的信息. 1logger --include-no-appender sc查看JVM已加载的类信息. 模糊搜索. 1sc com.xxx.* 打印类的详细信息. 1sc -d com.xxx.controller 打印类的Field的信息. 1sc -d -f com.xxx.controller sm查看已加载类的方法信息. 查看方法类别. 1sm com.xxx.controller 查看详细信息. 1sm -d com.xxx.controller heapdump查看堆内存信息 使用atrhas 输出到文件. 1heapdump /tmp/dump.hprof 只dump live对象. 1heapdump --live /tmp/dump.hprof 输出到临时目录 1heapdump 使用JDK 输出到文件. 1jmap -dump:live,format=b,file=heap-dump.bin &lt;pid&gt; jconsole生成. 分析文件使用命令 1jhat &lt;heap-dump-file&gt; 然后访问 http:localhost:7000 查看. jad反编译指定已加载类的源码. 反编译. 1jad com.xxx.service 反编译时只显示源码. 1jad --source-only com.xxx.service 反编译指定函数. 1jad com.xxx.service main 反编译时指定classLoader. 当有多个 ClassLoader 都加载了这个类时，jad 命令会输出对应 ClassLoader 实例的 hashcode，然后你只需要重新执行 jad 命令，并使用参数 -c 就可以反编译指定 ClassLoader 加载的那个类了； 1jad -c &lt;hash&gt; com.xxx.service classloader查看classloader的继承树，urls，类加载信息 按照类加载类型查看统计信息. 1classloader 按类加载实例查看. 1classloader -l 查看ClassLoader的继承树. 1classloader -t 查看URLClassLoader实际的urls. 1classloader -c &lt;hash&gt; 使用ClassLoader去查找resource. 1classloader -c 3d4eac69 -r META-INF/MANIFEST.MF 使用ClassLoader去加载类. 1classloader -c 3d4eac69 --load demo.MathGame monitor​ 方法执行监控. 每隔5s去统计类中方法的调用.不过本地测试时没有发现qps,rt等统计返回. 1monitor -c 5 demo.MathGame primeFactors watch(重点)方法执行数据观测,这点实用性强.因此做了全文摘录,方便查看. 让你能方便的观察到指定方法的调用情况。能观察到的范围为：返回值、抛出异常、入参，通过编写 OGNL 表达式进行对应变量的查看。 参数名称 参数说明 class-pattern 类名表达式匹配 method-pattern 方法名表达式匹配 express 观察表达式 condition-express 条件表达式 [b] 在方法调用之前观察 [e] 在方法异常之后观察 [s] 在方法返回之后观察 [f] 在方法结束之后(正常返回和异常返回)观察 [E] 开启正则表达式匹配，默认为通配符匹配 [x:] 指定输出结果的属性遍历深度，默认为 1 这里重点要说明的是观察表达式，观察表达式的构成主要由 ognl 表达式组成，所以你可以这样写&quot;{params,returnObj}&quot;，只要是一个合法的 ognl 表达式，都能被正常支持。 特别说明： watch 命令定义了4个观察事件点，即 -b 方法调用前，-e 方法异常后，-s 方法返回后，-f 方法结束后 4个观察事件点 -b、-e、-s 默认关闭，-f 默认打开，当指定观察点被打开后，在相应事件点会对观察表达式进行求值并输出 这里要注意方法入参和方法出参的区别，有可能在中间被修改导致前后不一致，除了 -b 事件点 params 代表方法入参外，其余事件都代表方法出参 当使用 -b 时，由于观察事件点是在方法调用前，此时返回值或异常均不存在 观察方法出参和返回值. 1234567891011121314$ watch demo.MathGame primeFactors &quot;&#123;params,returnObj&#125;&quot; -x 2Press Ctrl+C to abort.Affect(class-cnt:1 , method-cnt:1) cost in 44 ms.ts=2018-12-03 19:16:51; [cost=1.280502ms] result=@ArrayList[ @Object[][ @Integer[535629513], ], @ArrayList[ @Integer[3], @Integer[19], @Integer[191], @Integer[49199], ],] 观察方法入参. 123456789$ watch demo.MathGame primeFactors &quot;&#123;params,returnObj&#125;&quot; -x 2 -bPress Ctrl+C to abort.Affect(class-cnt:1 , method-cnt:1) cost in 50 ms.ts=2018-12-03 19:23:23; [cost=0.0353ms] result=@ArrayList[ @Object[][ @Integer[-1077465243], ], null,] 同时观察方法调用前和方法返回后. 1234567891011121314151617181920212223242526272829303132$ watch demo.MathGame primeFactors &quot;&#123;params,target,returnObj&#125;&quot; -x 2 -b -s -n 2Press Ctrl+C to abort.Affect(class-cnt:1 , method-cnt:1) cost in 46 ms.ts=2018-12-03 19:29:54; [cost=0.01696ms] result=@ArrayList[ @Object[][ @Integer[1544665400], ], @MathGame[ random=@Random[java.util.Random@522b408a], illegalArgumentCount=@Integer[13038], ], null,]ts=2018-12-03 19:29:54; [cost=4.277392ms] result=@ArrayList[ @Object[][ @Integer[1544665400], ], @MathGame[ random=@Random[java.util.Random@522b408a], illegalArgumentCount=@Integer[13038], ], @ArrayList[ @Integer[2], @Integer[2], @Integer[2], @Integer[5], @Integer[5], @Integer[73], @Integer[241], @Integer[439], ],] 参数里-n 2，表示只执行两次 这里输出结果中，第一次输出的是方法调用前的观察表达式的结果，第二次输出的是方法返回后的表达式的结果 结果的输出顺序和事件发生的先后顺序一致，和命令中 -s -b 的顺序无关 调整-x的值，观察具体的方法参数值. 12345678910111213141516171819202122232425262728$ watch demo.MathGame primeFactors &quot;&#123;params,target&#125;&quot; -x 3Press Ctrl+C to abort.Affect(class-cnt:1 , method-cnt:1) cost in 58 ms.ts=2018-12-03 19:34:19; [cost=0.587833ms] result=@ArrayList[ @Object[][ @Integer[47816758], ], @MathGame[ random=@Random[ serialVersionUID=@Long[3905348978240129619], seed=@AtomicLong[3133719055989], multiplier=@Long[25214903917], addend=@Long[11], mask=@Long[281474976710655], DOUBLE_UNIT=@Double[1.1102230246251565E-16], BadBound=@String[bound must be positive], BadRange=@String[bound must be greater than origin], BadSize=@String[size must be non-negative], seedUniquifier=@AtomicLong[-3282039941672302964], nextNextGaussian=@Double[0.0], haveNextNextGaussian=@Boolean[false], serialPersistentFields=@ObjectStreamField[][isEmpty=false;size=3], unsafe=@Unsafe[sun.misc.Unsafe@2eaa1027], seedOffset=@Long[24], ], illegalArgumentCount=@Integer[13159], ],] 条件表达式的例子. 1234567$ watch demo.MathGame primeFactors &quot;&#123;params[0],target&#125;&quot; &quot;params[0]&lt;0&quot;Press Ctrl+C to abort.Affect(class-cnt:1 , method-cnt:1) cost in 68 ms.ts=2018-12-03 19:36:04; [cost=0.530255ms] result=@ArrayList[ @Integer[-18178089], @MathGame[demo.MathGame@41cf53f9],] 观察异常信息的例子. 1234567891011$ watch demo.MathGame primeFactors &quot;&#123;params[0],throwExp&#125;&quot; -e -x 2Press Ctrl+C to abort.Affect(class-cnt:1 , method-cnt:1) cost in 62 ms.ts=2018-12-03 19:38:00; [cost=1.414993ms] result=@ArrayList[ @Integer[-1120397038], java.lang.IllegalArgumentException: number is: -1120397038, need &gt;= 2 at demo.MathGame.primeFactors(MathGame.java:46) at demo.MathGame.run(MathGame.java:24) at demo.MathGame.main(MathGame.java:16),] 按照耗时进行过滤. 123456789101112$ watch demo.MathGame primeFactors &apos;&#123;params, returnObj&#125;&apos; &apos;#cost&gt;200&apos; -x 2Press Ctrl+C to abort.Affect(class-cnt:1 , method-cnt:1) cost in 66 ms.ts=2018-12-03 19:40:28; [cost=2112.168897ms] result=@ArrayList[ @Object[][ @Integer[2141897465], ], @ArrayList[ @Integer[5], @Integer[428379493], ],] 观察当前对象中的属性. 8.1 如果想查看方法运行前后，当前对象中的属性，可以使用target关键字，代表当前对象. 1234567$ watch demo.MathGame primeFactors &apos;target&apos;Press Ctrl+C to abort.Affect(class-cnt:1 , method-cnt:1) cost in 52 ms.ts=2018-12-03 19:41:52; [cost=0.477882ms] result=@MathGame[ random=@Random[java.util.Random@522b408a], illegalArgumentCount=@Integer[13355],] 8.2 然后使用target.field_name访问当前对象的某个属性. 12345$ watch demo.MathGame primeFactors &apos;target.illegalArgumentCount&apos;Press Ctrl+C to abort.Affect(class-cnt:1 , method-cnt:1) cost in 67 ms.ts=2018-12-03 20:04:34; [cost=131.303498ms] result=@Integer[8]ts=2018-12-03 20:04:35; [cost=0.961441ms] result=@Integer[8] trace(重点)方法内部调用路径，并输出方法路径上的每个节点上耗时. 查看调用 1trace com.xxx.service mymethod 查看调用,次数限制. 1trace com.xxx.service mymethod -n 2 据调用耗时过滤.单位:ms 1trace com.xxx.service mymethod &apos;#cost &gt; 10&apos; stack输出当前方法被调用的调用路径. 很多时候我们都知道一个方法被执行，但这个方法被执行的路径非常多，或者你根本就不知道这个方法是从那里被执行了，此时你需要的是 stack 命令。 查看被执行情况. 1stack com.xxx.service mymethod 按照条件表达式来过滤. 1stack com.xxx.service &apos;params[0]&lt;0&apos; -n 2 按照执行时间来过滤. 1stack com.xxx.service mymethod &apos;#cost&gt;5&apos; profiler火焰图,需要在linux上.]]></content>
      <categories>
        <category>java基础</category>
        <category>java线上排查</category>
      </categories>
      <tags>
        <tag>线上排查</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring-boot-2.0实战]]></title>
    <url>%2F2019%2F11%2F06%2Fspring-boot-2-0%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[spring boot 2.0 实战spring boot 概述spring boot的特点1.快速构建项目 idea插件和官方都提供了快速一键构建一个spring boot的功能. 2.嵌入式web容器 在spring boot 1.0中,默认使用了Tomcat.在spring boot 2.0中,则会根据引入的依赖来决定:spring-boot-starter-web会嵌入Tomcat容器,spring-boot-startrt-webflux会嵌入Netty. 3.易于构建任何应用 spring boot提供了一个强大的starter依赖机制,比如:如果想使用mysql,则引用spring-boot-starter-mysql,在配置文件配置好数据库信息,就可以使用了. 4.自动化配置 在引用一些依赖后,spring boot提供了默认的配置供我们使用.如果需要定制化,则可以在application.yml中修改,就会自动覆盖. 5.开发者工具,热部署 在开发过程中,修改代码总是需要不断的重启项目,spring boot提供了spring-boot-devtools,引入此依赖,在修改代码后,项目会自动重新加载一些必要的类,实现热部署 6.应用监控 spring boot提供了一个依赖:spring-boot-starter-actuator来供我们查看应用的各项指标,如:health(健康检查),dump(活跃线程),env(环境变量),metrics(内存,cpu)等来监控我们的项目.同时可以配合spring-boot-admin-starter-server监控我们的微服务,还有prometheus也可以很简单的加入我们的spring boot 应用程序中. 7.默认提供测试框架 spring boot 默认提供了一个spring-boot-starter-test依赖,可以满足我们日常测试的要求. 8.可执行jar包部署. 由于内嵌了web容器,可以通过maven将spring boot 应用打包成一个jar包,从而快速通过java -jar 启动项目. spring boot的优点1.简化工作 可以简化我们的依赖,配置,部署,监控,测试等. 2.顺应微服务时代 3.背景强大,懂得自然懂. spring boot web实战spring boot webflux1.引入依赖 12345&lt;!--一般不需要写版本号,版本号由partent控制--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt; &lt;/dependency&gt; 2.编写请求处理逻辑 1234567891011121314151617package com.example.webflux.handle;import org.springframework.http.MediaType;import org.springframework.stereotype.Component;import org.springframework.web.reactive.function.BodyInserters;import org.springframework.web.reactive.function.server.ServerRequest;import org.springframework.web.reactive.function.server.ServerResponse;import reactor.core.publisher.Mono;@Componentpublic class HelloHandle &#123; public Mono&lt;ServerResponse&gt; hello(ServerRequest request) &#123; String s = request.methodName(); return ServerResponse.ok().contentType(MediaType.APPLICATION_JSON).body(BodyInserters.fromValue(s + " word")); &#125;&#125; 3.编写接收请求的路径 12345678910111213141516171819package com.example.webflux.router;import com.example.webflux.handle.HelloHandle;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.http.MediaType;import org.springframework.web.reactive.function.server.RequestPredicates;import org.springframework.web.reactive.function.server.RouterFunction;import org.springframework.web.reactive.function.server.RouterFunctions;import org.springframework.web.reactive.function.server.ServerResponse;@Configurationpublic class HelloRouterController &#123; @Bean public RouterFunction&lt;ServerResponse&gt; routerHello(HelloHandle helloHandle) &#123; return RouterFunctions.route(RequestPredicates.GET("/hello") .and(RequestPredicates.accept(MediaType.APPLICATION_JSON)), helloHandle::hello); &#125;&#125; git地址:https://github.com/daicx/spring-boot-2.0-demo/tree/master/webflux spring boot web引入依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; 其余的是经典的写controller层的代码,此处不再编写. 配置文件1.一般使用application.yml格式来定义配置.设置方式为: 12user: name: xiaoming 2.读取方式,需要注意的是,要在已经装配到spring容器中的类中读取. 12@value(&quot;$&#123;user.name&#125;&quot;)private string name; 3.随机数 配置文件中,默认有个随机数函数,用来随机数字,字符串,uuid等: 123456user: intValue: $&#123;random.int&#125; longValue: $&#123;random.long&#125; stringValue: $&#123;random.value&#125; uuidValue: $&#123;random.uuid&#125; int1000value: $&#123;random.int(1000)&#125; 同时可以通过以javabean的形式在接收这些配置,需要在启动类加上:@EnableConfigurationProperties(User.calss) 12345678@configurationProperties(prefix = "user")public class User()&#123; private int intValue; private long longValue; private string stringValue; private string uuidValue; private int int1000value;&#125; 使用的话,在需要使用的类中,引入这个类,就可以读取到配置的属性. 12@Autowiredprivate User user; 4.多环境配置 有时候,我们需要dev,tst,pro的不同的配置,则可以让文件以:application-{name}.yml的格式来命名.然后通过在application.yml中指定:spring.profiles.active={name}来选择使用哪个文件的配置. 页面模板ThymeleafThymeleaf 是spring boot 官方推荐使用的模板框架. 引用依赖: 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; 配置项: 12345678910spring: thymeleaf: #是否开启缓存 cache: false encoding: utf-8 mode: HTML5 #模板文件路径 prefix: classpath:/templates/ #模板文件后缀 suffix: .html demo Freemaker引入依赖: 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-freemarker&lt;/artifactId&gt; &lt;/dependency&gt; 配置项: 1234567891011spring: freemarker: template-loader-path: classpath:/templates cache: false charset: UTF-8 check-template-location: true content-type: text/html expose-request-attributes: false expose-session-attributes: false request-context-attribute: request suffix: .ftl demo 国际化: i18n加入i18n配置文件 12345678910111213141516171819202122232425262728@Configurationpublic class i18n &#123; /** * 默认解析器 其中locale表示默认语言 */ @Bean public LocaleResolver localeResolver() &#123; SessionLocaleResolver localeResolver = new SessionLocaleResolver(); localeResolver.setDefaultLocale(Locale.CHINA); return localeResolver; &#125; /** * 默认拦截器 其中lang表示切换语言的参数名 */ @Bean public WebMvcConfigurer localeInterceptor() &#123; return new WebMvcConfigurer() &#123; @Override public void addInterceptors(InterceptorRegistry registry) &#123; LocaleChangeInterceptor localeInterceptor = new LocaleChangeInterceptor(); localeInterceptor.setParamName("lang"); registry.addInterceptor(localeInterceptor); &#125; &#125;; &#125;&#125; demo]]></content>
      <categories>
        <category>spring家族</category>
        <category>spring-boot-2.0实战</category>
      </categories>
      <tags>
        <tag>spring-boot-2.0实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring5开发实战]]></title>
    <url>%2F2019%2F11%2F06%2Fspring5%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[Spring5 企业开发实战组成Core container核心模块,实现了IOC(控制反转)与DI(依赖注入). Data Access提供了对jdbc,orm,oxm,jms和transaction等模块的支持.使主流的ORM框架,消息中间件,持久化框架等快速的集成到系统. web模块提供了对对象模型-&gt;视图-&gt;控制器(mvc)功能的支持. aop模块提供了面向切面编程的环境,降低各模块间的耦合性. test模块提供了jtest的支持.方便测试. IOC容器原理本质IOC=控制+反转.以前对象的生成是由开发人员控制,而IOC则是将对象的控制转移到了容器,即对象的控制转移.ioc是一种设计理念,它遵守了软件设计原则中的依赖倒置原则,ioc是一种思想,而DI则是具体的实现方式. 注:依赖倒置原则:高层次的模块不应依赖低层次的模块,应当依赖于抽象. 主要组成 beanFactory: 基础的IOC容器,主要是提供了getbean(string name),通过名字获取对象.getType(string name)通过名字获取类型,isSingleton(string name)是否是单例等方法. applicationContext: 爷爷类是beanFactory,提供了一些获取应用名的方法.属于高阶的IOC容器. beanDefinition: javabean对象在容器中的表现形式,包含了类的属性,行为,类型,依赖等功能. 实现过程1.创建beanFactory refreshBeanFactory(),检查上下文中是否存在beanFactory,有则销毁,然后创建. 通过Inputstream()流的方式遍历获取resource资源路径下面的文件,解析xml文件,生成bean对象,然后执行register方法,生成一个以beanName为元素的ArrayList集合和一个concurtenttHashMap以beanname为key,以beanDefnition为value的数据结构. 2.实例化bean (1) 创建bean: 首先遍历第一步生成的arrayList数组,同时以递归的方式,查找到所需要的bean和bean的依赖关系.然后最终执行beanUtil方法的instantiate()方法,实现bean的创建. (2)注入依赖关系: 根据依赖关系,用递归的方式,一层一层的创建并注入依赖的bean,直到与当前bean的依赖链全部注入完成.其中注入最终使用的方法是:找到当前bean的set方法,调用Method的invode方法实现注入. 至此,IOC启动完成. Bean的生命周期1.Spring对Bean进行实例化（相当于程序中的new Xx()). 方式: beanFactory,懒加载. applicationContext:一次性加载所有bean. 2.Spring将值和Bean的引用注入进Bean对应的属性中(依赖注入) 3.如果Bean实现了BeanNameAware接口，Spring将Bean的ID传递给setBeanName()方法（实现BeanNameAware清主要是为了通过Bean的引用来获得Bean的ID，一般业务中是很少有用到Bean的ID的） 4.如果Bean实现了BeanFactoryAware接口，Spring将调用setBeanDactory(BeanFactory bf)方法并把BeanFactory容器实例作为参数传入。（实现BeanFactoryAware 主要目的是为了获取Spring容器，如Bean通过Spring容器发布事件等） 5.如果Bean实现了ApplicationContextAwaer接口，Spring容器将调用setApplicationContext(ApplicationContext ctx)方法，把y应用上下文作为参数传入.(作用与BeanFactory类似都是为了获取Spring容器，不同的是Spring容器在调用setApplicationContext方法时会把它自己作为setApplicationContext 的参数传入，而Spring容器在调用setBeanDactory前需要程序员自己指定（注入）setBeanDactory里的参数BeanFactory ) 6.如果Bean实现了BeanPostProcess接口，Spring将调用它们的postProcessBeforeInitialization（预初始化）方法.(AOP方法增强)（作用是在Bean实例创建成功后对进行增强处理，如对Bean进行修改，增加某个功能） 7.如果Bean实现了InitializingBean接口，Spring将调用它们的afterPropertiesSet方法，作用与在配置文件中对Bean使用init-method声明初始化的作用一样，都是在Bean的全部属性设置成功后执行的初始化方法。 8.如果Bean实现了BeanPostProcess接口，Spring将调用它们的postProcessAfterInitialization（后初始化）方法（作用与6的一样，只不过6是在Bean初始化前执行的，而这个是在Bean初始化后执行的，时机不同 ) 9.经过以上的工作后，Bean将一直驻留在应用上下文中给应用使用，直到应用上下文被销毁 10.如果Bean实现了DispostbleBean接口，Spring将调用它的destory方法，作用与在配置文件中对Bean使用destory-method属性的作用一样，都是在Bean实例销毁前执行的方法。 循环依赖问题发生场景发生在在创建bean的时候的第二步:依赖注入,设置bean的属性的时候.A对象有个属性为B,B对象属性有个A.这样创建A的时候,注入B,发现B还没有就去创建B,但是创建B的时候,发现需要A,但是A还处于未创建中状态,从而陷入死循环. 解决方法通常spring提供了三级缓存来解决循环依赖问题. 1234567891011121314/** 一级缓存：用于存放完全初始化好的 bean **/private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;String, Object&gt;(256);/** 二级缓存：存放原始的 bean 对象（尚未填充属性），用于解决循环依赖 */private final Map&lt;String, Object&gt; earlySingletonObjects = new HashMap&lt;String, Object&gt;(16);/** 三级级缓存：存放 bean 工厂对象，用于解决循环依赖 */private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap&lt;String, ObjectFactory&lt;?&gt;&gt;(16);/**bean 的获取过程：先从一级获取，失败再从二级、三级里面获取创建中状态：是指对象已经 new 出来了但是所有的属性均为 null 等待被 init*/ 流程: a. 创建A,然后创建中的A提前放进3级缓存singletonFactories,发现需要B,则去实例化B. b. 在实例化B的时候,需要A,则分别从1级,2级,3级缓存中获取bean.从3级缓存中获取到创建中的A,创建中的A挪进2级缓存earlySingletonObjects.清除3级缓存中的A,同时B根据创建中的A实例化自身,完成后把B放进singletonObjects,完成B的创建. c.创建中的A根据已经创建好的B,实例化A,完成A的创建.由于B中存储的是A的引用,因此B中的创建中的A,也完成了创建. 注:这只是解决了属性间的循环依赖,无法构造器之间的循环依赖.因为加入3级缓存的前提是执行了构造器方法. 至于为什么要用3级缓存,而不用2级缓存,因为,如果在2级缓存中发现了bean,则说明已经有循环依赖了,可以直接拿去使用. AOP揭秘简介aop(面向切面编程)是对oop(面向对象编程)的补充和扩展.众所周知,java以其将一系类具有相同属性和行为的事物封装为对象.但是要对一系类对象进行操作(比如监听),则会产生冗余代码.因此,引入aop编程来实现对一系列对象的控制. 原理所使用的的是动态代理.在spring5中,在选择代理方式的时候,会根据代理的对象是否实现了接口,来动态的选择代理的方式.主要有两种方式: jdk的动态代理: 需要代理的对象必须实现接口.由于java是单继承,而jdk的动态代理是生成一个继承了proxy的代理对象.因此就想要具有被代理类的特性,就无法再去继承了,只能以实现接口的方式.使用代理对象,调用invoke()方法,通过反射实现方法的调用. cglib的动态代理: 不需要实现接口.会动态生成一个被代理对象的子类.以callback()的方式来实现方法的调用. 注解组成(AspectJ) @AspectJ:切面. @PointCut:切入点. | 切点指示器 | 功能描述 || ————- | ————————————– || args() | 通过判断目标方法的入参类型 || @args() | 通过判断目标方法的入参是否含有指定注解 || execution() | 满足某一匹配条件 || this() | 满足代理类的所有连接点 || target() | 目标对象为指定类型,比this()范围小 || @target() | 目标对象为指定注解 || within() | 和execution()相似,但是最低精确到类 || @within() | 匹配指定注解的类及其子类 || @annotation() | 匹配带有指定注解的连接点 | @Before:前置通知. @AfterRunning:后置通知. @Around:环绕通知. @AfterThrowing:异常通知. Cilent网络请求客户端基于HTTP的接口的RestTemplate基于webFlux接口的WebClientHTTP2HTTP1.0一次请求响应,就会建立一个连接,用完关闭. HTTP1.1增加了keep-alive参数,是多个请求可以串行执行在一个连接中.前面耗时长的请求会影响到后面的请求. HTTP2多路复用,使多个请求在一个连接上,并且并行执行. Spring对事物的支持按照给定的规则来执行提交或者回滚操作. 特性A(原子性)C(一致性)I(隔离性)D(持久性) 隔离级别与并发问题 脏读 不可重复读 幻读 读未提交 √ √ √ 读已提交 × √ √ 可重复读 × × √ 串行化 × × × spring默认事物管理级别是根据数据库默认(mysql是可重复读,通过mvcc多版本并发控制实现.oracle默认是提交读). @TransactionIntercepter,运用的AOP原理. 主要接口 PlatformTransactionManager,事物管理器. spring 不会直接管理事物,而是提供了事物管理器.交给持久化框架,mybatis,Hibernate,jpa来执行. TransactionDefinition,定义事物.(事物隔离级别,传播行为,超时,只读,回滚规则). 2.1 事物传播行为,发生在调用一个事物方法是,选用的策略. a.支持当前事物,当前没有事物,则新建一个事物执行.(默认) b.支持当前事物,当前没有事物,则报异常. c.新建事物,如果当前有事物,则挂起当前事物. d.非事物方式运行,当前存在事物,则被挂起. 2.2 回滚 默认只有运行期异常才会回滚,检查型异常不会回滚. TransactionStatus,事物运行状态. 记录着事物的状态,主要有:是否新建的事物,设置回滚,是否回滚,是否有恢复点,是否已完成. Redis数据类型String,Hash,List,Set,Sortedset 持久化策略一般情况下,redis的数据保存在磁盘中,但是为了让这些数据在机器重启之后还能用,提供了2中持久化策略. RDB在指定的时间内,执行了多少次的操作后,将内存中的数据写到磁盘,生成一个dunp.rdb的文件.机器启动后,读取dump.rdb文件数据到内存. 命令:save 900 1 (在900秒内执行了一次更改,就保存内存数据到磁盘) AOF以日志的形式记录每次写操作,在恢复数据时,将日志里面的数据再执行一遍. 主从复制模式主节点负责接受写入请求,从节点负责查询请求,然后主节点定期同步数据到从节点,此处可见,redis实现的cap原则中的可用性和分区容忍性,没有强一致性. 缺陷: 主节点出现问题后,需要人工指定主节点. 主节点单机写入能力有限. 哨兵模式启动分布式哨兵进程监控各个节点,哨兵有3个定时任务. 每隔10s发送info命令,获取拓部结构图. 每隔2s发送每个哨兵自身的情况和主节点的情况 每个1s像其他节点和哨兵执行ping命令,查看进程情况. 当发现主节点出现问题后,通过选举,将从节点升级为主节点. 问题及处理缓存穿透出现原因:经常查询不存在的数据,导致查询会一直到达数据库. 解决: 缓存空对象,需要更多的存储空间 使用布隆过滤器. 布隆过滤器运用概率性,巧妙的用来判断一定不存在,或者存在. 缓存雪崩出现原因: 缓存的数据在同一时间失效,或者缓存不可用.导致查询直接到数据库. 解决: 保证redis的可用性,建立redis集群. 使用限流降级组件,如Hystrix 优化缓存过期时间 异步重建缓存,即另起线程监控缓存失效期,在其是失效前,对其续约. Zookeeper简介分布式治理服务的框架,一个为分布式提供一致性服务的组件. 配置 tickTime: server端与client端保持心跳时间,每隔tickTIme时间就会有一个心跳.单位:毫秒. initLimit: Leader与Follower初始连接时能容忍的最大心跳数. syncLimit: Leader与Follower保持正常连接所需要的心跳数. dataDir: 数据存放目录. clientPort: 客户端连接断开. dataLogDir: 日志存放目录. 架构组成 Leader: 领导者 Follower: 跟随者 Observer: 观察者 client: 客户端,一般使用apacha的Curator客户端. 选举机制在集群启动时或者leader节点故障是,就会发生选举.每个节点都会向其他节点发送一个投票,内容为:(SID,ZXID),SID:当前机器唯一标志,ZXID:在配置时分配的myid.其他节点在收到信息后,和自己的信息做对比.支持ZXID比自己大的,如果ZXID相同,则支持SID大的. 数据模型采用和文件系统一样的命名空间,每个节点称为一个Znode,有4中类型. 持久节点. 持久顺序节点:节点具有顺序. 临时节点 临时顺序节点 ZK分布式锁创建一个节点,用来表示根节点空间.每个客户端启动都会查询并创建一个临时顺序节点,如果是最小的节点,则获取锁,否则订阅最小的客户端,在最小的客户端释放后,获取锁.为保证原子性,ZK提供的查询最小客户端并订阅,是原子性操作. Kafka简介一款高吞吐量的分布式发布订阅系统.具有解耦,冗余,峰值处理等功能. 组成 broker kafka集群中每台机器称为一个broker topic 每条发送到集群中的消息都会有一个类别,称为topic partition 物理上的分区,每条topic都会被分为一个或者多个partition,持久到磁盘 producer 生产者 consumer 消费者 consumer group 消费者群组,一组消费者去消费消息. 策略 消息处理: 使用内存映射文件,磁盘顺序写入.当内存快满的时候,写入到磁盘. 具有producer像broker推送消息的push模式,由consumer从broker拉取消息的pull模式.push模式可以提交数据处理速度,但会造成消费信息不及时,从而拒绝连接.pull模式则可以根据消费者的消费能力去处理数据,更加可靠. 副本机制,每个leader broker在接收到消息后,会将消息写入到多个Follower 节点中,保证数据可用性. 数据可靠性保障ack通过设置ack来提供数据的可靠性级别 acks 描述 1 默认级别,生产者在leader接收到消息并确认后,发送下一条消息.如果leader宕机,数据丢失 0 生产者不等待leader的返回信息,直接发送下一条消息,此级别数据可靠性最低 -1 生产者需要等到所有ISR(副本同步队列)收到信息并且确认,才发送下一条消息.]]></content>
      <categories>
        <category>spring家族</category>
        <category>spring5实战</category>
      </categories>
      <tags>
        <tag>spring5实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql转换成es-rest]]></title>
    <url>%2F2019%2F10%2F24%2Fmysql%E8%BD%AC%E6%8D%A2%E6%88%90es-rest%2F</url>
    <content type="text"><![CDATA[es查询与mysql查询转换es的简介es作为一款搜索引擎,用作快速检索.本文不做es的过多分析,在实际使用中,可能会遇到mysql语句好实现,但是写成es的rest查询就比较困难的情况.因此列举一些常用转换,持续更新中. 所使用框架: bboss-elasticsearch: 一款类似于mybaits,将mysql封装成以xml的方式来进行sql查询.此开源项目是将es的rest语句封装成以xml的方式进行es查询.具有很大的便利性.github地址使用文档 进入正题,案例如下(持续更新中):注:自定义的属性,后面都会带着 _name,可按照实际情况替换. =查询mysql:1select * from table_name where name = &apos;&apos;名字 limit 1; es:123456789GET index_name /_search&#123; &quot;size&quot;: 1, &quot;query&quot;:&#123; &quot;match_phrase&quot;:&#123; &quot;pn&quot;: #[pn] &#125; &#125;&#125; between,order by,limit查询mysql:1select * from table_name where time_name between start_name and end_name order by id_name desc limit 100; es:12345678910111213141516171819202122232425262728293031GET index_name /_search&#123; &quot;size&quot;: 100, &quot;sort&quot;: [ &#123; &quot;time_name&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ], &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: [ &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;range&quot;: &#123; &quot;time_name&quot;: &#123; &quot;from&quot;: &quot;start_name&quot;, &quot;to&quot;: &quot;end_name&quot; &#125; &#125; &#125; ] &#125; &#125; ] &#125; &#125;&#125; min(),max()查询mysql:1select min(time_name),max(time_name) from table_name; es:1234567891011GET index_name/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;grades_stats&quot;: &#123; &quot;stats&quot;: &#123; &quot;field&quot;: &quot;time_name&quot; &#125; &#125; &#125;&#125; es返回值如下,分析结果,提取出值:12345678910111213&quot;aggregations&quot;: &#123; &quot;grades_stats&quot;: &#123; &quot;count&quot;: 10581611, &quot;min&quot;: 1483228800000, &quot;max&quot;: 1571184000000, &quot;avg&quot;: 1527674316115.0793, &quot;sum&quot;: 16165255347820800000, &quot;min_as_string&quot;: &quot;2017-01-01T00:00:00.000Z&quot;, &quot;max_as_string&quot;: &quot;2019-10-16T00:00:00.000Z&quot;, &quot;avg_as_string&quot;: &quot;2018-05-30T09:58:36.115Z&quot;, &quot;sum_as_string&quot;: &quot;292278994-08-17T07:12:55.807Z&quot; &#125;&#125; in,order by,between 查询mysql:1select * from table_name where name in (&apos;名字1&apos;,&apos;名字2&apos;,&apos;名字3&apos;) and time_name between start and end limit 100; es:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455GET index_name/_search&#123; &quot;size&quot;: #[size], &quot;sort&quot;: [&#123; &quot;so_date&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125;], &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;:[ #if(1&gt;2) &lt;!--时间范围查询,大于等于操作--&gt; #end &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;range&quot;: &#123; &quot;so_date&quot;: &#123; &quot;from&quot;: #[start_name], &quot;to&quot;: #[end_name] &#125; &#125; &#125; ] &#125; &#125; #if($name &amp;&amp; $name.size() &gt; 0), &#123; &quot;bool&quot;:&#123; &quot;must&quot;:[ &#123; &quot;bool&quot;:&#123; &quot;should&quot;: [ #foreach($nameTmp in $name) #if($velocityCount &gt; 0),#end &#123; &quot;match_phrase&quot;:&#123; &quot;name&quot;:&quot;$nameTmp&quot; &#125; &#125; #end ], &quot;minimum_should_match&quot;: 1 &#125; &#125; ] &#125; &#125; #end ] &#125; &#125; &#125; and ( (a=1 and b=2) or (c=3 and d=4) )操作###对象数组查询:12345class user &#123; private string name; private string desc; private string age;&#125; mysql:1select * from table_name where time_name between start_name and end_name and ( (name=1 and age=2) or (name=3 andage=4) ) ; es:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566GET index_name/_search&#123; &quot;size&quot;: #[size], &quot;sort&quot;: [&#123; &quot;so_date&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125;], &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;:[ #if(1&gt;2) &lt;!--时间范围查询,大于等于操作--&gt; #end &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;range&quot;: &#123; &quot;so_date&quot;: &#123; &quot;from&quot;: #[start_name], &quot;to&quot;: #[end_name] &#125; &#125; &#125; ] &#125; &#125; #if(1&gt;2) &lt;!--对象数组查询,and ( (a=1 and b=2) or (c=3 and d=4) )操作--&gt; #end #if( $user &amp;&amp; $user.size() &gt; 0 ) ,&#123; &quot;bool&quot;:&#123; &quot;should&quot;:[ #if($user &amp;&amp; $user.size() &gt; 0) #foreach($userTemp in $user) #if($velocityCount &gt; 0),#end &#123; &quot;bool&quot;:&#123; &quot;must&quot;:[ &#123; &quot;match_phrase&quot;:&#123; &quot;name&quot;:&quot;$userTemp.name&quot; &#125; &#125; #if($userTemp.productDetail), &#123; &quot;match_phrase&quot;:&#123; &quot;age&quot;:&quot;$userTemp.age&quot; &#125; &#125; #end ] &#125; &#125; #end #end ] &#125; &#125; #end ] &#125; &#125; &#125; like查询,并对查询结果去重mysql:1select distinct name from table_name where name like &apos;名字%&apos; es:123456789101112131415161718GET index_name/_search&#123; &quot;size&quot;: 0, &quot;query&quot;:&#123; &quot;wildcard&quot;:&#123; &quot;name&quot;:&#123; &quot;value&quot;: #[名字*] &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;distinct&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;name.keyword&quot; &#125; &#125; &#125;&#125; 对聚合结果进行分析,取出所要的值. 去重查询mysql:1select distinct name from table_name; es:1234567891011GET index_name/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;distinct&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;name.keyword&quot; &#125; &#125; &#125;&#125; 去重模糊查询,返回多个字段mysql:1select id_name,name form table_name where name like &quot;名字%&quot; group by name desc ,id_name desc limit 10; es:12345678910111213141516171819202122232425262728293031323334GET index_name/_search&#123; "size": 10, "sort": [&#123; "name.keyword": &#123; "order": "desc" &#125; &#125;], "query":&#123; "wildcard":&#123; "name.keyword":&#123; "value": #[var] &#125; &#125; &#125;, "aggs": &#123; "type":&#123; "terms": &#123; "field": "id_name.keyword", "size": 10 &#125;, "aggs": &#123; "redistinct": &#123; "top_hits": &#123; "sort": [&#123; "id_name.keyword": &#123;"order": "desc"&#125; &#125;], "size": 1 &#125; &#125; &#125; &#125; &#125; &#125;]]></content>
      <categories>
        <category>es</category>
        <category>es rest查询</category>
      </categories>
      <tags>
        <tag>es rest查询</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IO多路复用]]></title>
    <url>%2F2019%2F05%2F15%2FIO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%2F</url>
    <content type="text"><![CDATA[I/O多路复用技术原理1.简介I/O操作指的是应用层去操作内核层.例如:我们写的程序去执行read()操作,读取服务器里面的数据.基于此操作衍生出几种操作模型. 2.堵塞I/O通常IO操作都是阻塞I/O的，也就是说当你调用read时，如果没有数据收到，那么线程或者进程就会被挂起，直到收到数据。阻塞的意思，就是一直等着。阻塞I/O就是等着数据过来，进行读写操作。应用的函数进行调用，但是内核一直没有返回，就一直等着。应用的函数长时间处于等待结果的状态，我们就称为阻塞I/O。每个应用都得等着，每个应用都在等着，浪费啊！很像现实中的情况。大家都不干活，等着数据过来，过来工作一下，没有的话继续等着。 3.非堵塞I/O非阻塞IO很简单，通过fcntl（POSIX）或ioctl（Unix）设为非阻塞模式，这时，当你调用read时，如果有数据收到，就返回数据，如果没有数据收到，就立刻返回一个错误，如EWOULDBLOCK。这样是不会阻塞线程了，但是你还是要不断的轮询来读取或写入。相当于你去查看有没有数据，告诉你没有，过一会再来吧！应用过一会再来问，有没有数据？没有数据，会有一个返回。但是依旧很不好。应用必须得过一会来一下，问问内核有木有数据啊。这和现实很像啊！好多情况都得去某些地方问问好了没有？木有，明天再过来。明天，好了木有？木有，后天再过来。。。。。忙碌的应用。。。。 4.I/O多路复用多路复用是指使用一个线程来检查多个文件描述符（Socket）的就绪状态，比如调用select和poll函数，传入多个文件描述符（FileDescription，简称FD），如果有一个文件描述符（FileDescription）就绪，则返回，否则阻塞直到超时。得到就绪状态后进行真正的操作可以在同一个线程里执行，也可以启动线程执行（比如使用线程池）。虾米意思？就是派一个代表，同时监听多个文件描述符是否有数据到来。等着等着，如有有数据，就告诉某某你的数据来啦！赶紧来处理吧。有没有很感动，一个人待着，帮了很多人。医院的黄牛，一个人排队，大家只要把钱给它，它就会把号给需要的人. 5.I/O多路复用实现方式5.1 select 函数1int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); select 函数监视的文件描述符分3类，分别是writefds、readfds、和exceptfds。调用后select函数会阻塞，直到有描述副就绪（有数据 可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以 通过遍历fdset，来找到就绪的描述符。 select目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select的一 个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但 是这样也会造成效率的降低。 5.2 poll函数1int poll (struct pollfd *fds, unsigned int nfds, int timeout); 不同与select使用三个位图来表示三个fdset的方式，poll使用一个 pollfd的指针实现。 12345struct pollfd &#123; int fd; /* file descriptor */ short events; /* requested events to watch */ short revents; /* returned events witnessed */&#125;; ​ pollfd结构包含了要监视的event和发生的event，不再使用select“参数-值”传递的方式。同时，pollfd并没有最大数量限制（但是数量过大后性能也是会下降）。 和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符。 注: 从上面看，select和poll都需要在返回后，通过遍历文件描述符来获取已经就绪的socket。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。 5.3 epoll函数epoll是在2.6内核中提出的，是之前的select和poll的增强版本。相对于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。 epoll操作过程需要三个接口，分别如下： 123int epoll_create(int size)；//创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); 1. int epoll_create(int size);创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大，这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值，参数size并不是限制了epoll所能监听的描述符最大个数，只是对内核初始分配内部数据结构的一个建议。当创建好epoll句柄后，它就会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。 2.int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；函数是对指定描述符fd执行op操作。 epfd：是epoll_create()的返回值。 op：表示op操作，用三个宏来表示：添加EPOLL_CTL_ADD，删除EPOLL_CTL_DEL，修改EPOLL_CTL_MOD。分别添加、删除和修改对fd的监听事件。 fd：是需要监听的fd（文件描述符） epoll_event：是告诉内核需要监听什么事，struct epoll_event结构如下： 12345678910111213struct epoll_event &#123; __uint32_t events; /* Epoll events */ epoll_data_t data; /* User data variable */&#125;;//events可以是以下几个宏的集合：EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；EPOLLOUT：表示对应的文件描述符可以写；EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；EPOLLERR：表示对应的文件描述符发生错误；EPOLLHUP：表示对应的文件描述符被挂断；EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里 3. int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);等待epfd上的io事件，最多返回maxevents个事件。参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size，参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。 epool 工作模式: epoll对文件描述符的操作有两种模式：LT（level trigger）和ET（edge trigger）。LT模式是默认模式，LT模式与ET模式的区别如下： LT模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。 ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。 LT模式LT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的。 ET模式ET(edge-triggered)是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个EWOULDBLOCK 错误）。但是请注意，如果一直不对这个fd作IO操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once) ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。 epoll总结在 select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而epoll事先通过epoll_ctl()来注册一 个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait() 时便得到通知。(此处去掉了遍历文件描述符，而是通过监听回调的的机制。这正是epoll的魅力所在。) epoll的优点主要是一下几个方面：\1. 监视的描述符数量不受限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左 右，具体数目可以cat /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。select的最大缺点就是进程打开的fd是有数量限制的。这对 于连接数量比较大的服务器来说根本不能满足。虽然也可以选择多进程的解决方案( Apache就是这样实现的)，不过虽然linux上面创建进程的代价比较小，但仍旧是不可忽视的，加上进程间数据同步远比不上线程间同步的高效，所以也不是一种完美的方案。 IO的效率不会随着监视fd的数量的增长而下降。epoll不同于select和poll轮询的方式，而是通过每个fd定义的回调函数来实现的。只有就绪的fd才会执行回调函数。 如果没有大量的idle -connection或者dead-connection，epoll的效率并不会比select/poll高很多，但是当遇到大量的idle- connection，就会发现epoll的效率大大高于select/poll。 参考: https://zhuanlan.zhihu.com/p/65013389 https://segmentfault.com/a/1190000003063859]]></content>
      <categories>
        <category>策略</category>
        <category>通信策略</category>
      </categories>
      <tags>
        <tag>io多路复用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息队列之Kafka]]></title>
    <url>%2F2019%2F05%2F15%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%2F</url>
    <content type="text"><![CDATA[消息队列之Kafka1.简介Kafka是一个基于分布式的消息发布-订阅系统. 2.专用术语① Broker(代理)kafka包含一个或者多个服务器,一个服务器服务器被称为一个Broker. ②Topic(主题)发布到Kafka集群的消息都有一个类别,这个类别成为Topic. ③Partition(分区)物理上的概念,每个Topic包含一个或者多个Partition. ④Replica(副本)每个partition有多个副本,存储在不同的broker上,保证高可用. ⑤Segment(片段)partition是由多个segment组成,每个segment存储着message信息 ⑥Message(消息)基本的通信单位,由一个key,一个value和时间戳组成. ⑦Producer(生产者)消息生产者,向broker发送消息的客户端 ⑧Comsumer(消费者)消息消费者,从broker读取消息的客户端. ⑨ComsumerGroup(消费者群组)每个comsumer属于一个特定的Comsumer Group,一个消息可以发送到多个Comsumer Group,但是一个Comsumer Group中只能有一个Comsumer消费改消息. 3.分布式订阅模型生产者-&gt;brocker-&gt;消费者 4.消费模型采用拉取模型,消费的进度和速度,由消费者决定. 5.高可用模型(多副本)1.对于每一个Topic,我们都可以设置包含有多少个Partition,每个Partition包含Topic的一部分内容. 2.每个broker都会存储一些Partition,因此在kafka集群中,实现了Topic数据的分布式存储. 为了保证,在其中一台broker宕机后,数据不会丢失,采用了常见的分布式处理方式:多副本机制.在Kafka集群中,每个Partition都会有多个副本,包括一个主副本Leader和0或者多个子副本Follower.这些副本分布在不同的broker上,从而保证了数据的完整性. 3.多副本之间的数据同步 副本之间是如何保证同步的呢? 在生产者和消费者操作Kafka的时候,只有主副本Leader提供读写服务.其他Follower副本会不停的对Leader副本发送请求,拉取最新的数据,然后写到磁盘. 4.ISR是什么? ISR全称是“In-Sync Replicas”,意思是保持同步的副本.意思上图中的Follower+Leader. 6.生产者的ACKS参数acks是Producer设置的,选项有3个:0,1,all.含义是生产者的消息确认机制. 6.1 acks=0 当生产者把消息发出去之后,不管是否落到了那个broker的protition中,总是认为是成功了. 缺点:消息发出去,leader还没收到就宕机了,这条消息就丢失了. 6.2 acks=1(默认) 生产者把消息发出去,并且Leader收到并且保存到了磁盘,就认为是成功了. 缺点:Leader在保存后,发生了宕机,数据也会丢失. 6.3 acks=all 生产者发送消息后,Leader和Follower副本(即ISR列表)都将数据保存都了磁盘,就认为是成功了. 注意:需要ISR列表里至少有一个Follower副本. 7消费者 Consumers Kafka提供了两套consumer api，分为high-level api和sample-api。Sample-api 是一个底层的API，它维持了一个和单一broker的连接，并且这个API是完全无状态的，每次请求都需要指定offset值，因此，这套API也是最灵活的。在kafka中，当前读到哪条消息的offset值是由consumer来维护的，因此，consumer可以自己决定如何读取kafka中的数据。比如，consumer可以通过重设offset值来重新消费已消费过的数据。不管有没有被消费，kafka会保存数据一段时间，这个时间周期是可配置的，只有到了过期时间，kafka才会删除这些数据。（这一点与AMQ不一样，AMQ的message一般来说都是持久化到mysql中的，消费完的message会被delete掉）High-level API封装了对集群中一系列broker的访问，可以透明的消费一个topic。它自己维持了已消费消息的状态，即每次消费的都是下一个消息。High-level API还支持以组的形式消费topic，如果consumers有同一个组名，那么kafka就相当于一个队列消息服务，而各个consumer均衡的消费相应partition中的数据。若consumers有不同的组名，那么此时kafka就相当与一个广播服务，会把topic中的所有消息广播到每个consumer。High level api和Low level api是针对consumer而言的，和producer无关。High level api是consumer读的partition的offsite是存在zookeeper上。High level api 会启动另外一个线程去每隔一段时间，offsite自动同步到zookeeper上。换句话说，如果使用了 High level api， 每个message只能被读一次，一旦读了这条message之后，无论我consumer的处理是否ok。High level api的另外一个线程会自动的把offiste+1同步到zookeeper上。如果consumer读取数据出了问题，offsite也会在zookeeper上同步。因此，如果consumer处理失败了，会继续执行下一条。这往往是不对的行为。因此，Best Practice是一旦consumer处理失败，直接让整个conusmer group抛Exception终止，但是最后读的这一条数据是丢失了，因为在zookeeper里面的offsite已经+1了。等再次启动conusmer group的时候，已经从下一条开始读取处理了。Low level api是consumer读的partition的offsite在consumer自己的程序中维护。不会同步到zookeeper上。但是为了kafka manager能够方便的监控，一般也会手动的同步到zookeeper上。这样的好处是一旦读取某个message的consumer失败了，这条message的offsite我们自己维护，我们不会+1。下次再启动的时候，还会从这个offsite开始读。这样可以做到exactly once对于数据的准确性有保证。]]></content>
      <categories>
        <category>java 中间件</category>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息队列对比]]></title>
    <url>%2F2019%2F05%2F15%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[消息队列对比(持续更新中…) RabbitMq RocketMq Kafka 概念划分 消息代理 消息总线,针对高输入的数据流和重放进行优化 定位 消息队列,消息的可靠性传输 用于存储,读取(重复读取),分析大数据流. 运转机制 主要使用内存 主要使用磁盘 主要是用磁盘 单机吞吐量 6W/S 12W/S 可达18W/S topic优先级 支持 不支持 不支持 消息有效期 消费者使用后从队列中删除 一直存在commit log中,过期删除(默认72小时) 消费者使用后,默认保留一周,可设置为永久. 持久化方式 异步刷盘 异步刷盘,同步刷盘 异步刷盘 运转机制rabbitMq: 分为持久化消息和非持久化消息.并将消息分为索引和内容. 持久化消息:消息过来后存储到磁盘,同时在内存中保留备份,方便快速读取. 非持久化消息: 基本上只保存在内存中,在内存不足的情况下,持久化到磁盘. racketMq: 所有的消息存储到磁盘中一个commit log文件里面,并且有一个逻辑ConsumeQueue,记录了每个队列的在commit log文件的起止地址.所有队列去读取这个文件,运用零拷贝技术读取文件. kafka: 使用零拷贝技术,来实现磁盘文件的快速读取.每个topic的消息被分为多个partation,存储在不同的broker上. 注意点: kafka 由于kafka使用的zookeeper保证数据一致性,消费者默认每隔一段时间提交offset到zk的机制.会有数据丢失和数据重复消费问题: a.机器宕机时,消费者拿到数据还没消费,然后自动提交了offset,那么这条消息丢失. b.机器宕机时,消费者消费了数据,但是还没等提交offset,机器重启后会再次消费数据. 解决办法:关闭默认提交,进行手动提交. 注: 零拷贝: 在linux系统中,存在用户空间和内存空间.每次对文件进行读写时,都会经过这两个空间的传输,就会相当于文件多复制了一份.而零拷贝则是利用虚拟内存技术(即用户空间和内核空间内存中地址直接映射同一个文件),实现文件不用再2个空间之间拷贝就可以读写的方式.netty,kafka,java 的NIO机制,都是使用的此技术. 综上，各种对比之后，有如下建议： 一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了吧，我个人不推荐用这个了； 后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高； 不过现在确实越来越多的公司，会去用 RocketMQ，确实很不错（阿里出品），但社区可能有突然黄掉的风险，对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。 所以中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。 如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。 本文引自: https://www.cnblogs.com/loytime/p/10449138.html]]></content>
      <categories>
        <category>java 中间件</category>
        <category>消息队列对比</category>
      </categories>
      <tags>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于WebSocket的研究]]></title>
    <url>%2F2019%2F05%2F15%2F%E5%9F%BA%E4%BA%8EWebSocket%E7%9A%84%E7%A0%94%E7%A9%B6%2F</url>
    <content type="text"><![CDATA[基于WebSocket的Stomp的研究WebSocket与StompWebSocket是通过单个TCP连接提供全双工通信信道的计算机通信协议.实现的是客户端与服务端的双向通信.STOMP(Simple Text-Orientated Messaging Protocol) 面向消息的简单文本协议. WebSocket处在TCP上非常薄的一层，会将字节流转换为文本/二进制消息，因此，对于实际应用来说，WebSocket的通信形式层级过低，因此，可以在 WebSocket 之上使用 STOMP协议，来为浏览器 和 server间的 通信增加适当的消息语义。 如何理解 STOMP 与 WebSocket 的关系：1) HTTP协议解决了 web 浏览器发起请求以及 web 服务器响应请求的细节，假设 HTTP 协议 并不存在，只能使用 TCP 套接字来 编写 web 应用，你可能认为这是一件疯狂的事情；2) 直接使用 WebSocket（SockJS） 就很类似于 使用 TCP 套接字来编写 web 应用，因为没有高层协议，就需要我们定义应用间所发送消息的语义，还需要确保连接的两端都能遵循这些语义； 3) 同 HTTP 在 TCP 套接字上添加请求-响应模型层一样，STOMP 在 WebSocket 之上提供了一个基于帧的线路格式层，用来定义消息语义； 综上: TCP套接字(底层协议)-&gt;http(高层协议) WebSocket(底层协议)-&gt;Stomp(高层协议) stomp帧格式: 123456789101112131415&gt;&gt;&gt; SENDdestination:/app/chatcontent-length:221&gt;&gt;&gt; SUBSCRIBEid:sub-0destination:/user/22/notifications&lt;&lt;&lt; MESSAGEdestination:/user/22/notificationscontent-type:text/plain;charset=UTF-8subscription:sub-0message-id:kikxsuk1-0content-length:4138 客户端API1.引入js包 2.连接1234567891011121314151617181920212223// 建立连接对象（还未发起连接）var socket=new SockJS("/endpointChatServer");// 获取 STOMP 子协议的客户端对象var stompClient = Stomp.over(socket);// 向服务器发起websocket连接并发送CONNECT帧stompClient.connect( &#123;&#125;,function connectCallback (frame) &#123; // 连接成功时（服务器响应 CONNECTED 帧）的回调方法 document.getElementById("state-info").innerHTML = "连接成功"; console.log('已连接【' + frame + '】'); stompClient.subscribe('/topic/getResponse', function (response) &#123; showResponse(response.body); &#125;); &#125;,function errorCallBack (error) &#123; // 连接失败时（服务器响应 ERROR 帧）的回调方法 document.getElementById("state-info").innerHTML = "连接失败"; console.log('连接失败【' + error + '】'); &#125;); 说明: 1) socket连接对象也可通过WebSocket(不通过SockJS)连接. 1var socket=new WebSocket(&quot;/endpointChatServer&quot;); 2) stompClient.connect()方法签名： 1client.connect(headers, connectCallback, errorCallback); 其中headers表示客户端的认证信息，如： 1234var headers = &#123; login: &apos;mylogin&apos;, passcode: &apos;mypasscode&apos;,&#125;; 若无需认证，直接使用空对象 “{}” 即可； connectCallback 表示连接成功时（服务器响应 CONNECTED 帧）的回调方法；errorCallback 表示连接失败时（服务器响应 ERROR 帧）的回调方法，非必须； 3.断开连接若要从客户端主动断开连接，可调用 disconnect() 方法: 123client.disconnect(function () &#123; alert(&quot;See you next time!&quot;);&#125;; 该方法为异步进行，因此包含了回调参数，操作完成时自动回调； 4.心跳机制若使用STOMP 1.1 版本，默认开启了心跳检测机制，可通过client对象的heartbeat field进行配置（默认值都是10000 ms）： 123client.heartbeat.outgoing = 20000; // client will send heartbeats every 20000msclient.heartbeat.incoming = 0; // client does not want to receive heartbeats from the server// The heart-beating is using window.setInterval() to regularly send heart-beats and/or check server heart-beats 5.发送消息连接成功后，客户端可使用 send() 方法向服务器发送信息： 1client.send(destination url, headers, body); 其中destination url 为服务器 controller中 @MessageMapping 中匹配的URL，字符串，必须参数；headers 为发送信息的header，JavaScript 对象，可选参数；body 为发送信息的 body，字符串，可选参数； 例: 1234//header里面的参数,在服务端用注解:@Headers Map&lt;String,String&gt; headers接收.client.send(&quot;/queue/test&quot;, &#123;priority: 9&#125;, &quot;Hello, STOMP&quot;);//url里面的参数,服务端用注解:@DestinationVariable(&quot;appkey&quot;) String appkey接收.client.send(&quot;/queue/test/&#123;appkey&#125;&quot;, &#123;&#125;, &quot;Hello, STOMP&quot;); 6.消息的订阅 STOMP 客户端要想接收来自服务器推送的消息，必须先订阅相应的URL，即发送一个 SUBSCRIBE 帧，然后才能不断接收来自服务器的推送消息；订阅和接收消息通过 subscribe() 方法实现： 1subscribe(destination url, callback, headers) 其中destination url 为服务器 @SendTo 匹配的 URL，字符串；callback 为每次收到服务器推送的消息时的回调方法，该方法包含参数 message；headers 为附加的headers，JavaScript 对象；什么作用？该方法返回一个包含了id属性的 JavaScript 对象，可作为 unsubscribe() 方法的参数； 例: 123456789var headers = &#123;ack: &apos;client&apos;, &apos;selector&apos;: &quot;location = &apos;Europe&apos;&quot;&#125;;var callback = function(message) &#123; if (message.body) &#123; alert(&quot;got message with body &quot; + message.body) &#125; else &#123; alert(&quot;got empty message&quot;); &#125;&#125;);var subscription = client.subscribe(&quot;/queue/test&quot;, callback, headers); 7.取消订阅123var subscription = client.subscribe(...);subscription.unsubscribe(); 8.JSON的支持STOMP 帧的 body 必须是 string 类型，若希望接收/发送 json 对象，可通过 JSON.stringify() and JSON.parse() 实现；例： 1234567var quote = &#123;symbol: &apos;APPL&apos;, value: 195.46&#125;;client.send(&quot;/topic/stocks&quot;, &#123;&#125;, JSON.stringify(quote));client.subcribe(&quot;/topic/stocks&quot;, function(message) &#123;var quote = JSON.parse(message.body);alert(quote.symbol + &quot; is at &quot; + quote.value);&#125;); 9.事物支持 STOMP 客户端支持在发送消息时用事务进行处理：举例说明： 123456789// start the transaction// 该方法返回一个包含了事务 id、commit()、abort() 的JavaScript 对象var tx = client.begin();// send the message in a transaction// 最关键的在于要在 headers 对象中加入事务 id，若没有添加，则会直接发送消息，不会以事务进行处理client.send(&quot;/queue/test&quot;, &#123;transaction: tx.id&#125;, &quot;message in a transaction&quot;);// commit the transaction to effectively send the messagetx.commit();// tx.abort(); 10.消息接收确认默认情况下，在将消息传递到客户机之前，服务器将自动确认STOMP消息。 客户端可以选择通过订阅目的地来处理消息确认，并将ack头集指定给客户端或单独的客户端。 在这种情况下，客户机必须使用message.ack()方法通知服务器它已经确认了消息。 123456789var subscription = client.subscribe("/queue/test", function(message) &#123; // do something with the message ... // and acknowledge it message.ack(); &#125;, &#123;ack: 'client'&#125;); 服务端1.引入依赖12345//基于spring-boot-2.2.0.M2版本&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-websocket&lt;/artifactId&gt;&lt;/dependency&gt; 2.加入配置文件12345678910111213141516171819@Configuration@EnableWebSocketMessageBrokerpublic class WebSocketStompConfig extends AbstractWebSocketMessageBrokerConfigurer&#123;@Overridepublic void registerStompEndpoints(StompEndpointRegistry registry) &#123; //为 /endpointChatServer 路径启用SockJS功能 registry.addEndpoint("/endpointChatServer").setAllowedOrigins("*").withSockJS();&#125;@Overridepublic void configureMessageBroker(MessageBrokerRegistry registry)&#123; //表明在topic、queue、users这三个域上可以向客户端发消息。 registry.enableSimpleBroker("/topic","/queue","/user"); //客户端向服务端发起请求时，需要以/app为前缀。 registry.setApplicationDestinationPrefixes("/app"); //给指定用户发送一对一的消息前缀是/user。 registry.setUserDestinationPrefix("/user");&#125;&#125; 3.消息传递的三种用例:客户端:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/*STOMP*/var url = &apos;http://localhost:8080/stomp&apos;;var sock = new SockJS(url);var stomp = Stomp.over(sock);var strJson = JSON.stringify(&#123;&apos;message&apos;: &apos;Marco!&apos;&#125;);//默认的和STOMP端点连接/*stomp.connect(&quot;guest&quot;, &quot;guest&quot;, function (franme) &#123;&#125;);*/var headers=&#123; username:&apos;admin&apos;, password:&apos;admin&apos;&#125;;stomp.connect(headers, function (frame) &#123; //发送消息 //第二个参数是一个头信息的Map，它会包含在STOMP的帧中 //事务支持 var tx = stomp.begin(); stomp.send(&quot;/app/marco&quot;, &#123;transaction: tx.id&#125;, strJson); tx.commit(); //订阅服务端消息 subscribe(destination url, callback, headers) stomp.subscribe(&quot;/topic/marco&quot;, function (message) &#123; var content = message.body; var obj = JSON.parse(content); console.log(&quot;订阅的服务端消息：&quot; + obj.message); &#125;, &#123;&#125;); stomp.subscribe(&quot;/app/getShout&quot;, function (message) &#123; var content = message.body; var obj = JSON.parse(content); console.log(&quot;订阅的服务端直接返回的消息：&quot; + obj.message); &#125;, &#123;&#125;); /*以下是针对特定用户的订阅*/ var adminJSON = JSON.stringify(&#123;&apos;message&apos;: &apos;ADMIN&apos;&#125;); /*第一种*/ stomp.send(&quot;/app/singleShout&quot;, &#123;&#125;, adminJSON); stomp.subscribe(&quot;/user/queue/shouts&quot;,function (message) &#123; var content = message.body; var obj = JSON.parse(content); console.log(&quot;admin用户特定的消息1：&quot; + obj.message); &#125;); /*第二种*/ stomp.send(&quot;/app/shout&quot;, &#123;&#125;, adminJSON); stomp.subscribe(&quot;/user/queue/notifications&quot;,function (message) &#123; var content = message.body; var obj = JSON.parse(content); console.log(&quot;admin用户特定的消息2：&quot; + obj.message); &#125;); //若使用STOMP 1.1 版本，默认开启了心跳检测机制（默认值都是10000ms） stomp.heartbeat.outgoing = 20000; stomp.heartbeat.incoming = 0; //客户端不从服务端接收心跳包&#125;); 3.1广播12345678910111213141516171819@MessageMapping(&quot;/marco&quot;)@SendTo(&quot;/topic/marco&quot;) public Shout stompHandle(Shout shout)&#123; LOGGER.info(&quot;接收到消息：&quot; + shout.getMessage()); Shout s = new Shout(); s.setMessage(&quot;Polo!&quot;); return s; &#125; //方式2 @Autowired private SimpMessagingTemplate messagingTemplate; /** * 广播消息，不指定用户，所有订阅此的用户都能收到消息 * @param shout */ @MessageMapping(&quot;/broadcastShout&quot;) public void broadcast(String shout) &#123; messagingTemplate.convertAndSend(&quot;/topic/shouts&quot;, shout); &#125; 3.2 点对点聊天12345678@Autowired private SimpMessagingTemplate messagingTemplate;@MessageMapping(&quot;/chat&quot;) public void handleChat( String msg) &#123; messagingTemplate .convertAndSendToUser(&quot;hjx&quot;, &quot;/queue/notifications&quot;, msg+ &quot;-send:&quot; + msg); &#125; 3.3 客户端与服务端的通信前端 1234567891011var sock = new SockJS(&quot;http://127.0.0.1:8080/endpointChatServer&quot;); var stomp = Stomp.over(sock); var headers = &#123; platform : &apos;mylogin&apos;, name : &apos;mypasscode&apos;, &#125;; stomp.connect(&#123;&#125;, function(frame) &#123; //此处订阅了点对点的通信,注意22为返回用户id stomp.subscribe(&quot;/user/22/notifications&quot;, handleNotification) &#125;)stomp.send(&quot;/app/appkey/chat-test/22&quot;, headers,reader.result) 后端 config: 123456789101112131415161718192021222324252627import org.springframework.context.annotation.Configuration;import org.springframework.messaging.simp.config.MessageBrokerRegistry;import org.springframework.web.socket.config.annotation.EnableWebSocketMessageBroker;import org.springframework.web.socket.config.annotation.StompEndpointRegistry;import org.springframework.web.socket.config.annotation.WebSocketMessageBrokerConfigurer;@Configuration@EnableWebSocketMessageBrokerpublic class WebSocketConfig implements WebSocketMessageBrokerConfigurer &#123; @Override public void configureMessageBroker(MessageBrokerRegistry registry) &#123; //表明在topic、queue、users这三个域上可以向客户端发消息。 registry.enableSimpleBroker(&quot;/topic&quot;,&quot;/user&quot;); //客户端向服务端发起请求时，需要以/app为前缀。 registry.setApplicationDestinationPrefixes(&quot;/app&quot;); //给指定用户发送一对一的消息前缀是/user。 registry.setUserDestinationPrefix(&quot;/user&quot;); &#125; @Override public void registerStompEndpoints(StompEndpointRegistry registry) &#123; registry.addEndpoint(&quot;/endpointChatServer&quot;).setAllowedOrigins(&quot;*&quot;).withSockJS(); &#125; &#125; 使用: 1234567891011@Autowired // 通过SimpMessagingTemplate模板向浏览器发送消息。如果是广播模式，可以直接使用注解@SendToprivate SimpMessagingTemplate simpMessagingTemplate;@MessageMapping(&quot;/&#123;appkey&#125;/chat-test/&#123;to&#125;&quot;) public void handleChatTest(String message,@DestinationVariable(&quot;to&quot;) String to,@Headers Map&lt;String, String&gt; header) &#123; System.out.println(&quot;开始chat&quot;); //此处的to对应上面的用户id,22 simpMessagingTemplate.convertAndSendToUser( to,&quot;/notifications&quot;,header+message); System.out.println(&quot;结束chat&quot;); &#125;]]></content>
      <categories>
        <category>通信机制</category>
      </categories>
      <tags>
        <tag>websocket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx十万并发配置]]></title>
    <url>%2F2019%2F04%2F19%2FNginx%E5%8D%81%E4%B8%87%E5%B9%B6%E5%8F%91%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Nginx 数十万并发设置 1.系统参数1234567891011121314151617[root@node101 ~]# ulimit -a core file size (blocks, -c) 0 #core文件的最大值为100 blocks。data seg size (kbytes, -d) unlimited #进程的数据段可以任意大。scheduling priority (-e) 0 #指定调度优先级file size (blocks, -f) unlimited #文件可以任意大。pending signals (-i) 31152 #最多有31152个待处理的信号。max locked memory (kbytes, -l) 64 #一个任务锁住的物理内存的最大值为64KB。max memory size (kbytes, -m) unlimited #一个任务的常驻物理内存的最大值。open files (-n) 1024 #一个任务最多可以同时打开1024的文件。pipe size (512 bytes, -p) 8 #管道的最大空间为4096字节。POSIX message queues (bytes, -q) 819200 #POSIX的消息队列的最大值为819200字节。real-time priority (-r) 0 #指定实时优先级stack size (kbytes, -s) 8192 #进程的栈的最大值为8192字节。cpu time (seconds, -t) unlimited #进程使用的CPU时间。max user processes (-u) 31152 #当前用户同时打开的进程（包括线程）的最大个数为31152。virtual memory (kbytes, -v) unlimited #没有限制进程的最大地址空间。file locks (-x) unlimited #所能锁住的文件的最大个数没有限制。 2.nginx参数调优worker进程数是否合理1worker_processes auto; nginx worker进程数量，建议和cpu核数相当. 分析: worker_processes是worker进程的数量，默认值为auto，这个优化值受很多因素的影响，如果不确定的话，将其设置为CPU内核数是一个不错的选择 worker连接数是否合理1worker_connections 1024; 单个worker进程可服务的客户端数量（根据并发要求进行更改） 分析:worker_connections 设置了一个worker进程可以同时打开的链接数，有高并发需求时，按照需求进行设定。链接最大数目= worker_processes * worker_connections worker可打开最大文件数是否合理1worker_rlimit_nofileulimit -n Nginx 最大可用文件描述符数量linux可同时打开最大文件数 分析:worker_rlimit_nofile为Nginx单个worker进程最大可用文件描述符数量，和链接数相同；最大数目= worker_processes * worker_rlimit_nofile。 同时需要配置操作系统的 “ulimit -n XXXX”，或者在 /etc/security/limits.conf 中配置。 来达到对应配置。 配置数量按照需求情况设定，不建议配置较高的值。 multi_accept1multi_accept on; 是否尽可能的接受请求（建议打开） 分析: multi_accept 的作用是告诉 nginx 在收到新链接的请求通知时，尽可能接受链接。当然，得让他开着。 读写方式是否合理1sendfile on; 直接从磁盘上读取数据到操作系统缓冲（建议打开） 分析: ​ 在 sendfile 出现之前，为了传输这样的数据，需要在用户空间上分配一块数据缓存，使用 read() 从源文件读取数据到缓存，然后使用 write() 将缓存写入到网络。 sendfile() 直接从磁盘上读取数据到操作系统缓冲。由于这个操作是在内核中完成的，sendfile() 比 read() 和 write() 联合使用要更加有效率。 tcp_nopush1tcp_nopush on; 在一个包中发送全部头文件（建议打开,默认为打开状态） 分析: 配置 nginx 在一个包中发送全部的头文件，而不是一个一个发送。这个选项使服务器在 sendfile 时可以提前准备 HTTP 首部，能够达到优化吞吐的效果。 tcp_nodelay1tcp_nodelay on; 配置 nginx 不缓存数据，快速发送小数据 分析: 不要缓存 data-sends （关闭 Nagle 算法），这个能够提高高频发送小数据报文的实时性。系统存在高频发送小数据报文的时候，打开它。 客户端在keep-alive的请求数量是否合理1keepalive_requests 100000; 建议配置较大的值 分析: 设置通过”一个存活长连接”送达的最大请求数（默认是100，建议根据客户端在”keepalive”存活时间内的总请求数来设置）当送达的请求数超过该值后，该连接就会被关闭。（通过设置为5，验证确实是这样）建议设置为一个较大的值 客户端通信超时时间是否合理1keepalive_timeout 65; 超时时间. 分析: ​ 配置连接 keep-alive 超时时间，服务器将在超时之后关闭相应的连接。 指定了与客户端的 keep-alive 链接的超时时间。服务器会在这个时间后关闭链接。 keep-alive设置过小客户端和服务器会频繁建立连接；设置过大由于连接需要等待keep-alive才会关闭，所以会造成不必要的资源浪费。 后端服务器超时时间是否合理12345proxy_connect_timeout //默认60sproxy_read_timeout //默认60sproxy_send_timeout //默认60s 默认60s 分析: proxy_connect_timeout :后端服务器连接的超时时间_发起握手等候响应超时时间 proxy_read_timeout:连接成功后等候后端服务器响应时间其实已经进入后端的排队之中等候处理（也可以说是后端服务器处理请求的时间） proxy_send_timeout :后端服务器数据回传时间_就是在规定时间之内后端服务器必须传完所有的数据 reset_timedout_connection配置是否合理1reset_timedout_connection on; 服务器在客户端停止发送应答之后关闭连接. 分析: ​ 允许server在client停止响应以后关闭连接,释放分配给该连接的内存。当有大并发需求时，建议打开。 request body读超时时间是否合理1client_body_timeout 10; 默认60s. 分析; 该指令设置请求体（request body）的读超时时间。仅当在一次readstep中，没有得到请求体，就会设为超时。超时后，nginx返回HTTP状态码408(“Request timed out”) types_hash_max_size1types_hash_max_size 2048; types_hash_max_size越小，消耗的内存就越小，但散列key的冲突率可能上升。 分析; ​ ypes_hash_max_size影响散列表的冲突率。types_hash_max_size越大，就会消耗更多的内存，但散列key的冲突率会降低，检索速度就更快。types_hash_max_size越小，消耗的内存就越小，但散列key的冲突率可能上升。 日志记录是否合理1access_log /var/log/nginx/access.log main; #默认打开 access_log和error_logaccess_log建议关闭，降低磁盘IO提高速度error_log 压缩选用是否合理1234567# nginx默认不进行压缩处理 gzip on; gzip_disable &quot;msie6&quot;; gzip_proxied any; gzip_comp_level 9; gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript;设置数据压缩设置禁止压缩是否压缩基于请求、响应的压缩压缩等级压缩类型 gzip ：设置nginx gzip压缩发送的数据，建议打开 gzip_disable：为指定的客户端禁用gzip功能，(IE5.5和IE6 SP1使用msie6参数来禁止gzip压缩 )指定哪些不需要gzip压缩的浏览器(将和User-Agents进行匹配),依赖于PCRE库 gzip_proxied：Nginx作为反向代理的时候启用，根据某些请求和应答来决定是否在对代理请求的应答启用gzip压缩，是否压缩取决于请求头中的“Via”字段，指令中可以同时指定多个不同的参数，意义如下（ 无特殊需求建议设置为any）： off(关闭所有代理结果的数据的压缩) expired - 启用压缩，如果header头中包含 “Expires” 头信息 no-cache - 启用压缩，如果header头中包含 “Cache-Control:no-cache” 头信息 no-store - 启用压缩，如果header头中包含 “Cache-Control:no-store” 头信息 private - 启用压缩，如果header头中包含 “Cache-Control:private” 头信息 no_last_modified - 启用压缩,如果header头中不包含 “Last-Modified” 头信息 no_etag - 启用压缩 ,如果header头中不包含 “ETag” 头信息 auth - 启用压缩 , 如果header头中包含 “Authorization” 头信息 any - 无条件启用压缩 gzip_comp_level：数据压缩的等级。等级可以是 1-9 的任意一个值，9 表示最慢但是最高比例的压缩 gzip_types：设置进行 gzip 的类型；对于多数以文本为主的站点来说，文本自身内容占流量的绝大部分。虽然单个文本体积并不算大，但是如果数量众多的话，流量还是相当可观。启用GZIP以后，可以大幅度减少所需的流量. linux系统是否启用epoll1use epoll; Linux 关键配置，允许单个线程处理多个客户端请求。 分析: ​ Linux 关键配置，允许单个线程处理多个客户端请求。 缓存设置是否合理1234567open_file_cache max=200000 inactive=20s;//缓存最大数目及超时时间检测open_file_cache_valid 30s; //缓存源文件是否超时的间隔时间open_file_cache_min_uses 2;//缓存文件最小访问次数open_file_cache_errors on;//缓存文件错误信息 3.服务器内核参数调优net.ipv4.tcp_max_tw_buckets = 6000 timewait 的数量，默认是180000。 net.ipv4.ip_local_port_range = 1024 65000 允许系统打开的端口范围。 net.ipv4.tcp_tw_recycle = 1 启用timewait 快速回收。 net.ipv4.tcp_tw_reuse = 1 开启重用。允许将TIME-WAIT sockets 重新用于新的TCP 连接。 net.ipv4.tcp_syncookies = 1 开启SYN Cookies，当出现SYN 等待队列溢出时，启用cookies 来处理。 net.core.somaxconn = 262144 web 应用中listen 函数的backlog 默认会给我们内核参数的net.core.somaxconn 限制到128，而nginx 定义的NGX_LISTEN_BACKLOG 默认为511，所以有必要调整这个值。 net.core.netdev_max_backlog = 262144 每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目。 net.ipv4.tcp_max_orphans = 262144 系统中最多有多少个TCP 套接字不被关联到任何一个用户文件句柄上。如果超过这个数字，孤儿连接将即刻被复位并打印出警告信息。这个限制仅仅是为了防止简单的DoS 攻击，不能过分依靠它或者人为地减小这个值，更应该增加这个值(如果增加了内存之后)。 net.ipv4.tcp_max_syn_backlog = 262144 记录的那些尚未收到客户端确认信息的连接请求的最大值。对于有128M 内存的系统而言，缺省值是1024，小内存的系统则是128。 net.ipv4.tcp_timestamps = 0 时间戳可以避免序列号的卷绕。一个1Gbps 的链路肯定会遇到以前用过的序列号。时间戳能够让内核接受这种“异常”的数据包。这里需要将其关掉。 net.ipv4.tcp_synack_retries = 1 为了打开对端的连接，内核需要发送一个SYN 并附带一个回应前面一个SYN 的ACK。也就是所谓三次握手中的第二次握手。这个设置决定了内核放弃连接之前发送SYN+ACK 包的数量。 net.ipv4.tcp_syn_retries = 1 在内核放弃建立连接之前发送SYN 包的数量。 net.ipv4.tcp_fin_timeout = 1 如 果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2 状态的时间。对端可以出错并永远不关闭连接，甚至意外当机。缺省值是60 秒。2.2 内核的通常值是180 秒，3你可以按这个设置，但要记住的是，即使你的机器是一个轻载的WEB 服务器，也有因为大量的死套接字而内存溢出的风险，FIN- WAIT-2 的危险性比FIN-WAIT-1 要小，因为它最多只能吃掉1.5K 内存，但是它们的生存期长些。 net.ipv4.tcp_keepalive_time = 30 当keepalive 起用的时候，TCP 发送keepalive 消息的频度。缺省是2 小时。 4.nginx.conf配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869user root;# cpu数量，建议使用默认worker_processes auto;pid /run/nginx.pid;# 配置nginx worker进程最大打开文件数 worker_rlimit_nofile 65535;events &#123; # 单个进程允许的客户端最大连接数 worker_connections 20480; # multi_accept on;&#125;http &#123; ## # Basic Settings ## sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; # server_tokens off; # server_names_hash_bucket_size 64; # server_name_in_redirect off; include /etc/nginx/mime.types; default_type application/octet-stream; ## # SSL Settings ## ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # Dropping SSLv3, ref: POODLE ssl_prefer_server_ciphers on; ## # Logging Settings ## access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; ## # Gzip Settings ## gzip on; gzip_disable "msie6"; # gzip_vary on; # gzip_proxied any; # gzip_comp_level 6; # gzip_buffers 16 8k; # gzip_http_version 1.1; # gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript; ## # Virtual Host Configs ## include /etc/nginx/conf.d/*.conf; include /etc/nginx/sites-enabled/*;&#125; 5.内核配置文件1234567891011121314151617181920212223242526272829303132net.ipv4.ip_forward = 0net.ipv4.conf.default.rp_filter = 1net.ipv4.conf.default.accept_source_route = 0kernel.sysrq = 0kernel.core_uses_pid = 1net.ipv4.tcp_syncookies = 1kernel.msgmnb = 65536kernel.msgmax = 65536kernel.shmmax = 68719476736kernel.shmall = 4294967296net.ipv4.tcp_max_tw_buckets = 6000net.ipv4.tcp_sack = 1net.ipv4.tcp_window_scaling = 1net.ipv4.tcp_rmem = 4096 87380 4194304net.ipv4.tcp_wmem = 4096 16384 4194304net.core.wmem_default = 8388608net.core.rmem_default = 8388608net.core.rmem_max = 16777216net.core.wmem_max = 16777216net.core.netdev_max_backlog = 262144net.core.somaxconn = 262144net.ipv4.tcp_max_orphans = 3276800net.ipv4.tcp_max_syn_backlog = 262144net.ipv4.tcp_timestamps = 0net.ipv4.tcp_synack_retries = 1net.ipv4.tcp_syn_retries = 1net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_mem = 94500000 915000000 927000000net.ipv4.tcp_fin_timeout = 1net.ipv4.tcp_keepalive_time = 30net.ipv4.ip_local_port_range = 1024 65000 vi /etc/sysctl.conf CentOS5.5中可以将所有内容清空直接替换. 使配置立即生效可使用如下命令：/sbin/sysctl -p 6.系统连接数linux 默认值 open files 和 max user processes 为 1024 #ulimit -n 1024 #ulimit Cu 1024 问题描述： 说明 server 只允许同时打开 1024 个文件，处理 1024 个用户进程 使用ulimit -a 可以查看当前系统的所有限制值，使用ulimit -n 可以查看当前的最大打开文件数。 新装的linux 默认只有1024 ，当作负载较大的服务器时，很容易遇到error: too many open files 。因此，需要将其改大。 解决方法： 使用 ulimit Cn 65535 可即时修改，但重启后就无效了。（注ulimit -SHn 65535 等效 ulimit -n 65535 ，-S 指soft ，-H 指hard) 有如下三种修改方式： \1. 在/etc/rc.local 中增加一行 ulimit -SHn 65535 2. 在/etc/profile 中增加一行 ulimit -SHn 65535 3. 在/etc/security/limits.conf 最后增加： 1234* soft nofile 65535* hard nofile 65535* soft nproc 65535* hard nproc 65535 具体使用哪种，在 CentOS 中使用第1 种方式无效果，使用第3 种方式有效果，而在Debian 中使用第2 种有效果 # ulimit -n 65535 # ulimit -u 65535 备注：ulimit 命令本身就有分软硬设置，加-H 就是硬，加-S 就是软默认显示的是软限制 soft 限制指的是当前系统生效的设置值。 hard 限制值可以被普通用户降低。但是不能增加。 soft 限制不能设置的比 hard 限制更高。 只有 root 用户才能够增加 hard 限制值。]]></content>
      <categories>
        <category>Nginx</category>
        <category>Nginx十万并发</category>
      </categories>
      <tags>
        <tag>Nginx并发配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx配置介绍]]></title>
    <url>%2F2019%2F04%2F19%2FNginx%E9%85%8D%E7%BD%AE%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Nginx配置信息详解 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327#nginx进程,一般设置为和cpu核数一样,使用auto将自动配置worker_processes 4; #错误日志存放目录 error_log /data/logs/error.log crit; #运行用户，默认即是nginx，可不设置user nginx #进程pid存放位置pid /application/nginx/nginx.pid; #Specifies the value for maximum file descriptors that can be opened by this process. #最大文件打开数（连接），可设置为系统优化后的ulimit -HSn的结果worker_rlimit_nofile 51200;#cpu亲和力配置，让不同的进程使用不同的cpuworker_cpu_affinity 0001 0010 0100 1000 0001 00100100 1000;#工作模式及连接数上限events &#123; use epoll; #epoll是多路复用IO(I/O Multiplexing)中的一种方式,但是仅用于linux2.6以上内核,可以大大提高nginx的性能 worker_connections 1024; #;单个后台worker process进程的最大并发链接数&#125;http &#123;include mime.types; #文件扩展名与类型映射表default_type application/octet-stream; #默认文件类型#limit模块，可防范一定量的DDOS攻击#用来存储session会话的状态，如下是为session分配一个名为one的10M的内存存储区，限制了每秒只接受一个ip的一次请求 1r/s limit_req_zone $binary_remote_addr zone=one:10m rate=1r/s; limit_conn_zone $binary_remote_addr zone=addr:10m; include mime.types; default_type application/octet-stream;#第三方模块lua防火墙 lua_need_request_body on; #lua_shared_dict limit 50m; lua_package_path &quot;/application/nginx/conf/waf/?.lua&quot;; init_by_lua_file &quot;/application/nginx/conf/waf/init.lua&quot;; access_by_lua_file &quot;/application/nginx/conf/waf/access.lua&quot;; #设定请求缓存 server_names_hash_bucket_size 128; client_header_buffer_size 512k; large_client_header_buffers 4 512k; client_max_body_size 100m; #隐藏响应header和错误通知中的版本号 server_tokens off; #开启高效传输模式 sendfile on; #激活tcp_nopush参数可以允许把httpresponse header和文件的开始放在一个文件里发布,积极的作用是减少网络报文段的数量,tcp_nopush = on 会设置调用tcp_cork方法，这个也是默认的，结果就是数据包不会马上传送出去，等到数据包最大时，一次性的传输出去，这样有助于解决网络堵塞。对于nginx配置文件中的tcp_nopush，默认就是tcp_nopush,不需要特别指定，这个选项对于www，ftp等大文件很有帮助. tcp_nopush on; #激活tcp_nodelay，内核会等待将更多的字节组成一个数据包，从而提高I/O性能.TCP_NODELAY和TCP_CORK基本上控制了包的“Nagle化”，Nagle化在这里的含义是采用Nagle算法把较小的包组装为更大的帧。Nagle化后来成了一种标准并且立即在因特网上得以实现。它现在已经成为缺省配置了. # 现在让我们假设某个应用程序发出了一个请求，希望发送小块数据。我们可以选择立即发送数据或者等待产生更多的数据然后再一次发送两种策略。如果我们马上发送数据，那么交互性的以及客户/服务器型的应用程序将极大地受益。如果请求立即发出那么响应时间也会快一些。以上操作可以通过设置套接字的TCP_NODELAY = on 选项来完成，这样就禁用了Nagle 算法。 另外一种情况则需要我们等到数据量达到最大时才通过网络一次发送全部数据，这种数据传输方式有益于大量数据的通信性能，典型的应用就是文件服务器。应用 Nagle算法在这种情况下就会产生问题。但是，如果你正在发送大量数据，你可以设置TCP_CORK选项禁用Nagle化，其方式正好同 TCP_NODELAY相反（TCP_CORK和 TCP_NODELAY是互相排斥的）。 tcp_nodelay on; #FastCGI相关参数：为了改善网站性能：减少资源占用，提高访问速度fastcgi_connect_timeout 300;fastcgi_send_timeout 300;fastcgi_read_timeout 300;fastcgi_buffer_size 64k;fastcgi_buffers 4 64k;fastcgi_busy_buffers_size 128k;fastcgi_temp_file_write_size 128k;#连接超时时间，单位是秒 keepalive_timeout 60; #开启gzip压缩功能 gzip on； #设置允许压缩的页面最小字节数，页面字节数从header头的Content-Length中获取。默认值是0，表示不管页面多大都进行压缩。建议设置成大于1K。如果小于1K可能会越压越大。 gzip_min_length 1k;#压缩缓冲区大小。表示申请4个单位为16K的内存作为压缩结果流缓存，默认值是申请与原始数据大小相同的内存空间来存储gzip压缩结果。 gzip_buffers 4 16k;#压缩版本（默认1.1，前端为squid2.5时使用1.0）用于设置识别HTTP协议版本，默认是1.1，目前大部分浏览器已经支持GZIP解压，使用默认即可。 gzip_http_version 1.0;#压缩比率。用来指定GZIP压缩比，1压缩比最小，处理速度最快；9压缩比最大，传输速度快，但处理最慢，也比较消耗cpu资源。 gzip_comp_level 9;#用来指定压缩的类型，“text/html”类型总是会被压缩 gzip_types text/plain application/x-javascript text/css application/xml; #vary header支持。该选项可以让前端的缓存服务器缓存经过GZIP压缩的页面，例如用Squid缓存经过Nginx压缩的数据。gzip_vary off;#开启ssi支持，默认是off ssi on; ssi_silent_errors on;#设置日志模式 log_format access &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; $http_x_forwarded_for&apos;;#反向代理负载均衡设定部分#upstream表示负载服务器池，定义名字为backend_server的服务器池upstream backend_server &#123; server 10.254.244.20:81 weight=1 max_fails=2 fail_timeout=30s; server 10.254.242.40:81 weight=1 max_fails=2 fail_timeout=30s; server 10.254.245.19:81 weight=1 max_fails=2 fail_timeout=30s; server 10.254.243.39:81 weight=1 max_fails=2 fail_timeout=30s; #设置由 fail_timeout 定义的时间段内连接该主机的失败次数，以此来断定 fail_timeout 定义的时间段内该主机是否可用。默认情况下这个数值设置为 1。零值的话禁用这个数量的尝试。设置在指定时间内连接到主机的失败次数，超过该次数该主机被认为不可用。#这里是在30s内尝试2次失败即认为主机不可用！ &#125;####################基于域名的虚拟主机 server &#123;#监听端口 listen 80; server_name www.abc.com abc.com; index index.html index.htm index.php; #首页排序 root /data0/abc; #站点根目录，即网站程序存放目录 error_page 500 502 404 /templates/kumi/phpcms/404.html; #错误页面#伪静态 将www.abc.com/list....html的文件转发到index.php。。。#rewrite ^/list-([0-9]+)-([0-9]+)-([0-9]+)-([0-9]+)-([0-9]+)-([0-9]+)-([0-9]+)-([0-9]+)-([0-9]+)\.html$ /index.php?m=content&amp;c=index&amp;a=lists&amp;catid=$1&amp;types=$2&amp;country=$3&amp;language=$4&amp;age=$5&amp;startDate=$6&amp;typeLetter=$7&amp;type=$8&amp;page=$9 last;#location 标签，根目录下的.svn目录禁止访问 location ~ /.svn/ &#123; deny all; &#125; location ~ \.php$ &#123; #符合php扩展名的请求调度到fcgi server fastcgi_pass 127.0.0.1:9000; #抛给本机的9000端口 fastcgi_index index.php; #设定动态首页 include fcgi.conf; &#125; allow 219.237.222.30 ; #允许访问的ip allow 219.237.222.31 ; allow 219.237.222.32 ; allow 219.237.222.33 ; allow 219.237.222.34 ; allow 219.237.222.35 ; allow 219.237.222.61 ; allow 219.237.222.28 ; deny all; #禁止其他ip访问 &#125; location ~ ^/admin.php &#123; location ~ \.php$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fcgi.conf; &#125; allow 219.237.222.30 ; allow 219.237.222.31 ; allow 219.237.222.32 ; allow 219.237.222.33 ; allow 219.237.222.34 ; allow 219.237.222.35 ; allow 219.237.222.61; allow 219.237.222.28; deny all; &#125;#将符合js,css文件的等设定expries缓存参数，要求浏览器缓存。location~ .*\.(js|css)?$ &#123; expires 30d; #客户端缓存上述js,css数据30天 &#125;##add by 20140321#######nginx防sql注入#############start####if ( $query_string ~* &quot;.*[\;&apos;\&lt;\&gt;].*&quot; )&#123; return 444; &#125;if ($query_string ~* &quot;.*(insert|select|delete|update|count|\*|%|master|truncate|declare|\&apos;|\;|and|or|\(|\)|exec).* &quot;) &#123; return 444; &#125;if ($request_uri ~* &quot;(cost\()|(concat\()&quot;) &#123; return 444; &#125;if ($request_uri ~* &quot;[+|(%20)]union[+|(%20)]&quot;) &#123; return 444; &#125;if ($request_uri ~* &quot;[+|(%20)]and[+|(%20)]&quot;) &#123; return 444; &#125;if ($request_uri ~* &quot;[+|(%20)]select[+|(%20)]&quot;) &#123; return 444; &#125;set $block_file_injections 0;if ($query_string ~ &quot;[a-zA-Z0-9_]=(\.\.//?)+&quot;) &#123;set $block_file_injections 1;&#125;if ($query_string ~ &quot;[a-zA-Z0-9_]=/([a-z0-9_.]//?)+&quot;) &#123;set $block_file_injections 1;&#125;if ($block_file_injections = 1) &#123;return 448;&#125;set $block_common_exploits 0;if ($query_string ~ &quot;(&lt;|%3C).*script.*(&gt;|%3E)&quot;) &#123;set $block_common_exploits 1;&#125;if ($query_string ~ &quot;GLOBALS(=|\[|\%[0-9A-Z]&#123;0,2&#125;)&quot;) &#123;set $block_common_exploits 1;&#125;if ($query_string ~ &quot;_REQUEST(=|\[|\%[0-9A-Z]&#123;0,2&#125;)&quot;) &#123;set $block_common_exploits 1;&#125;if ($query_string ~ &quot;proc/self/environ&quot;) &#123;set $block_common_exploits 1;&#125;if ($query_string ~ &quot;mosConfig_[a-zA-Z_]&#123;1,21&#125;(=|\%3D)&quot;) &#123;set $block_common_exploits 1;&#125;if ($query_string ~ &quot;base64_(en|de)code\(.*\)&quot;) &#123;set $block_common_exploits 1;&#125;if ($block_common_exploits = 1) &#123;return 444;&#125;set $block_spam 0;if ($query_string ~ &quot;\b(ultram|unicauca|valium|viagra|vicodin|xanax|ypxaieo)\b&quot;) &#123;set $block_spam 1;&#125;if ($query_string ~ &quot;\b(erections|hoodia|huronriveracres|impotence|levitra|libido)\b&quot;) &#123;set $block_spam 1;&#125;if ($query_string ~ &quot;\b(ambien|blue\spill|cialis|cocaine|ejaculation|erectile)\b&quot;) &#123;set $block_spam 1;&#125;if ($query_string ~ &quot;\b(lipitor|phentermin|pro[sz]ac|sandyauer|tramadol|troyhamby)\b&quot;) &#123;set $block_spam 1;&#125;if ($block_spam = 1) &#123;return 444;&#125;set $block_user_agents 0;if ($http_user_agent ~ &quot;Wget&quot;) &#123; set $block_user_agents 1;&#125;# Disable Akeeba Remote Control 2.5 and earlierif ($http_user_agent ~ &quot;Indy Library&quot;) &#123;set $block_user_agents 1;&#125;# Common bandwidth hoggers and hacking tools.if ($http_user_agent ~ &quot;libwww-perl&quot;) &#123;set $block_user_agents 1;&#125;if ($http_user_agent ~ &quot;GetRight&quot;) &#123;set $block_user_agents 1;&#125;if ($http_user_agent ~ &quot;GetWeb!&quot;) &#123;set $block_user_agents 1;&#125;if ($http_user_agent ~ &quot;Go!Zilla&quot;) &#123;set $block_user_agents 1;&#125;if ($http_user_agent ~ &quot;Download Demon&quot;) &#123;set $block_user_agents 1;&#125;if ($http_user_agent ~ &quot;Go-Ahead-Got-It&quot;) &#123;set $block_user_agents 1;&#125;if ($http_user_agent ~ &quot;TurnitinBot&quot;) &#123;set $block_user_agents 1;&#125;if ($http_user_agent ~ &quot;GrabNet&quot;) &#123;set $block_user_agents 1;&#125;if ($block_user_agents = 1) &#123;return 444;&#125;###end#### location ~ ^/list &#123; #如果后端的服务器返回502、504、执行超时等错误，自动将请求转发到upstream负载均衡池中的另一台服务器，实现故障转移。 proxy_next_upstream http_502 http_504 error timeout invalid_header; proxy_cache cache_one; #对不同的HTTP状态码设置不同的缓存时间 proxy_cache_valid 200 301 302 304 1d; #proxy_cache_valid any 1d; #以域名、URI、参数组合成Web缓存的Key值，Nginx根据Key值哈希，存储缓存内容到二级缓存目录内 proxy_cache_key $host$uri$is_args$args; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $remote_addr; proxy_ignore_headers &quot;Cache-Control&quot; &quot;Expires&quot; &quot;Set-Cookie&quot;; #proxy_ignore_headers Set-Cookie; #proxy_hide_header Set-Cookie; proxy_pass http://backend_server; add_header Nginx-Cache &quot;$upstream_cache_status from km&quot;; expires 1d; &#125; access_log /data1/logs/abc.com.log access; #nginx访问日志 &#125;-----------------------ssl（https）相关------------------------------------server &#123; listen 13820; #监听端口 server_name localhost; charset utf-8; #gbk,utf-8,gb2312,gb18030 可以实现多种编码识别 ssl on; #开启ssl ssl_certificate /ls/app/nginx/conf/mgmtxiangqiankeys/server.crt; #服务的证书 ssl_certificate_key /ls/app/nginx/conf/mgmtxiangqiankeys/server.key; #服务端key ssl_client_certificate /ls/app/nginx/conf/mgmtxiangqiankeys/ca.crt; #客户端证书 ssl_session_timeout 5m; #session超时时间 ssl_verify_client on; # 开户客户端证书验证 ssl_protocols SSLv2 SSLv3 TLSv1; #允许SSL协议 ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv2:+EXP; #加密算法 ssl_prefer_server_ciphers on; #启动加密算法 access_log /lw/logs/nginx/dataadmin.test.com.ssl.access.log access ; #日志格式及日志存放路径 error_log /lw/logs/nginx/dataadmin.test.com.ssl.error.log; #错误日志存放路径&#125;-------------------------------------------------------------------------&#125;]]></content>
      <categories>
        <category>Nginx</category>
        <category>Nginx配置详解</category>
      </categories>
      <tags>
        <tag>Nginx配置详解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对象的回收和分配]]></title>
    <url>%2F2019%2F03%2F17%2F%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%9B%9E%E6%94%B6%E5%92%8C%E5%88%86%E9%85%8D%2F</url>
    <content type="text"><![CDATA[GC算法 2019年3月17日 12:32 简介:由内存模型可知,虚拟机栈,本地方法栈,程序计数器都是线程私有的,伴随着线程的结束而消失.其中虚拟机栈和本地方法栈中的栈桢也会随着方法的进入和退出而进行着入栈和出栈的操作.每一个栈桢需要的内存基本上是在类结构确定下来后就确定了.因此内存的回收和分配都具有确定性,因此不需要过多考虑.但是在java堆和方法区,由于是动态加载,只有在运行期才会知道需要分配和回收的内存.因此,垃圾回收器所关注的也是这部分的内存. 判断对象已死?要进行对象回收首先要确定对象是否已死,目前有以下几种方式: 1. 引用计数算法:为每个对象添加一个引用计数器,每有一个地方引用,就加1,引用失效就减1.当为0时,则判为死亡.此方法在两个对象相互引用时,无法用此算法清除. 2. 可达性分析算法:通过一个GC root的根节点,往下搜索,如果搜索不到,则为死亡. 在java中,可作为GC roots的对象有: a. 虚拟机栈中的栈桢中局部变量表所引用的对象. b. 方法区中静态类属性所引用的对象. c. 方法区中的常量所引用的对象. d. 本地方法栈中native方法所引用的对象. 3. java中的引用a. 强引用: 例如:Object obj= new Object();只要引用还存在,永远不会GC. 用途:对象的默认状态. b. 软引用: 用SoftReference类来实现,被软引用关联的对象,在内存溢出发生之前,会对这部分进行回收. 用途:对象缓存. c. 弱引用 用WeakReference类实现,无论是否会发生内存溢出,在GC的时候,都会清除. 用途:对象缓存. d. 虚引用 用PhantomReference类实现.唯一目的是在这些对象被回收之前,会得到一个系统通知. 用途:垃圾回收监控. 4. Finallize()方法只会被系统调用一次,有不确定性,并且代价极大.建议用try-finally来代替. GC算法1. 标记-清除法首先标记出对象,然后统一回收.缺点是:效率低.并会产生大量不连续的内存碎片. 2. 复制法将内存分为两块,每次讲需要回收的对象放到一块,然后统一清理.缺点是:活跃内存减少一半,代价极大. 3. 复制法进阶版由于新生代对象具有极高的死亡率,因此将内存按照8:1的比例划分为一个大的Eden和2个小的survivor空间.每次使用的是Eden空间和一个survivor空间.在进行回收时,将活着的对象复制到那个暂时不用的survivor空间中.然后清理掉Eden空间和survivor空间.这样基本上只会浪费10%的内存空间.这种方式只适用于新生代. 4. 标记-整理算法​ 将活着的对象移动到一块,然后清理掉后面的对象.适用于老年代. 内存的分配策略:1. 新生代的Eden区分配大多数情况下,对象在新生代的Eden区分配,内存不足时,发起一次Minor GC 2. 大对象直接进入老年代.需要注意的是,避免编写短命的大对象. 3. 长期存活的对象直接进入老年代.每个对象都有一个年龄计数器(Age),在Eden出生,在第一次Minor GC下仍存活,并且被survivor所接受的话,就会移动到survivor中,年龄设为1.每次熬过一次Minor GC,年龄就加1,直到默认的15岁后,便会移动到老年代. 4. 动态对象年龄判断如果13岁的对象所占的空间大于survivor空间的一般,则会吧13的也放到老年代. 5. 空间分配担保当新生代在进行一个Minor GC后,仍有大量对象存活.将会将survivor空间无法容纳的对象直接进入老年代.在每次发生Minor GC 之前,都会检查之前每次晋升到老年代的平均大小,如果大于,便会改为一个Full GC.]]></content>
      <categories>
        <category>jvm</category>
        <category>对象的回收和分配</category>
      </categories>
      <tags>
        <tag>GC算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm之内存模型]]></title>
    <url>%2F2019%2F03%2F17%2Fjvm%E4%B9%8B%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[简介:由于java将内存的管理交给了虚拟机,了解内存结构,有利于排查内存溢出和泄露问题. jdk1.8之后,hotspot的jvm改动: 使用内存中的元空间代替原先的方法区. 字符串常量池放进堆中. jvm运行时的数据区域:程序计数器: 一块很小的内存区域,主要作用是当前线程所执行字节码的行号指示器.字节码解释工作是,根据改变程序计数器的值来选取下一条所执行的字节码指令.例如:分支,循环,跳转,异常处理.因为jvm多线程是通过线程轮流切换来实现的,因此程序计数器是线程私有的. 如果是正在执行java方法,此区域存储的是正在执行的虚拟机的字节码指令.如果是native方法,此区域为undefined.程序计数器是唯一一片不会出现outofmemoryerror的区域. java虚拟机栈 描述的是方法的执行的内存模型,每一个方法的执行都会创建一个栈桢,用于存储局部变量表,操作数栈,动态连接,方法出口信息. 局部变量表 ​ 存储着编译期可知的基本数据类型,对象引用.基本单位是32个字节的slot,long和double会用2个slot存储,因此不具有原子性,但是由于是线程私有的,所以不具有安全问题.所需的空间在编译期就可以确定. ​ 常出现的异常: ​ ①.stackOverFlowError:请求的栈深度大于所允许的深度. ​ ②.OutMemoryError:虚拟机栈允许动态扩展的情况下,无法申请到足够的内存. 操作数栈 一个先进后出的栈,主要处理运算操作. 例如: 123int a= 1;int b= 2;int count = a+b; 过程: ① 压入 1. ② 压入 2. ③ 执行iadd命令,将栈顶的2个元素相加,即1+2. 本地方法栈和虚拟机栈的区别是: 虚拟机栈是为java方法提供服务,而本地方法栈则是为了native方法提供服务. java堆(重点) 是java虚拟机所管理的最大的一块内存区域.被所有线程共享.唯一目的是存放对象的实例.按照java虚拟机规范:堆中存放着所有的对象实例和数组信息.后续技术更加成熟也有稍微变化的地方. java堆是垃圾收集器管理的主要区域.因此称为GC堆.按照内存回收方面看,可以分为新生代和老年代.比例默认是1/3新生代,2/3老年代. java堆可以存放在不连续的内存空间,只要是逻辑上连续即可.当无法分配更多的实例时,会抛出OutOfMemoryError. 元空间 主要是存放被虚拟机加载的类的信息,常量,静态变量,编译后的代码. jdk1,8之后出现的,主要在本机内存中,更方便了根据情况扩容. 运行时的常量池 运行期的常量池是堆的一部分,用于存放编译期生成的字面量和符号引用. 需要注意的是和局部变量表的局别.同样是存储编译期的数据,不过一个是方法的参数,方法内定义的变量.另一个是存储字面量的常量. 直接内存直接内存不是jvm的内存.是介于本地方法栈中的native方法,与java堆之间的一种机器中的内存.例如在native方法进行io流的时候进行缓存数据的区域.是实现NIO的主要地方. 对象的访问在了解了虚拟机运行时候的数据区域后,下面来看下是如何进行对象访问的. 例如: Object obj = new Object(); 假设这块方法出现在方法体中,其中Object obj会映射到虚拟机栈的栈桢的局部变量表中的reference类型. new Object()则会映射到java堆中.另外java堆中还必须能够查到次对象类型的数据(对象的类型,父类,实现的接口,方法),这些数据存放在方法区.其中reference方法是如何定位到java堆中的?主要有2种方式: 句柄池访问: 屏幕剪辑的捕获时间: 2019/3/16 12:38 在java堆中划分出一块内存作为句柄池,reference存储的是句柄池的地址.句柄池内存储着对象实例的数据和对象类型的数据. 主要优点是:对象进行移动时,只需要改变句柄池中的对象实例信息. 直接指针访问方式 reference存储的是对象实例的数据,这是就要考虑如何放置对象类型的数据的地址.主要优点是节省了一次指针的定位,速度更快.sun 的hotspot所用的虚拟机主要是采用此方案. OOM异常java堆溢出通过-Xms指定最小值,-Xmx指定最大值 提示是java heap space,查看对象信息,确定是否是要用的,如果对象必须要或者,则是内存溢出,此时加大虚拟机内存.如果对象不必须活着,则是内心泄露,此时定位到对象点,进行处理. 虚拟机栈和本地方法栈溢出通过-Xss128k,表示指定大小为128k,主要有2种异常 线程请求的深度大于虚拟机允许的最大深度:Stack OverflowError. 虚拟机在扩展栈时,无法请求到足够的内存:OOM 运行时常量池溢出方法区内存,指定大小和方法区一致.提示信息是:PermGen space . 方法区溢出(1.8之后移除)通过-XX:PerSize和-XX:MaxPermSize指定大小,提示信息是:PermGen space ,方法区主要存储的是Class的信息,如类名,修饰符,常量池,字段描述,方法描述等.(1.8之后已经失效) 元空间在jdk1.8之后,hotspot对jvm架构进行了改变,将类的元数据放在了本地内存里面,静态变量和常量池放在了java堆中,移除了永久代,元数据是描述数据之间的关系的数据,其表现形式是1.5之后的注解,1.5之前的xml文件.由于在本地内存中,可以避免两个项目引用了同样的jar包,会出现大量的相同类信息引发的oom异常.避免了与老年代的资源竞争. 可用-XX:MetaspaceSize 和 -XX:MaxMetaspaceSize指定元数据大小 本机直接内存溢出通过-XX:MaxDirectMemorySize指定大小,仅是抛出OOM溢出.]]></content>
      <categories>
        <category>jvm</category>
        <category>jvm内存模型</category>
      </categories>
      <tags>
        <tag>java内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm之字节码执行引擎]]></title>
    <url>%2F2019%2F03%2F04%2Fjvm%E4%B9%8B%E5%AD%97%E8%8A%82%E7%A0%81%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E%2F</url>
    <content type="text"><![CDATA[概述从宏观上来说,java虚拟机的执行引擎基本上都是按照输入字节码,字节码解析,输出执行结果.从概念模型上来说,分为方法的调用和字节码的执行.下面详细描述: 运行时的栈桢结构2.1 简介栈桢是支持方法调用和字节码执行的数据结构.他是虚拟机运行时数据区的虚拟机栈的栈元素.栈桢存储着局部变量表,操作数栈,动态连接和方法返回地址等信息.每一个方法的调用到完成都对应着虚拟机栈的入栈到出栈的过程.在代码编译的时候,栈桢的深度就已经确定了,因此一个栈需要多少内存,不会受到运行期数据的影响. 在活动线程中,只有最顶层的当前栈桢是有效的.引擎中所有的字节码指令都是针对当前栈桢的. 2.2 局部变量表一组变量值存储空间.用于存放方法的参数和方法内部定义的变量.最小单位是slot.一个slot可以存放32位的数据类型,比如:short,byte,boolean,char,float,int,refearce(对象实例的引用).而对于64位的long和double,则会使用2个连续的slot来存储. 注意:java中的long和double都是非原子性的,在32位操作系统上,即读写都是分离的.每次读32位数据.因此在高并发性,需要volatile关键字标注. 但是在局部变量表里面,局部变量表是建立在线程的栈桢上,单线程私有,因此不会要安全问题. slot顺序: 虚拟机通过索引使用局部变量表.方法在执行时,第0位索引是当前实例变量的引用,即this关键字.其余则会按照顺序排序. 为节省栈桢空间,slot是可以重复使用的.即一个变量即使后续确定不再使用,则也不会立即回收其空间. 对一个变量赋值null使其被回收是没有意义的,因此在经过JIT编译器后,赋值null的操作会被清除掉. 局部变量表没有在类加载准备阶段对变量赋值,因此如果不赋值,会在编译期间报错. 2.3 操作数栈操作栈是一个先入后出的栈.可存储任意类型的数据.32位的栈容量为1,64位的为2.操作栈的深度不会超过max_stacks的值.在方法执行的时候,操作栈是空的,随着方法的执行,不断进行入栈和出栈操作. 例子:int a +int b ,在执行时a和b在栈顶的2个位置,然后字节码指令iadd,栈顶2个元素出栈,则会对其相加,然后将结果入栈. 在概念模型中,两个栈是独立的,但是虚拟机做了一些处理,2个栈可能会共享一部分数据.以便在调用方法时,无需再进行参数的复制传递.称之为:基于栈的执行引擎. 2.4 动态连接 每个栈桢都包含一个指向运行期常量池中该栈桢所属方法的引用.这个引用是为了动态连接. 2.5 方法返回地址 方法开始执行后,会有2条退出指令. ① 正常退出:遇到方法返回的字节码指令.栈桢中存储了PC计数器的值,作为返回地址 ② 异常退出: athrow 字节码指令.通过异常处理器表来确定返回地址. 方法调用方法调用不是方法执行,方法调用唯一的任务是确定调用那个方法.因为在编译阶段,没有连接过程,只是进行了符号引用,而没有真正分配内存布局的入口地址(直接引用).这个特性给java带来了强大的动态加载能力,只有在类加载期间,甚至是执行期间才能确定目标方法的直接引用. 3.1 解析:在解析阶段,会有一部分符号引用变为直接引用: 包括:静态方法,私有方法,实例构造器,父类方法4中.再加上final修饰的方法 3.2 分派① 静态分派:依靠静态类型来执行方法执行版本的分配动作.典型的应用是方法的重载,会自动加载适合的版本. ② 动态分派:典型额应用是方法的重写.即在运行期才会确定方法执行的版本.]]></content>
      <categories>
        <category>jvm</category>
        <category>jvm之字节码执行引擎</category>
      </categories>
      <tags>
        <tag>字节码执行引擎</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK 1.8新特性之Lambda和Stream]]></title>
    <url>%2F2019%2F03%2F04%2FJDK8%E4%B9%8BLambda%E4%B8%8EStream%2F</url>
    <content type="text"><![CDATA[JDK 1.8新特性之Lambda和Stream1.Lambda表达式1.1 lambda简介lambda是为了简化代码,代替一些匿名类而推出的. 1.2公式123456(Type1 param1, Type2 param2, ..., TypeN paramN) -&gt; &#123; statment1; statment2; //............. return statmentM;&#125; 1.3 用例1 创建线程12345678910// Java 8之前：new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;Before Java8, too much code for too little to do&quot;); &#125;&#125;).start();//Java 8方式：new Thread( () -&gt; System.out.println(&quot;In Java8, Lambda expression rocks !!&quot;) ).start(); 2 列表的循环12345678910111213// Java 8之前：List features = Arrays.asList(&quot;Lambdas&quot;, &quot;Default Method&quot;, &quot;Stream API&quot;, &quot;Date and Time API&quot;);for (String feature : features) &#123; System.out.println(feature);&#125;// Java 8之后：List features = Arrays.asList(&quot;Lambdas&quot;, &quot;Default Method&quot;, &quot;Stream API&quot;, &quot;Date and Time API&quot;);features.forEach(n -&gt; System.out.println(n)); // 使用Java 8的方法引用更方便，方法引用由::双冒号操作符标示，// 看起来像C++的作用域解析运算符features.forEach(System.out::println); 3 结合函数式接口Predicate1234567891011121314151617181920212223242526public static void main(args[])&#123; List languages = Arrays.asList(&quot;Java&quot;, &quot;Scala&quot;, &quot;C++&quot;, &quot;Haskell&quot;, &quot;Lisp&quot;); System.out.println(&quot;Languages which starts with J :&quot;); filter(languages, (str)-&gt;str.startsWith(&quot;J&quot;)); System.out.println(&quot;Languages which ends with a &quot;); filter(languages, (str)-&gt;str.endsWith(&quot;a&quot;)); System.out.println(&quot;Print all languages :&quot;); filter(languages, (str)-&gt;true); System.out.println(&quot;Print no language : &quot;); filter(languages, (str)-&gt;false); System.out.println(&quot;Print language whose length greater than 4:&quot;); filter(languages, (str)-&gt;str.length() &gt; 4);&#125; public static void filter(List names, Predicate condition) &#123; for(String name: names) &#123; if(condition.test(name)) &#123; System.out.println(name + &quot; &quot;); &#125; &#125;&#125; 4.Lambda中this的概念Lambda中的this指的是声明它的外部对象,不是闭包里面的. 1234567891011121314151617181920212223242526public class WhatThis &#123; public void whatThis()&#123; //转全小写 List&lt;String&gt; proStrs = Arrays.asList(new String[]&#123;&quot;Ni&quot;,&quot;Hao&quot;,&quot;Lambda&quot;&#125;); List&lt;String&gt; execStrs = proStrs.stream().map(str -&gt; &#123; System.out.println(this.getClass().getName()); return str.toLowerCase(); &#125;).collect(Collectors.toList()); execStrs.forEach(System.out::println); &#125; public static void main(String[] args) &#123; WhatThis wt = new WhatThis(); wt.whatThis(); &#125;&#125;输出：com.wzg.test.WhatThiscom.wzg.test.WhatThiscom.wzg.test.WhatThisnihaolambda 1.4 性能对比1. 遍历66万个对象对比.1234567891011121314151617ArrayList&lt;user&gt; objects = new ArrayList&lt;&gt;(); for(int i1=0;i1&lt;666666;i1++)&#123; objects.add(new user(&quot;dcx&quot;,&quot;12&quot;)); &#125; System.out.println(&quot;对象大小是:&quot;+objects.size()); long start = System.currentTimeMillis(); objects.forEach (n-&gt; &#123; System.out.println(n.name); &#125;); long end = System.currentTimeMillis(); long start1 = System.currentTimeMillis(); for (user u:objects)&#123; System.out.println(u.name); &#125; long end1 = System.currentTimeMillis(); System.out.println(&quot;lambda耗时=&quot;+ (end-start)); System.out.println(&quot;普通方法耗时=&quot;+ (end1-start1)); 结果: 12lambda耗时=2978普通方法耗时=2971 1.5 函数式接口为了支持Lambda表达式,推出了函数式接口,注解是@FunctionInterface,其本质是一个接口.个人理解是,先定义一个数学中的公式(y=x+b),然后,在使用的时候,直接通过接口使用,而不用去实现这个接口. 1.特点a.只能有一个抽象方法. b.可以定义静态方法. c.可以定义一个或者多个默认方法. 2.demo//目前研究较浅,具体使用例子还无法定位. 1234567891011121314151617181920@FunctionalInterfacepublic interface AvageUtil&lt;Y,X,A&gt; &#123; Y avage(X x,A a); //求平均值 default int avager(int x,int a)&#123; return (a+x)/2; &#125; static void demo(int a)&#123; System.out.println(a); &#125; public static void main(String[] args) &#123; AvageUtil avageUtil=(x,a)-&gt;(int) x+ (int) a; //结果是8 System.out.println(avageUtil.avage(3,5)); //结果是4 System.out.println(avageUtil.avager(3,5)); //结果是3 AvageUtil.demo(3); &#125;&#125; 3.jdk中的函数式接口1.对于单个入参 1234567891011121314151617181920212223242526272829public class FunctionalInterfaceMain &#123; public static void main(String[] args) &#123; //接收一个T类型的参数，返回一个R类型的结果 Function&lt;String,String&gt; function1 = item -&gt; item +&quot;返回值&quot;; // 接收一个T类型的参数，不返回值 Consumer&lt;String&gt; function2 = iterm -&gt; &#123;System.out.println(iterm);&#125;;//lambda语句，使用大括号，没有return关键字，表示没有返回值 // 接收一个T类型的参数，返回一个boolean类型的结果 Predicate&lt;String&gt; function3 = iterm -&gt; &quot;&quot;.equals(iterm); //不接受参数，返回一个T类型的结果 Supplier&lt;String&gt; function4 = () -&gt; new String(&quot;&quot;); /** * 再看看怎么使用 * demo释义： * 1、创建一个String类型的集合 * 2、将集合中的所有元素的末尾追加字符串&apos;1&apos; * 3、选出长度大于2的字符串 * 4、遍历输出所有元素 */ List&lt;String&gt; list = Arrays.asList(&quot;zhangsan&quot;,&quot;lisi&quot;,&quot;wangwu&quot;,&quot;xiaoming&quot;,&quot;zhaoliu&quot;); list.stream() .map(value -&gt; value + &quot;1&quot;) //传入的是一个Function函数式接口 .filter(value -&gt; value.length() &gt; 2) //传入的是一个Predicate函数式接口 .forEach(value -&gt; System.out.println(value)); //传入的是一个Consumer函数式接口 &#125; &#125; 2.对于多个入参 1234567891011121314151617181920212223242526272829303132333435363738394041public class FunctionalInterfaceTest &#123; public static void main(String[] args) &#123; /** * Bi类型的接口创建 */ //接收T类型和U类型的两个参数，返回一个R类型的结果 BiFunction&lt;String, String, Integer&gt; biFunction = (str1,str2) -&gt; str1.length()+str2.length(); //接收T类型和U类型的两个参数，不返回值 BiConsumer&lt;String, String&gt; biConsumer = (str1,str2) -&gt; System.out.println(str1+str2); //接收T类型和U类型的两个参数，返回一个boolean类型的结果 BiPredicate&lt;String, String&gt; biPredicate = (str1,str2) -&gt; str1.length() &gt; str2.length(); /** * Bi类型的接口使用 */ int length = getLength(&quot;hello&quot;, &quot;world&quot;, (str1,str2) -&gt; str1.length() + str2.length()); //输出10 boolean boolean1 = getBoolean(&quot;hello&quot;, &quot;world&quot;, (str1,str2) -&gt; str1.length() &gt; str2.length()); //输出false System.out.println(length); System.out.println(boolean1); noResult(&quot;hello&quot;, &quot;world&quot;, (str1,str2) -&gt; System.out.println(str1+&quot; &quot;+str2)); //没有输出 &#125; public static int getLength(String str1,String str2,BiFunction&lt;String, String, Integer&gt; function)&#123; return function.apply(str1, str2); &#125; public static void noResult(String str1,String str2,BiConsumer&lt;String, String&gt; biConcumer)&#123; biConcumer.accept(str1, str2); &#125; public static boolean getBoolean(String str1,String str2,BiPredicate&lt;String, String&gt; biPredicate)&#123; return biPredicate.test(str1, str2); &#125;&#125; 2.stream API1.stream是什么?stream是元素的集合,可以类比于iterator,并可以支持顺序或者并行的对原stream进行汇聚的操作.相比于iterator的逐个去操作每个集合的元素,stream支持对所有元素的一口气操作.比如:求平均值,找出大于10的个数等等. 2.操作流程示例//找出大于3的个数 12List&lt;Integer&gt; nums = Lists.newArrayList(1,2,3,4,5,6);nums.stream().filter(num -&gt; num &gt;3).count(); 3.创建1.通过Stream的静态工厂方法. 2.通过collection接口的方法.col.stream(). 4.功能 主要针对的是对Collection集合的操作.详情可看Stream源码. 4.性能对比//求平均值,6千万个数据 123456789101112131415161718ArrayList&lt;Integer&gt; objects = new ArrayList&lt;&gt;(); for (int i=0;i&lt;66666666;i++)&#123; objects.add(i); &#125; //求平均值 //传统循环 long start = System.currentTimeMillis(); long n=0; for (int i=0;i&lt;objects.size();i++)&#123; n+=objects.get(i); &#125; System.out.println(n/objects.size()); System.out.println(&quot;传统方式耗时:&quot;+(System.currentTimeMillis()-start)); //stream API long start1 = System.currentTimeMillis(); IntSummaryStatistics intSummaryStatistics = objects.stream().mapToInt((x) -&gt; x).summaryStatistics(); System.out.println(intSummaryStatistics.getAverage()); System.out.println(&quot;stream API 耗时:&quot;+(System.currentTimeMillis()-start1)); 结果:stream耗时会比传统方法长. 123433333332传统方式耗时:1263.33333325E7stream API 耗时:195]]></content>
      <categories>
        <category>java基础</category>
        <category>JDK1.8</category>
      </categories>
      <tags>
        <tag>Lambda</tag>
        <tag>Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm类加载机制]]></title>
    <url>%2F2019%2F03%2F04%2Fjvm%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[1.概述得知class文件是一段规则有序的二进制流后,类加载流程简述是: 虚拟机将二进制流加载到内存,对数据进行校验,转换解析和初始化,最后形成可以直接被虚拟机使用的java类型. 需要注意的是,java的类型的加载,连接,初始化都是在运行期执行的,从而提供了动态扩展的特性. 由于动态加载的特性,其中解析动作不一定在初始化之前执行. 2.类加载过程2.1 加载① 通过一个类的权限定名称获取这个类的二进制字节流 ② 将这个字节流所代表的静态存储结构转化为方法区的运行时的数据结构 ③ 在内存中生成一个java.lang.class对象,作为方法区这个类的各种数据的访问接口 注: 在类创建的时候,并不包括数组,数组是由java虚拟机直接创建的,不经过类加载器. 但是数组中的元素类型,则会通过类加载器创建. 2.2 验证验证主要是保证字节流中的信息不回危害虚拟机本身,并且符合虚拟机的规范要求.主要有4个方面的验证: ① 文件格式验证: 对开头的二进制魔数进行验证,对主版本号的验证,等等. 目的是保证二进制能够正确解析,存到方法区之内. ② 元数据验证:是否符合java语言规范,比如继承了一个被final修饰的类. ③ 字节码验证: 通过分析数据流和控制流,确定语义是否合法并且符合逻辑. ④ 符号引用验证: 可以看做是对类自身外的信息进行的匹配性校验,比如:是否可以通过权限定名找到相应的类,或者符号引用中的方法字段的可访问性,是否为public或者private等. 2.3 准备此阶段是为类变量分配内存并设置初始值的阶段.这里分配的内存都在方法区进行.并且只会实例化static修饰的变量.类实例变量会在进行类初始化的时候,分配在堆内存中.需要注意的是,private static int a=1;在这句代码中设置的初始值为0,并不是1, ①在类进行初始化后,会为1. ② private static final int a=1,这时也是1. 下面是基本数据类型的零值(准备阶段初始值): 2.4 解析将符号引用变换为直接引用. ① 符号引用: 和内存无关,一组用来描述所引用目标的一组符号. ② 直接引用: 和内存有关,直接引用的数据,在内存中肯定存在.可以是指针,偏移量或者句柄. 主要解析的符号引用为: （1） 类或者接口 （2） 字段 （3） 类方法 （4） 接口方法 （5） 方法类型 （6） 方法句柄 （7） 调用点限定符 2.5 初始化根据java类中的代码进行初始化.也可以说是执行类构造器()方法的执行过程. 类构造器()的具体流程是: （1） 由编译器自动收集类中的所有类变量的赋值动作和静态语句块中的语句合并产生. （2） 和类构造函数不同,不会去调用父类构造器,因为虚拟机会保证父类构造器在子类之前,肯定会执行完.因此第一个执行的类构造器是java.lang.Object. （3） 如果一个类中没有静态语句块和变量赋值操作,则不会生成类构造器. （4） 接口中的如果没有用到父类的变量,则不会生成父类构造器. （5） 虚拟机会保证类构造器在多线程环境下的加锁与同步. 3.类加载器3.1 概述通过一个类的全限定名称来获取类的二进制流的动作 3.2 类与类加载器一个类的唯一性是由同一个类加载器和和这个类本身共同决定的. 一个简单的类加载器: 用这个类加载器和系统加载出来的类,不是同一个类. 3.3 双亲委派模型对于java开发人员来说,有3种系统提供的类加载器: （1） 启动类加载器(bootstrap classLoder):加载/lib下的类,不能被java程序直接引用 （2） 扩展类加载器(extension_classLoder): 由sun.misc.Launcher$ExtClassLoder实现,加载/lib/ext下的类,java程序可以直接使用 （3） 应用程序类加载器(Application classLoder): 由sun.misc.Launcher$app-ClassLoder实现,加载用户类路径下的类.可以直接使用,并且默认类加载器为这个 类加载器层次的关系可以用双亲委派模型来说明,不过层级间不是父子继承关系,而是组合关系.当然这个关系模型是java设计者所推荐的模型. 双亲委派工作流程是: 当一个类加载器接收到加载类的请求时,不会立马去加载这个类,而是将这个请求委派给父类,一直到启动类加载器.当父类无法搜索到这个类时,最终类加载器才会去加载. 3.4 双亲委派模型的破坏1) 在这个模型出现之前,可以继承java.lang.ClassLoder,重写loadClass(),后续建议只有在loadClass()加载失败后,然后再去加载自己的类. 2) 线程上下文类加载器的出现.线程还没创建时,他是父类加载器,如果全局没有设置,则会默认为应用加载器.从而实现了父类加载器请求子类去加载类.所有的SPI操作都是这个原理,比如常见的jdbc,jndi 3) 热部署话等动态的追求,OSGN的java模块化标准.使得每一个模块都有独自的类加载器.双亲委派模型成为一个网状结构,收到类加载请求时,按照以下的标准执行:]]></content>
      <categories>
        <category>jvm</category>
        <category>jvm类加载机制</category>
      </categories>
      <tags>
        <tag>类加载机制</tag>
        <tag>双亲委派模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm之类文件结构]]></title>
    <url>%2F2019%2F03%2F04%2F%E7%B1%BB%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[1.1 机器码与字节码①机器码(Native code): 可由CPU直接读取操作的机器指令.处于最底层,不需要编译.速度最快.不可跨平台.依靠硬件存在. ②字节码:是一种中间码，它比机器码更抽象，需要直译器转译后才能成为机器码的中间代码。依靠平台存在. 以前用机器码操作系统，现在是用字节码操作机器码，进一步操作系统,使得语言跨平台成为可能. 1.2.跨平台原理:java语言一如既往的保持着”一次编写,到处运行的特性”,依靠的就是其JVM.JVM不关注语言的来源,只关注的是”CLASS 二进制文件”,只要符合JVM的类的编写规范,就可以经过JVM进行字节码到机器码的转换. 因此, 功能性比较: java或者其他语言&gt;字节码&gt;机器码 速度上:相反 1.3.class文件结构简介:1.3.1.class内容格式Class文件是一组以8位字节为单位的二进制流,各个数据项目以严格的顺序紧凑的排列在CLass文件中,中间没有分隔符.因此文件中的数据全是必要的数据.当遇到需要占用8个字节以上的数据时,则会按照高位在前的方式分割为若干个8位字节进行存储. ①.高位在前,高位字节在地址最低位 例如:32位下,int i=10 低位在前为: 00001010 00000000 00000000 00000000 而在高位优先的内存中： 00000000 00000000 00000000 00001010 字节码本身是无序的,需要有一定的规则去读取数据,高位优先更符合人的操作习惯,但是在X86处理器采用的是低位优先的规则. 1.3.2.class 结构CLass文件中定义了类似于C语言的2种数据类型:无符号数和表. ①.无符号数是最基本的数据类型,用u1,u2,u4,u8分别表示1,2,3,8个字节. ②.表是由无符号数和表构成的复合数据类型.习惯性的以”_info”结尾. 因此Class文件本质上是由众多无符号数组成的一张表. 下面贴上一张结构组成图: 因此,一个class里面会有1个4个字节的magic+1个2个字节的minor_version……..组成.其中具体含义后续详细介绍. 1.3.2.1 magic 魔数在对一个文件进行类型判断时,比如是.png还是.jpg,文件存储标准中都会使用魔数来进行判断.同样,判断是否为一个class文件,也使用了这个标准.对于class文件,开头的前4个字节为魔数,值为:0XCAFEBABE,用来表示这是一个class文件.紧接着魔数后面的4个字节存储着class文件的版本号,5,6位次版本号,7,8是主版本号.高版本的JDK会向下兼容以前版本的class文件,但是会拒绝执行超过其版本的class文件,即使文件格式没有发生变化. 对一个class文件,用16进制编辑器打开,可以看到上面的结果. 第一行的0xCAFEBEBE为文件类型标志,5,6为0x0000次版本号,7,8为0x0032,十进制为50,表示可被1.6以上的jdk进行编译.下面是jdk和class文件版本对应表: 1.3.2.2 constant_pool (常量池)紧接着主版本号之后,便是常量池的入口,可以把常量池比作class文件的资源仓库.这是文件中第一个表数据结构,由于常量的个数不固定,所以在常量池前面会有一个u2类型的数据,constant_pool_count来表示类中定义的常量池的个数.不过常量池会将索引为0的位置留作备用,用来满足指向常量池的索引值需要表示为”不引用任何常量池”的意思.因此常量池的实际个数为constant_pool_count-1. 常量池主要有2中结构:字面量和符号引用. ①.字面量:java语言层面的常量,如文本字符串,final声明的常量等. ②.符号引用:编译方面的概念,主要有类和接口的全限定名,字段的名称和描述符,方法的名称和描述符. 在C/C++,会经历编辑,编译,链接,运行阶段,但是java中没有链接这一阶段,而是采用的是在虚拟机加载类文件的时候动态连接.意思是class文件中不会保存方法字段的占用内存信息,只有在运行期间进行转换才会得到.这部分内容在类加载机制中会详细说明. 常量池中每一个常量都是一张表,开始的第一位是u1类型的标志位,对应关系如下: 每个常量又都会有各自的结构组成, 具体class字节码分析工具,可以用jdk下的bin目录下,javap工具,通过执行: javap -verbose ttclass ​ 可将class转化为字节码,输出. 1.3.2.3 访问标志主要是用于标志一些类和接口层次的信息,占用2个字节 例如:一个普通的public类,access_flags的值为:0x0001+0x0020=0x0021 1.3.2.4 类索引,父类索引,接口索引访问标志后面是这些索引的信息,其中类索引和父类索引是一个u2类型的数据,接口索引是一组u2类型的数据. 顺序为:类索引+父类索引+接口索引大小+接口索引 1.3.2.5 字段表集合字段表是用于描述类或者接口中声明的变量.字段主要有类级变量和实例级变量,不包括方法内部声明的变量. 其中access_flag和类中的访问标志相似.可以设置的值如下: 然后name_index和descriptor_index分别代表了简单名称和描述符; 概念: ① 全限定名:类名中的点变成”/“,例如:org/da/dad/aa.class ② 简单名称: 没有类型和参数修饰的字段或者方法,inf()简称为inf ③ 描述符: 描述字段的数据类型.基本数据类型通常用一个大写字母表示,对象类型用”L+全限定名”来表示. 对于数组类型,一维数组使用[描述,例如int[],用’[I’表示.二维的用’[[‘表示,例如:Integer[][]用[[I表示. 后续的attribute_count和attribute只有在给字段添加默认值的时候,才会显示. 1.3.2.6 方法表集合字段表后面跟着的是方法表,两者很类似, 其中的访问标志选项为: 顺序为:方法数量+访问标志+…(结构体) 其中,方法里面的代码在编译成字节码后,会存放在方法属性表里面,对应的key为Code. 1.3.2.7 属性表集合和其他结构不同,属性表不要求有严格的顺序,长度和内容.只需要和已有的属性名不同即可. java虚拟机定义了21个预定义属性,详情可看书中,暂不一一列举. 对于每一个属性,都会用一个constant_utf8_info属性来表示. 其中简要说下Code属性: code属性是方法里面的代码编译后的字节码: ① attribute_name_index : constant_utf8_info的常量,固定值为”code”,表示属性名称. ② attribute_length: 属性长度,为6个字节. ③ max_stack: 操作栈的深度.java虚拟机会根据这个深度来分配. ④ max_locals: 局部变量表所需要的空间. ⑤ code_length ,code: java源文件编译后的字节码指令. 1.4 字节码指令java虚拟机的指令是由一个字节的,代表某种操作含义的数字(操作码)+(0,n)个代表此操作所需的参数(操作数)组成.但是由于java虚拟机的架构是面向操作数栈而不是面向寄存器的,因此一般都是只有一个操作码组成. 一个字节的操作码长度为:0-255. 因此java虚拟机的执行基础模型为: 1.4.1 字节码与数据类型 在java虚拟机指令集中,大多都会携带其操作所需的数据类型,但是并非所有的数据类型都会有对于的指令.会有一些单独的指令在必要的时候,将不支持的类型转化为支持的.大体上可划分为9个指令. 1.4.2 加载和存储指令主要将数据在栈帧中的局部变量表和操作数栈之间传输,主要包括: ① 将一个局部变量加载到操作栈:iload等; ② 将一个数值从操作栈存储到局部变量表: istore等 ③ 将一个常量加载到操作栈:bipush 1.4.3 运算指令用于对操作数栈上的数值进行某种运算,然后把结果存到栈顶. 注意:java虚拟机没有操作byte,boolean,short,char的指令,最后都会转化为int指令来特殊操作. 1.4.4 类型转化指令 java数据类型及其大小,在进行转化的时候,是默认支持小范围向大范围转化的.而在大范围向小范围转化时,需要进行强制转化. 1.4.5 对象的创建以及访问指令数组和类实例用到了不同的创建指令. 1.4.6 操作数栈管理指令和操作普通的堆栈一样,java虚拟机提供了用于直接操作操作数栈的指令: ① 出栈: pop,pop2 ② 复制栈顶数值,并重新压入栈顶,dup,dup2 ③ 将栈最顶端的2个值互换,swap 1.4.7 控制转移指令有条件或者无条件的修改寄存器的值: 1.4.8 方法调用和返回指令 1.4.9 异常处理指令显示抛出异常的语句,是由athrow指令实现,并且除了显示跑出异常,java虚拟机也会有一些指令进行异常检测. 处理异常(catch语句),没有字节码指令,这部分功能是由异常表来完成 1.4.10 同步指令包括2种同步方式,都是由管程(Monitor)支持: ① 方法级同步: 不通过字节码来操作,是在方法的调用和返回中实现. 流程是:从常量池中取出acc_synchronized得知是否为同步方法,如果是,当前线程则会持有管程,方法执行完后,释放管程.同一个管程只能在一个线程中存在. ② 同步一段指令集序列: 是由java语言的synchronized语句块表示,这个关键字由2个指令支持:monitorenter和monitorexit 例如: 以下代码的字节码为:]]></content>
      <categories>
        <category>jvm</category>
        <category>jvm之类文件结构</category>
      </categories>
      <tags>
        <tag>字节码</tag>
        <tag>字节码指令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java之线程,线程池]]></title>
    <url>%2F2019%2F03%2F03%2Fjava%E4%B9%8B%E7%BA%BF%E7%A8%8B-%E7%BA%BF%E7%A8%8B%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[线程与进程&nbsp;&nbsp; 进程是系统进行资源分配和调度的独立单位,一个进程下可以包括多个线程.一个进程在执行时,总需要多个子任务去执行,这就需要多线程.最大的利用CPU的空闲时间去执行更多的任务. 创建线程继承Thread类12345678910111213public class StringUtil extends Thread&#123; @Override public void run() &#123; System.out.println(this.getName()); super.run(); &#125; public static void main(String[] args) &#123; for(int i=0;i&lt;100;i++) &#123; new StringUtil().start(); &#125; &#125; &#125; 实现runnable接口123456789101112public class Aa implements Runnable&#123; @Override public void run() &#123; System.out.println(Thread.currentThread().getName()); &#125; public static void main(String[] args) &#123; for(int i=0;i&lt;100;i++) &#123; new Thread(new Aa()).start(); &#125; &#125;&#125; 两种方法对比 看源码1234567891011121314151617publicclass Thread implements Runnable &#123; /* Make sure registerNatives is the first thing &lt;clinit&gt; does. */ private static native void registerNatives(); static &#123; registerNatives(); &#125; private volatile String name; private int priority; /* Whether or not the thread is a daemon thread. */ private boolean daemon = false; /* Fields reserved for exclusive use by the JVM */ private boolean stillborn = false; private long eetop; 在Thread源码中,Thread是Runnable的实现.因此可以得出, a) 我们如果使用继承Thread类,系统已经给我们经过继承,封装好了.可以直接使用. b) 我们如果直接使用实现Runnable接口方式,需要将我们的类放进Thread方法中,去生成一个Thread类,从而具有Thread的所有功能. 由于两种方法基本上是一样的,但是java是单继承的,为了不会影响到我们写的类去继承其他类,所以推荐使用第二种方法. 从demo可知,run()方法是线程中我们需要实现的业务.start()方法是开启一个线程. start()方法源码:1234567891011121314151617181920212223242526272829303132public synchronized void start() &#123; /** * This method is not invoked for the main method thread or &quot;system&quot; * group threads created/set up by the VM. Any new functionality added * to this method in the future may have to also be added to the VM. * * A zero status value corresponds to state &quot;NEW&quot;. */ if (threadStatus != 0) throw new IllegalThreadStateException(); /* Notify the group that this thread is about to be started * so that it can be added to the group&apos;s list of threads * and the group&apos;s unstarted count can be decremented. */ group.add(this); boolean started = false; try &#123; //开启线程 start0(); started = true; &#125; finally &#123; try &#123; if (!started) &#123; group.threadStartFailed(this); &#125; &#125; catch (Throwable ignore) &#123; /* do nothing. If start0 threw a Throwable then it will be passed up the call stack */ &#125; &#125; &#125; &nbsp;&nbsp;可知真正开启一个线程的方法是start0(),并且在创建的时候,用synchronized关键字,进行锁住;同时,可知为了进行线程的管理,将创建的线程加入了线程组. Thread类 线程的方法（Method）、属性（Property） 1）优先级（priority） &amp;nbsp;&amp;nbsp;每个类都有自己的优先级，一般property用1-10的整数表示，默认优先级是5，优先级最高是10；优先级高的线程并不一定比优先级低的线程执行的机会高，只是执行的机率高；默认一个线程的优先级和创建他的线程优先级相同； 2）Thread.sleep()/sleep(long millis) &amp;nbsp;&amp;nbsp;当前线程睡眠/millis的时间（millis指定睡眠时间是其最小的不执行时间，因为sleep(millis)休眠到达后，无法保证会被JVM立即调度）；sleep()是一个静态方法(static method) ，所以他不会停止其他的线程也处于休眠状态；线程sleep()时不会失去拥有的对象锁。 作用：保持对象锁，让出CPU，调用目的是不让当前线程独自霸占该进程所获取的CPU资源，以留一定的时间给其他线程执行的机会； 3）Thread.yield() &amp;nbsp;&amp;nbsp; 让出CPU的使用权，给其他线程执行机会、让同等优先权的线程运行（但并不保证当前线程会被JVM再次调度、使该线程重新进入Running状态），如果没有同等优先权的线程，那么yield()方法将不会起作用。 4）thread.join() 使用该方法的线程会在此之间执行完毕后再往下继续执行。 5）object.wait() 当一个线程执行到wait()方法时，他就进入到一个和该对象相关的等待池(Waiting Pool)中，同时失去了对象的机锁—暂时的，wait后还要返还对象锁。当前线程必须拥有当前对象的锁，如果当前线程不是此锁的拥有者，会抛出IllegalMonitorStateException异常,所以wait()必须在synchronized block中调用。 6）object.notify()/notifyAll() 唤醒在当前对象等待池中等待的第一个线程/所有线程。notify()/notifyAll()也必须拥有相同对象 锁，否则也会抛出IllegalMonitorStateException异常。 7）Synchronizing Block Synchronized Block/方法控制对类成员变量的访问；Java中的每一个对象都有唯一的一个内置的锁，每个Synchronized Block/方法只有持有调用该方法被锁定对象的锁才可以访问，否则所属线程阻塞；机锁具有独占性、一旦被一个Thread持有，其他的Thread就不能再拥有（不能访问其他同步方法），方法一旦执行，就独占该锁，直到从该方法返回时才将锁释放，此后被阻塞的线程方能获得该锁，重新进入可执行状态。 注: 线程组是一个进程下所有的线程管理类,线程以树形的结构存储 线程池是为了管理线程的生命周期，复用线程，减少创建销毁线程的开销 继续查看:源码: 1private native void start0(); 可知最终是调用的原生C/C++方法去开启的一个线程. 线程池: ThreadPoolExecutor&nbsp;&nbsp;ThreadPoolExecutor是jdk1.5之后package java.util.concurrent;官方提供的线程池创建类.主要负责线程的调度,任务的执行,线程池的管理等. 构造方法: 12345678 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler);&#125; 参数 说明 核心线程数. 线程池中一直保持存活的线程数. 最大线程数 线程池中活跃的最大线程数.对于cpu密集型的应用,设置为N+1,io密集型的应用,设置为2N+1.N为CPU核心数. 线程池队列 当核心线程数满了后,继续创建线程,会进入线程池队列中. 线程存活时间 超出核心线程数的多余的线程,在空闲时存活的的时间. 线程创建工厂 使用工厂模式,为线程池创建线程. 拒绝策略 在最大线程数也满了后,再次添加线程,则会拒绝,一般有4中策略. 重点讲解：其中比较容易让人误解的是：corePoolSize，maximumPoolSize，workQueue之间关系。 当线程池小于corePoolSize时，新提交任务将创建一个新线程执行任务，即使此时线程池中存在空闲线程。 当线程池达到corePoolSize时，新提交任务将被放入workQueue中，等待线程池中任务调度执行 当workQueue已满，且maximumPoolSize&gt;corePoolSize时，新提交任务会创建新线程执行任务 当提交任务数超过maximumPoolSize时，新提交任务由RejectedExecutionHandler处理 当线程池中超过corePoolSize线程，空闲时间达到keepAliveTime时，关闭空闲线程 当设置allowCoreThreadTimeOut(true)时，线程池中corePoolSize线程空闲时间达到keepAliveTime也将关闭 拒绝策略: a. AbortPolicy:丢弃线程,并抛异常.(默认) b. DiscardPolicy: 丢弃线程,不抛出异常, c. DiscardOldestPolicy: 丢弃最前面的线程,并尝试运行, d. CallerRunsPolicy: 由调用线程处理. 可见,多余的线程会被丢弃,可以自定义拒绝策略. ​ 官方给出了工具类Executors,package java.util.concurrent;,用来快速创建一些线程池方案. 构造一个固定线程数目的线程池，配置的corePoolSize与maximumPoolSize大小相同，同时使用了一个无界LinkedBlockingQueue存放阻塞任务，因此多余的任务将存在再阻塞队列，不会由 RejectedExecutionHandler处理 12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); &#125; 构造一个缓冲功能的线程池，配置corePoolSize=0，maximumPoolSize=Integer.MAX_VALUE，keepAliveTime=60s,以及一个无容量的阻塞队列 SynchronousQueue，因此任务提交之后，将会创建新的 线程执行；线程空闲超过60s将会销毁 12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;()); &#125; 构造一个只支持一个线程的线程池，配置corePoolSize=maximumPoolSize=1，无界阻塞队列 LinkedBlockingQueue；保证任务由一个线程串行执行 123456public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;())); &#125; 构造有定时功能的线程池，配置corePoolSize，无界延迟阻塞队列DelayedWorkQueue；有意思的是： maximumPoolSize=Integer.MAX_VALUE，由于DelayedWorkQueue是无界队列，所以这个值是没有意义的 123456789101112public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) &#123; return new ScheduledThreadPoolExecutor(corePoolSize); &#125;public static ScheduledExecutorService newScheduledThreadPool( int corePoolSize, ThreadFactory threadFactory) &#123; return new ScheduledThreadPoolExecutor(corePoolSize, threadFactory); &#125;public ScheduledThreadPoolExecutor(int corePoolSize, ThreadFactory threadFactory) &#123; super(corePoolSize, Integer.MAX_VALUE, 0, TimeUnit.NANOSECONDS, new DelayedWorkQueue(), threadFactory); &#125; 注: 工具类中提供的创建线程池,所用的队列都是无界队列,因此会存在OOM问题,当然,也可以自定制自己的线程池方案,自定义拒绝策略:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.ExecutorService;import java.util.concurrent.RejectedExecutionHandler;import java.util.concurrent.ThreadFactory;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicInteger; public class CustomThreadPoolExecutor &#123; private ThreadPoolExecutor pool = null; /** * 线程池初始化方法 * * corePoolSize 核心线程池大小----10 * maximumPoolSize 最大线程池大小----30 * keepAliveTime 线程池中超过corePoolSize数目的空闲线程最大存活时间----30+单位TimeUnit * TimeUnit keepAliveTime时间单位----TimeUnit.MINUTES * workQueue 阻塞队列----new ArrayBlockingQueue&lt;Runnable&gt;(10)====10容量的阻塞队列 * threadFactory 新建线程工厂----new CustomThreadFactory()====定制的线程工厂 * rejectedExecutionHandler 当提交任务数超过maxmumPoolSize+workQueue之和时, * 即当提交第41个任务时(前面线程都没有执行完,此测试方法中用sleep(100)), * 任务会交给RejectedExecutionHandler来处理 */ public void init() &#123; pool = new ThreadPoolExecutor( 10, 30, 30, TimeUnit.MINUTES, new ArrayBlockingQueue&lt;Runnable&gt;(10), new CustomThreadFactory(), new CustomRejectedExecutionHandler()); &#125; public void destory() &#123; if(pool != null) &#123; pool.shutdownNow(); &#125; &#125; public ExecutorService getCustomThreadPoolExecutor() &#123; return this.pool; &#125; private class CustomThreadFactory implements ThreadFactory &#123; private AtomicInteger count = new AtomicInteger(0); @Override public Thread newThread(Runnable r) &#123; Thread t = new Thread(r); String threadName = CustomThreadPoolExecutor.class.getSimpleName() + count.addAndGet(1); System.out.println(threadName); t.setName(threadName); return t; &#125; &#125; private class CustomRejectedExecutionHandler implements RejectedExecutionHandler &#123; @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) &#123; // 记录异常 // 报警处理等 System.out.println(&quot;error.............&quot;); &#125; &#125; // 测试构造的线程池 public static void main(String[] args) &#123; CustomThreadPoolExecutor exec = new CustomThreadPoolExecutor(); // 1.初始化 exec.init(); ExecutorService pool = exec.getCustomThreadPoolExecutor(); for(int i=1; i&lt;100; i++) &#123; System.out.println(&quot;提交第&quot; + i + &quot;个任务!&quot;); pool.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;running=====&quot;); &#125; &#125;); &#125; // 2.销毁----此处不能销毁,因为任务没有提交执行完,如果销毁线程池,任务也就无法执行了 // exec.destory(); try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; &nbsp;&nbsp;&nbsp;&nbsp;方法中建立一个核心线程数为30个，缓冲队列有10个的线程池。每个线程任务，执行时会先睡眠3秒，保证提交10任务时，线程数目被占用完，再提交30任务时，阻塞队列被占用完，，这样提交第41个任务是，会交给CustomRejectedExecutionHandler 异常处理类来处理。&nbsp;提交任务的代码如下：123456789101112131415161718192021222324252627282930313233343536373839public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); /* * Proceed in 3 steps: * * 1. If fewer than corePoolSize threads are running, try to * start a new thread with the given command as its first * task. The call to addWorker atomically checks runState and * workerCount, and so prevents false alarms that would add * threads when it shouldn&apos;t, by returning false. * * 2. If a task can be successfully queued, then we still need * to double-check whether we should have added a thread * (because existing ones died since last checking) or that * the pool shut down since entry into this method. So we * recheck state and if necessary roll back the enqueuing if * stopped, or start a new thread if there are none. * * 3. If we cannot queue task, then we try to add a new * thread. If it fails, we know we are shut down or saturated * and so reject the task. */ int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; else if (!addWorker(command, false)) reject(command); &#125; 注意：41以后提交的任务就不能正常处理了，因为，execute中提交到任务队列是用的offer方法，如上面代码，这个方法是非阻塞的，所以就会交给CustomRejectedExecutionHandler 来处理，所以对于大数据量的任务来说，这种线程池，如果不设置队列长度会OOM，设置队列长度，会有任务得不到处理，接下来我们构建一个阻塞的自定义线程池 定制属于自己的阻塞线程池 :123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138package com.tongbanjie.trade.test.commons;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.ExecutorService;import java.util.concurrent.RejectedExecutionHandler;import java.util.concurrent.ThreadFactory;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicInteger; public class CustomThreadPoolExecutor &#123; private ThreadPoolExecutor pool = null; /** * 线程池初始化方法 * * corePoolSize 核心线程池大小----1 * maximumPoolSize 最大线程池大小----3 * keepAliveTime 线程池中超过corePoolSize数目的空闲线程最大存活时间----30+单位TimeUnit * TimeUnit keepAliveTime时间单位----TimeUnit.MINUTES * workQueue 阻塞队列----new ArrayBlockingQueue&lt;Runnable&gt;(5)====5容量的阻塞队列 * threadFactory 新建线程工厂----new CustomThreadFactory()====定制的线程工厂 * rejectedExecutionHandler 当提交任务数超过maxmumPoolSize+workQueue之和时, * 即当提交第41个任务时(前面线程都没有执行完,此测试方法中用sleep(100)), * 任务会交给RejectedExecutionHandler来处理 */ public void init() &#123; pool = new ThreadPoolExecutor( 1, 3, 30, TimeUnit.MINUTES, new ArrayBlockingQueue&lt;Runnable&gt;(5), new CustomThreadFactory(), new CustomRejectedExecutionHandler()); &#125; public void destory() &#123; if(pool != null) &#123; pool.shutdownNow(); &#125; &#125; public ExecutorService getCustomThreadPoolExecutor() &#123; return this.pool; &#125; private class CustomThreadFactory implements ThreadFactory &#123; private AtomicInteger count = new AtomicInteger(0); @Override public Thread newThread(Runnable r) &#123; Thread t = new Thread(r); String threadName = CustomThreadPoolExecutor.class.getSimpleName() + count.addAndGet(1); System.out.println(threadName); t.setName(threadName); return t; &#125; &#125; private class CustomRejectedExecutionHandler implements RejectedExecutionHandler &#123; @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) &#123; try &#123; // 核心改造点，由blockingqueue的offer改成put阻塞方法 executor.getQueue().put(r); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; // 测试构造的线程池 public static void main(String[] args) &#123; CustomThreadPoolExecutor exec = new CustomThreadPoolExecutor(); // 1.初始化 exec.init(); ExecutorService pool = exec.getCustomThreadPoolExecutor(); for(int i=1; i&lt;100; i++) &#123; System.out.println(&quot;提交第&quot; + i + &quot;个任务!&quot;); pool.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println(&quot;&gt;&gt;&gt;task is running=====&quot;); TimeUnit.SECONDS.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; // 2.销毁----此处不能销毁,因为任务没有提交执行完,如果销毁线程池,任务也就无法执行了 // exec.destory(); try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; 解释：当提交任务被拒绝时，进入拒绝机制，我们实现拒绝方法，把任务重新用阻塞提交方法put提交，实现阻塞提交任务功能，防止队列过大，OOM，提交被拒绝方法在下面 public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; else if (!addWorker(command, false)) // 进入拒绝机制， 我们把runnable任务拿出来，重新用阻塞操作put，来实现提交阻塞功能 reject(command); &#125; 总结： 用ThreadPoolExecutor自定义线程池，看线程是的用途，如果任务量不大，可以用无界队列，如果任务量非常大，要用有界队列，防止OOM ,尽量使用一个有界的线程池,避免线程死锁导致大量线程堆积. 如果任务量很大，还要求每个任务都处理成功，要对提交的任务进行阻塞提交，重写拒绝机制，改为阻塞提交。保证不抛弃一个任务 最大线程数一般IO密集型设为2N+1,CPU密集型N+1最好，N是CPU核数 核心线程数，看应用，如果是任务，一天跑一次，设置为0，合适，因为跑完就停掉了，如果是常用线程池，看任务量，是保留一个核心还是几个核心线程数 如果要获取任务执行结果，用CompletionService，但是注意，获取任务的结果的要重新开一个线程获取，如果在主线程获取，就要等任务都提交后才获取，就会阻塞大量任务结果，队列过大OOM，所以最好异步开个线程获取结果 自定义线程池 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112package com.lenovo.csdcodrule.util;import java.util.ArrayList;import java.util.List;import java.util.concurrent.*;import java.util.concurrent.atomic.AtomicInteger;/** * @author dcx * @description 多线程调度 * @date 2019/7/8 */public class MyThreadPoolExecutor &#123; /** * 创建一个无回调的线程池 * * @return */ public static ExecutorService newPoolService() &#123; return new ThreadPoolExecutor(3, 3, 3, TimeUnit.HOURS, new ArrayBlockingQueue&lt;Runnable&gt;(100), new MyThreadFactory(), new CustomRejectedExecutionHandler()); &#125; /** * 执行任务,具有线程回调 * * @param futureTasks */ public static void exeFutureTasks(List&lt;FutureTask&lt;Object&gt;&gt; futureTasks) &#123; ExecutorService executorService = newPoolService(); for (FutureTask&lt;Object&gt; futureTask : futureTasks) &#123; executorService.execute(futureTask); &#125; executorService.shutdown(); &#125; /** * 线程池创建工厂 */ private static class MyThreadFactory implements ThreadFactory &#123; private AtomicInteger count = new AtomicInteger(0); @Override public Thread newThread(Runnable r) &#123; Thread thread = new Thread(r); thread.setName(MyThreadPoolExecutor.class.getSimpleName() + count.addAndGet(1)); return thread; &#125; &#125; /** * 自定义堵塞策略 */ private static class CustomRejectedExecutionHandler implements RejectedExecutionHandler &#123; @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) &#123; try &#123; //由blockingqueue的offer改成put阻塞方法 executor.getQueue().put(r); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * 使用 * @param args */ public static void main(String[] args) throws ExecutionException, InterruptedException &#123; //无回调,线程池使用 MyThreadPoolExecutor.newPoolService().execute(()-&gt;&#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); //带有回调的线程池使用,3个任务同时执行 FutureTask&lt;Object&gt; futureTask1 = new FutureTask&lt;&gt;(()-&gt;&#123; Thread.sleep(1000); return 1; &#125;); FutureTask&lt;Object&gt; futureTask2 = new FutureTask&lt;Object&gt;(()-&gt;&#123; Thread.sleep(1000); return 1; &#125;); FutureTask&lt;Object&gt; futureTask3 = new FutureTask&lt;Object&gt;(()-&gt;&#123; Thread.sleep(1000); return 1; &#125;); ArrayList&lt;FutureTask&lt;Object&gt;&gt; futureTasks = new ArrayList&lt;&gt;(); futureTasks.add(futureTask1); futureTasks.add(futureTask2); futureTasks.add(futureTask3); //调用方法 MyThreadPoolExecutor.exeFutureTasks(futureTasks); //堵塞创建的3个任务,拿到回调 Object result1 = futureTask1.get(); Object result2 = futureTask2.get(); Object result3 = futureTask3.get(); &#125;&#125;]]></content>
      <categories>
        <category>java基础</category>
        <category>java之线程,线程池</category>
      </categories>
      <tags>
        <tag>线程</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F02%2F28%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
